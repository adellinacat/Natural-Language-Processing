{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c316ff5",
      "metadata": {
        "id": "8c316ff5"
      },
      "source": [
        "### Урок 10. Машинный перевод. Модель seq2seq и механизм внимания<br>\n",
        "**Задание**\n",
        "\n",
        "Разобраться с моделью перевода (без механизма внимания) как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b9408a",
      "metadata": {
        "id": "02b9408a"
      },
      "source": [
        "Подготовим окружение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eb505320",
      "metadata": {
        "id": "eb505320"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12670faf",
      "metadata": {
        "id": "12670faf"
      },
      "source": [
        "Посмотрим на данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "822c204a",
      "metadata": {
        "id": "822c204a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8525d9b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8525d9b6",
        "outputId": "3aec5e8a-de8b-46aa-ffeb-9ff466725829"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9eb48ece-66cd-408e-be67-b17dbeac3646\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9eb48ece-66cd-408e-be67-b17dbeac3646\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rus.txt to rus.txt\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"rus.txt\""
      ],
      "metadata": {
        "id": "nyZ1WDVuegvL"
      },
      "id": "nyZ1WDVuegvL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8bd0689f",
      "metadata": {
        "id": "8bd0689f"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "\n",
        "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # заменим все пробелом, кроме (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # добавление начала и конца предложения\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "42e164e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "42e164e4",
        "outputId": "dbcd9d2e-53d4-4dce-874e-4baf4b4f0053"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> i can't go . <end>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "preprocess_sentence(\"I can't go.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#создадим новый датасет\n",
        "#почистим предложения\n",
        "#вернем пары в формате: [ENG, RUS]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "metadata": {
        "id": "wMX-B6MidcMa"
      },
      "id": "wMX-B6MidcMa",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en, ru = create_dataset(path, None)\n",
        "print(en[0])\n",
        "print(ru[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksyDoP1zdcO1",
        "outputId": "1eb3cc11-cd65-4aea-c54e-a64c80804ae3"
      },
      "id": "ksyDoP1zdcO1",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> go . <end>\n",
            "<start> марш ! <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#функция токенизации\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "rPQJEHGJdcRR"
      },
      "id": "rPQJEHGJdcRR",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#создание очищенных входных и выходных пар\n",
        "def load_dataset(path, num_examples=None):\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "z7oTy5vrepMw"
      },
      "id": "z7oTy5vrepMw",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опционально, ограничим размер датасета для большей скорости обучения"
      ],
      "metadata": {
        "id": "6uVWeVmqfXbx"
      },
      "id": "6uVWeVmqfXbx"
    },
    {
      "cell_type": "code",
      "source": [
        "len(en), len(ru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSis8H9QepPL",
        "outputId": "ee2b3bd0-c7d9-4557-e1cc-c967742409a6"
      },
      "id": "KSis8H9QepPL",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(479223, 479223)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path, num_examples)\n",
        "\n",
        "#вычислим максимальную длину целевых тензоров\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "metadata": {
        "id": "dCVmt6yxepRg"
      },
      "id": "dCVmt6yxepRg",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#разделим датасет на трйн и тест\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqCm6pBwf35I",
        "outputId": "9cc58a0f-c1c3-47f9-f2bc-4b54caefdd45"
      },
      "id": "fqCm6pBwf35I",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80000 80000 20000 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "      if t!=0:\n",
        "        print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "metadata": {
        "id": "OSwV7yR3f377"
      },
      "id": "OSwV7yR3f377",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#сравним вводимый и целевой тензор\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ng67tzdepUG",
        "outputId": "f13633d6-6a36-4894-9e86-02357e343407"
      },
      "id": "-Ng67tzdepUG",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> это\n",
            "9510 ----> слива\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "18 ----> this\n",
            "8 ----> is\n",
            "9 ----> a\n",
            "3757 ----> plum\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#создадим датасет для тензорфлоу\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "p4VfwuO3ggng"
      },
      "id": "p4VfwuO3ggng",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF5DEiUYhRDw",
        "outputId": "fc28a926-a097-41f9-93c4-c9ebc49937f1"
      },
      "id": "mF5DEiUYhRDw",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 15]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Подключим класс Энкодер\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.batch_sz = batch_sz\n",
        "      self.enc_units = enc_units\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "      self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "      x = self.embedding(x)\n",
        "      output, state = self.gru(x, initial_state = hidden)\n",
        "      return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "yeeB0419hRNk"
      },
      "id": "yeeB0419hRNk",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "#образцы ввода\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_MYMgSrhRQ8",
        "outputId": "734ab7c2-2c17-40d5-ca84-8d93ca10b774"
      },
      "id": "B_MYMgSrhRQ8",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 15, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#еще один класс, описывающий механизм внимания\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "      super(BahdanauAttention, self).__init__()\n",
        "      self.W1 = tf.keras.layers.Dense(units)\n",
        "      self.W2 = tf.keras.layers.Dense(units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "\n",
        "      query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "      score = self.V(tf.nn.tanh(\n",
        "      self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "\n",
        "      attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "\n",
        "      context_vector = attention_weights * values\n",
        "      context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "      return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "h3TNl4alggp6"
      },
      "id": "h3TNl4alggp6",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbvmc2eihqqQ",
        "outputId": "a6e18009-1a7a-4e53-b9b0-cc12e45348da"
      },
      "id": "qbvmc2eihqqQ",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 15, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Вводим класс декодера\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.batch_sz = batch_sz\n",
        "      self.dec_units = dec_units\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "      self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "      self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "      #используется для внимания\n",
        "      self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "      context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "      x = self.embedding(x)\n",
        "\n",
        "      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "      #передача объединенного вектора в GPU\n",
        "      output, state = self.gru(x)\n",
        "\n",
        "      output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "      x = self.fc(output)\n",
        "\n",
        "      return x, state, attention_weights"
      ],
      "metadata": {
        "id": "pe8iJ8Hlhqt7"
      },
      "id": "pe8iJ8Hlhqt7",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy62fXNLggsb",
        "outputId": "9ba1c5cb-0208-46e1-c009-caa2c88da2cd"
      },
      "id": "hy62fXNLggsb",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 7386)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определим оптимизатор и функцию потерь"
      ],
      "metadata": {
        "id": "VoIUhoWHi5qZ"
      },
      "id": "VoIUhoWHi5qZ"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "cEoy_ZJpix9k"
      },
      "id": "cEoy_ZJpix9k",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Контрольные точки объектов"
      ],
      "metadata": {
        "id": "lAQWTlEjjFX-"
      },
      "id": "lAQWTlEjjFX-"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_attention_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "obgoaogViyAD"
      },
      "id": "obgoaogViyAD",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "      dec_hidden = enc_hidden\n",
        "\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      #передача целевой переменной в качестве следующего ввода\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        # передача в декодер\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "YVrMd9jSiyCa"
      },
      "id": "YVrMd9jSiyCa",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step(inp, targ, enc_hidden)\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                    batch,\n",
        "                                                    batch_loss.numpy()))\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUJgyRgViyEu",
        "outputId": "bdfe0186-c52c-4fcd-8f27-e25cbb9ffd09"
      },
      "id": "NUJgyRgViyEu",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.6688\n",
            "Epoch 1 Batch 100 Loss 2.0629\n",
            "Epoch 1 Batch 200 Loss 1.8643\n",
            "Epoch 1 Batch 300 Loss 1.7428\n",
            "Epoch 1 Batch 400 Loss 1.5708\n",
            "Epoch 1 Batch 500 Loss 1.5245\n",
            "Epoch 1 Batch 600 Loss 1.3327\n",
            "Epoch 1 Batch 700 Loss 1.2896\n",
            "Epoch 1 Batch 800 Loss 1.1371\n",
            "Epoch 1 Batch 900 Loss 1.1635\n",
            "Epoch 1 Batch 1000 Loss 1.0995\n",
            "Epoch 1 Batch 1100 Loss 1.0088\n",
            "Epoch 1 Batch 1200 Loss 0.8496\n",
            "Epoch 1 Loss 1.4429\n",
            "Time taken for 1 epoch 116.59973812103271 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8492\n",
            "Epoch 2 Batch 100 Loss 0.8622\n",
            "Epoch 2 Batch 200 Loss 0.7692\n",
            "Epoch 2 Batch 300 Loss 0.5689\n",
            "Epoch 2 Batch 400 Loss 0.5288\n",
            "Epoch 2 Batch 500 Loss 0.6445\n",
            "Epoch 2 Batch 600 Loss 0.5273\n",
            "Epoch 2 Batch 700 Loss 0.5473\n",
            "Epoch 2 Batch 800 Loss 0.6305\n",
            "Epoch 2 Batch 900 Loss 0.4630\n",
            "Epoch 2 Batch 1000 Loss 0.5625\n",
            "Epoch 2 Batch 1100 Loss 0.4410\n",
            "Epoch 2 Batch 1200 Loss 0.4731\n",
            "Epoch 2 Loss 0.6174\n",
            "Time taken for 1 epoch 97.69097685813904 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.3724\n",
            "Epoch 3 Batch 100 Loss 0.3731\n",
            "Epoch 3 Batch 200 Loss 0.3530\n",
            "Epoch 3 Batch 300 Loss 0.3262\n",
            "Epoch 3 Batch 400 Loss 0.2921\n",
            "Epoch 3 Batch 500 Loss 0.4248\n",
            "Epoch 3 Batch 600 Loss 0.2684\n",
            "Epoch 3 Batch 700 Loss 0.3768\n",
            "Epoch 3 Batch 800 Loss 0.2803\n",
            "Epoch 3 Batch 900 Loss 0.2721\n",
            "Epoch 3 Batch 1000 Loss 0.2545\n",
            "Epoch 3 Batch 1100 Loss 0.3259\n",
            "Epoch 3 Batch 1200 Loss 0.3359\n",
            "Epoch 3 Loss 0.3290\n",
            "Time taken for 1 epoch 96.81145596504211 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2231\n",
            "Epoch 4 Batch 100 Loss 0.1994\n",
            "Epoch 4 Batch 200 Loss 0.2118\n",
            "Epoch 4 Batch 300 Loss 0.1586\n",
            "Epoch 4 Batch 400 Loss 0.2008\n",
            "Epoch 4 Batch 500 Loss 0.2398\n",
            "Epoch 4 Batch 600 Loss 0.2216\n",
            "Epoch 4 Batch 700 Loss 0.2269\n",
            "Epoch 4 Batch 800 Loss 0.1961\n",
            "Epoch 4 Batch 900 Loss 0.2507\n",
            "Epoch 4 Batch 1000 Loss 0.2208\n",
            "Epoch 4 Batch 1100 Loss 0.2384\n",
            "Epoch 4 Batch 1200 Loss 0.2003\n",
            "Epoch 4 Loss 0.2062\n",
            "Time taken for 1 epoch 103.22505831718445 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1281\n",
            "Epoch 5 Batch 100 Loss 0.1558\n",
            "Epoch 5 Batch 200 Loss 0.0871\n",
            "Epoch 5 Batch 300 Loss 0.1516\n",
            "Epoch 5 Batch 400 Loss 0.1093\n",
            "Epoch 5 Batch 500 Loss 0.1530\n",
            "Epoch 5 Batch 600 Loss 0.1821\n",
            "Epoch 5 Batch 700 Loss 0.1533\n",
            "Epoch 5 Batch 800 Loss 0.1699\n",
            "Epoch 5 Batch 900 Loss 0.1385\n",
            "Epoch 5 Batch 1000 Loss 0.1369\n",
            "Epoch 5 Batch 1100 Loss 0.1499\n",
            "Epoch 5 Batch 1200 Loss 0.1912\n",
            "Epoch 5 Loss 0.1476\n",
            "Time taken for 1 epoch 96.99285864830017 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1351\n",
            "Epoch 6 Batch 100 Loss 0.1246\n",
            "Epoch 6 Batch 200 Loss 0.1344\n",
            "Epoch 6 Batch 300 Loss 0.1310\n",
            "Epoch 6 Batch 400 Loss 0.1034\n",
            "Epoch 6 Batch 500 Loss 0.1278\n",
            "Epoch 6 Batch 600 Loss 0.1265\n",
            "Epoch 6 Batch 700 Loss 0.1493\n",
            "Epoch 6 Batch 800 Loss 0.1189\n",
            "Epoch 6 Batch 900 Loss 0.1082\n",
            "Epoch 6 Batch 1000 Loss 0.1239\n",
            "Epoch 6 Batch 1100 Loss 0.1034\n",
            "Epoch 6 Batch 1200 Loss 0.1413\n",
            "Epoch 6 Loss 0.1170\n",
            "Time taken for 1 epoch 102.61664056777954 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0659\n",
            "Epoch 7 Batch 100 Loss 0.0912\n",
            "Epoch 7 Batch 200 Loss 0.1004\n",
            "Epoch 7 Batch 300 Loss 0.1048\n",
            "Epoch 7 Batch 400 Loss 0.0844\n",
            "Epoch 7 Batch 500 Loss 0.1147\n",
            "Epoch 7 Batch 600 Loss 0.0947\n",
            "Epoch 7 Batch 700 Loss 0.1047\n",
            "Epoch 7 Batch 800 Loss 0.1090\n",
            "Epoch 7 Batch 900 Loss 0.1240\n",
            "Epoch 7 Batch 1000 Loss 0.1140\n",
            "Epoch 7 Batch 1100 Loss 0.1466\n",
            "Epoch 7 Batch 1200 Loss 0.1841\n",
            "Epoch 7 Loss 0.0998\n",
            "Time taken for 1 epoch 96.6943609714508 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0977\n",
            "Epoch 8 Batch 100 Loss 0.0632\n",
            "Epoch 8 Batch 200 Loss 0.0800\n",
            "Epoch 8 Batch 300 Loss 0.0870\n",
            "Epoch 8 Batch 400 Loss 0.0789\n",
            "Epoch 8 Batch 500 Loss 0.0751\n",
            "Epoch 8 Batch 600 Loss 0.0724\n",
            "Epoch 8 Batch 700 Loss 0.0679\n",
            "Epoch 8 Batch 800 Loss 0.0739\n",
            "Epoch 8 Batch 900 Loss 0.1281\n",
            "Epoch 8 Batch 1000 Loss 0.0664\n",
            "Epoch 8 Batch 1100 Loss 0.0728\n",
            "Epoch 8 Batch 1200 Loss 0.1295\n",
            "Epoch 8 Loss 0.0887\n",
            "Time taken for 1 epoch 102.48653316497803 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0788\n",
            "Epoch 9 Batch 100 Loss 0.0632\n",
            "Epoch 9 Batch 200 Loss 0.0838\n",
            "Epoch 9 Batch 300 Loss 0.1158\n",
            "Epoch 9 Batch 400 Loss 0.0722\n",
            "Epoch 9 Batch 500 Loss 0.1044\n",
            "Epoch 9 Batch 600 Loss 0.0951\n",
            "Epoch 9 Batch 700 Loss 0.0730\n",
            "Epoch 9 Batch 800 Loss 0.0727\n",
            "Epoch 9 Batch 900 Loss 0.1234\n",
            "Epoch 9 Batch 1000 Loss 0.1245\n",
            "Epoch 9 Batch 1100 Loss 0.1027\n",
            "Epoch 9 Batch 1200 Loss 0.0397\n",
            "Epoch 9 Loss 0.0821\n",
            "Time taken for 1 epoch 96.62352991104126 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0527\n",
            "Epoch 10 Batch 100 Loss 0.0764\n",
            "Epoch 10 Batch 200 Loss 0.0763\n",
            "Epoch 10 Batch 300 Loss 0.0656\n",
            "Epoch 10 Batch 400 Loss 0.0555\n",
            "Epoch 10 Batch 500 Loss 0.0573\n",
            "Epoch 10 Batch 600 Loss 0.0824\n",
            "Epoch 10 Batch 700 Loss 0.0834\n",
            "Epoch 10 Batch 800 Loss 0.0702\n",
            "Epoch 10 Batch 900 Loss 0.1078\n",
            "Epoch 10 Batch 1000 Loss 0.0690\n",
            "Epoch 10 Batch 1100 Loss 0.0465\n",
            "Epoch 10 Batch 1200 Loss 0.0973\n",
            "Epoch 10 Loss 0.0770\n",
            "Time taken for 1 epoch 100.34112334251404 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0925\n",
            "Epoch 11 Batch 100 Loss 0.0672\n",
            "Epoch 11 Batch 200 Loss 0.0378\n",
            "Epoch 11 Batch 300 Loss 0.0377\n",
            "Epoch 11 Batch 400 Loss 0.1008\n",
            "Epoch 11 Batch 500 Loss 0.0754\n",
            "Epoch 11 Batch 600 Loss 0.0662\n",
            "Epoch 11 Batch 700 Loss 0.0791\n",
            "Epoch 11 Batch 800 Loss 0.0822\n",
            "Epoch 11 Batch 900 Loss 0.0792\n",
            "Epoch 11 Batch 1000 Loss 0.0857\n",
            "Epoch 11 Batch 1100 Loss 0.0818\n",
            "Epoch 11 Batch 1200 Loss 0.0680\n",
            "Epoch 11 Loss 0.0717\n",
            "Time taken for 1 epoch 96.67756032943726 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0518\n",
            "Epoch 12 Batch 100 Loss 0.0579\n",
            "Epoch 12 Batch 200 Loss 0.0461\n",
            "Epoch 12 Batch 300 Loss 0.0596\n",
            "Epoch 12 Batch 400 Loss 0.0645\n",
            "Epoch 12 Batch 500 Loss 0.0520\n",
            "Epoch 12 Batch 600 Loss 0.0783\n",
            "Epoch 12 Batch 700 Loss 0.0700\n",
            "Epoch 12 Batch 800 Loss 0.0801\n",
            "Epoch 12 Batch 900 Loss 0.0844\n",
            "Epoch 12 Batch 1000 Loss 0.0645\n",
            "Epoch 12 Batch 1100 Loss 0.0560\n",
            "Epoch 12 Batch 1200 Loss 0.0672\n",
            "Epoch 12 Loss 0.0695\n",
            "Time taken for 1 epoch 102.6373860836029 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0349\n",
            "Epoch 13 Batch 100 Loss 0.0626\n",
            "Epoch 13 Batch 200 Loss 0.0910\n",
            "Epoch 13 Batch 300 Loss 0.0462\n",
            "Epoch 13 Batch 400 Loss 0.0623\n",
            "Epoch 13 Batch 500 Loss 0.0484\n",
            "Epoch 13 Batch 600 Loss 0.0639\n",
            "Epoch 13 Batch 700 Loss 0.0331\n",
            "Epoch 13 Batch 800 Loss 0.0447\n",
            "Epoch 13 Batch 900 Loss 0.0925\n",
            "Epoch 13 Batch 1000 Loss 0.0699\n",
            "Epoch 13 Batch 1100 Loss 0.1057\n",
            "Epoch 13 Batch 1200 Loss 0.0713\n",
            "Epoch 13 Loss 0.0668\n",
            "Time taken for 1 epoch 96.63865303993225 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0354\n",
            "Epoch 14 Batch 100 Loss 0.0627\n",
            "Epoch 14 Batch 200 Loss 0.0567\n",
            "Epoch 14 Batch 300 Loss 0.0654\n",
            "Epoch 14 Batch 400 Loss 0.0740\n",
            "Epoch 14 Batch 500 Loss 0.0751\n",
            "Epoch 14 Batch 600 Loss 0.0692\n",
            "Epoch 14 Batch 700 Loss 0.1027\n",
            "Epoch 14 Batch 800 Loss 0.0706\n",
            "Epoch 14 Batch 900 Loss 0.1058\n",
            "Epoch 14 Batch 1000 Loss 0.0741\n",
            "Epoch 14 Batch 1100 Loss 0.0850\n",
            "Epoch 14 Batch 1200 Loss 0.0645\n",
            "Epoch 14 Loss 0.0649\n",
            "Time taken for 1 epoch 102.65523862838745 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0673\n",
            "Epoch 15 Batch 100 Loss 0.0465\n",
            "Epoch 15 Batch 200 Loss 0.0523\n",
            "Epoch 15 Batch 300 Loss 0.0375\n",
            "Epoch 15 Batch 400 Loss 0.0440\n",
            "Epoch 15 Batch 500 Loss 0.0549\n",
            "Epoch 15 Batch 600 Loss 0.0434\n",
            "Epoch 15 Batch 700 Loss 0.1015\n",
            "Epoch 15 Batch 800 Loss 0.0673\n",
            "Epoch 15 Batch 900 Loss 0.0555\n",
            "Epoch 15 Batch 1000 Loss 0.0747\n",
            "Epoch 15 Batch 1100 Loss 0.0767\n",
            "Epoch 15 Batch 1200 Loss 0.0724\n",
            "Epoch 15 Loss 0.0610\n",
            "Time taken for 1 epoch 96.69102215766907 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0639\n",
            "Epoch 16 Batch 100 Loss 0.0344\n",
            "Epoch 16 Batch 200 Loss 0.0573\n",
            "Epoch 16 Batch 300 Loss 0.0452\n",
            "Epoch 16 Batch 400 Loss 0.0351\n",
            "Epoch 16 Batch 500 Loss 0.0568\n",
            "Epoch 16 Batch 600 Loss 0.0702\n",
            "Epoch 16 Batch 700 Loss 0.0866\n",
            "Epoch 16 Batch 800 Loss 0.0377\n",
            "Epoch 16 Batch 900 Loss 0.0989\n",
            "Epoch 16 Batch 1000 Loss 0.0927\n",
            "Epoch 16 Batch 1100 Loss 0.0611\n",
            "Epoch 16 Batch 1200 Loss 0.0928\n",
            "Epoch 16 Loss 0.0598\n",
            "Time taken for 1 epoch 97.74137783050537 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0533\n",
            "Epoch 17 Batch 100 Loss 0.0561\n",
            "Epoch 17 Batch 200 Loss 0.0673\n",
            "Epoch 17 Batch 300 Loss 0.0642\n",
            "Epoch 17 Batch 400 Loss 0.0610\n",
            "Epoch 17 Batch 500 Loss 0.0714\n",
            "Epoch 17 Batch 600 Loss 0.0436\n",
            "Epoch 17 Batch 700 Loss 0.0670\n",
            "Epoch 17 Batch 800 Loss 0.0625\n",
            "Epoch 17 Batch 900 Loss 0.0857\n",
            "Epoch 17 Batch 1000 Loss 0.1087\n",
            "Epoch 17 Batch 1100 Loss 0.0674\n",
            "Epoch 17 Batch 1200 Loss 0.0667\n",
            "Epoch 17 Loss 0.0597\n",
            "Time taken for 1 epoch 96.69498610496521 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0626\n",
            "Epoch 18 Batch 100 Loss 0.0257\n",
            "Epoch 18 Batch 200 Loss 0.0301\n",
            "Epoch 18 Batch 300 Loss 0.0536\n",
            "Epoch 18 Batch 400 Loss 0.0951\n",
            "Epoch 18 Batch 500 Loss 0.0467\n",
            "Epoch 18 Batch 600 Loss 0.0306\n",
            "Epoch 18 Batch 700 Loss 0.0566\n",
            "Epoch 18 Batch 800 Loss 0.0948\n",
            "Epoch 18 Batch 900 Loss 0.0501\n",
            "Epoch 18 Batch 1000 Loss 0.0435\n",
            "Epoch 18 Batch 1100 Loss 0.0757\n",
            "Epoch 18 Batch 1200 Loss 0.0471\n",
            "Epoch 18 Loss 0.0569\n",
            "Time taken for 1 epoch 102.4634337425232 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0573\n",
            "Epoch 19 Batch 100 Loss 0.0443\n",
            "Epoch 19 Batch 200 Loss 0.0408\n",
            "Epoch 19 Batch 300 Loss 0.0435\n",
            "Epoch 19 Batch 400 Loss 0.0404\n",
            "Epoch 19 Batch 500 Loss 0.0518\n",
            "Epoch 19 Batch 600 Loss 0.0504\n",
            "Epoch 19 Batch 700 Loss 0.0554\n",
            "Epoch 19 Batch 800 Loss 0.0592\n",
            "Epoch 19 Batch 900 Loss 0.0607\n",
            "Epoch 19 Batch 1000 Loss 0.0491\n",
            "Epoch 19 Batch 1100 Loss 0.0415\n",
            "Epoch 19 Batch 1200 Loss 0.0211\n",
            "Epoch 19 Loss 0.0550\n",
            "Time taken for 1 epoch 96.60715103149414 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0483\n",
            "Epoch 20 Batch 100 Loss 0.0710\n",
            "Epoch 20 Batch 200 Loss 0.0573\n",
            "Epoch 20 Batch 300 Loss 0.0580\n",
            "Epoch 20 Batch 400 Loss 0.0677\n",
            "Epoch 20 Batch 500 Loss 0.0383\n",
            "Epoch 20 Batch 600 Loss 0.0446\n",
            "Epoch 20 Batch 700 Loss 0.0903\n",
            "Epoch 20 Batch 800 Loss 0.0787\n",
            "Epoch 20 Batch 900 Loss 0.0342\n",
            "Epoch 20 Batch 1000 Loss 0.0460\n",
            "Epoch 20 Batch 1100 Loss 0.0845\n",
            "Epoch 20 Batch 1200 Loss 0.0603\n",
            "Epoch 20 Loss 0.0548\n",
            "Time taken for 1 epoch 102.46038031578064 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0280\n",
            "Epoch 21 Batch 100 Loss 0.0272\n",
            "Epoch 21 Batch 200 Loss 0.0294\n",
            "Epoch 21 Batch 300 Loss 0.0618\n",
            "Epoch 21 Batch 400 Loss 0.0459\n",
            "Epoch 21 Batch 500 Loss 0.0631\n",
            "Epoch 21 Batch 600 Loss 0.0441\n",
            "Epoch 21 Batch 700 Loss 0.0263\n",
            "Epoch 21 Batch 800 Loss 0.0340\n",
            "Epoch 21 Batch 900 Loss 0.0555\n",
            "Epoch 21 Batch 1000 Loss 0.0693\n",
            "Epoch 21 Batch 1100 Loss 0.0555\n",
            "Epoch 21 Batch 1200 Loss 0.0450\n",
            "Epoch 21 Loss 0.0526\n",
            "Time taken for 1 epoch 96.55828404426575 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0442\n",
            "Epoch 22 Batch 100 Loss 0.0521\n",
            "Epoch 22 Batch 200 Loss 0.0456\n",
            "Epoch 22 Batch 300 Loss 0.0481\n",
            "Epoch 22 Batch 400 Loss 0.0386\n",
            "Epoch 22 Batch 500 Loss 0.0384\n",
            "Epoch 22 Batch 600 Loss 0.0710\n",
            "Epoch 22 Batch 700 Loss 0.0855\n",
            "Epoch 22 Batch 800 Loss 0.0359\n",
            "Epoch 22 Batch 900 Loss 0.0740\n",
            "Epoch 22 Batch 1000 Loss 0.0603\n",
            "Epoch 22 Batch 1100 Loss 0.0794\n",
            "Epoch 22 Batch 1200 Loss 0.0482\n",
            "Epoch 22 Loss 0.0517\n",
            "Time taken for 1 epoch 102.67813110351562 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0335\n",
            "Epoch 23 Batch 100 Loss 0.0595\n",
            "Epoch 23 Batch 200 Loss 0.0349\n",
            "Epoch 23 Batch 300 Loss 0.0284\n",
            "Epoch 23 Batch 400 Loss 0.0662\n",
            "Epoch 23 Batch 500 Loss 0.0573\n",
            "Epoch 23 Batch 600 Loss 0.0710\n",
            "Epoch 23 Batch 700 Loss 0.0332\n",
            "Epoch 23 Batch 800 Loss 0.0540\n",
            "Epoch 23 Batch 900 Loss 0.0421\n",
            "Epoch 23 Batch 1000 Loss 0.0755\n",
            "Epoch 23 Batch 1100 Loss 0.0511\n",
            "Epoch 23 Batch 1200 Loss 0.0538\n",
            "Epoch 23 Loss 0.0513\n",
            "Time taken for 1 epoch 96.56835985183716 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0333\n",
            "Epoch 24 Batch 100 Loss 0.0361\n",
            "Epoch 24 Batch 200 Loss 0.0274\n",
            "Epoch 24 Batch 300 Loss 0.0515\n",
            "Epoch 24 Batch 400 Loss 0.0486\n",
            "Epoch 24 Batch 500 Loss 0.0397\n",
            "Epoch 24 Batch 600 Loss 0.0330\n",
            "Epoch 24 Batch 700 Loss 0.0486\n",
            "Epoch 24 Batch 800 Loss 0.0941\n",
            "Epoch 24 Batch 900 Loss 0.0477\n",
            "Epoch 24 Batch 1000 Loss 0.0438\n",
            "Epoch 24 Batch 1100 Loss 0.0647\n",
            "Epoch 24 Batch 1200 Loss 0.0526\n",
            "Epoch 24 Loss 0.0497\n",
            "Time taken for 1 epoch 102.51578736305237 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0423\n",
            "Epoch 25 Batch 100 Loss 0.0516\n",
            "Epoch 25 Batch 200 Loss 0.0284\n",
            "Epoch 25 Batch 300 Loss 0.0523\n",
            "Epoch 25 Batch 400 Loss 0.0313\n",
            "Epoch 25 Batch 500 Loss 0.0661\n",
            "Epoch 25 Batch 600 Loss 0.0452\n",
            "Epoch 25 Batch 700 Loss 0.0442\n",
            "Epoch 25 Batch 800 Loss 0.0506\n",
            "Epoch 25 Batch 900 Loss 0.0533\n",
            "Epoch 25 Batch 1000 Loss 0.0624\n",
            "Epoch 25 Batch 1100 Loss 0.0371\n",
            "Epoch 25 Batch 1200 Loss 0.0396\n",
            "Epoch 25 Loss 0.0494\n",
            "Time taken for 1 epoch 96.66694712638855 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0455\n",
            "Epoch 26 Batch 100 Loss 0.0412\n",
            "Epoch 26 Batch 200 Loss 0.0512\n",
            "Epoch 26 Batch 300 Loss 0.0276\n",
            "Epoch 26 Batch 400 Loss 0.0303\n",
            "Epoch 26 Batch 500 Loss 0.0565\n",
            "Epoch 26 Batch 600 Loss 0.0577\n",
            "Epoch 26 Batch 700 Loss 0.0430\n",
            "Epoch 26 Batch 800 Loss 0.0565\n",
            "Epoch 26 Batch 900 Loss 0.0354\n",
            "Epoch 26 Batch 1000 Loss 0.0715\n",
            "Epoch 26 Batch 1100 Loss 0.0495\n",
            "Epoch 26 Batch 1200 Loss 0.0666\n",
            "Epoch 26 Loss 0.0483\n",
            "Time taken for 1 epoch 97.89646458625793 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0361\n",
            "Epoch 27 Batch 100 Loss 0.0381\n",
            "Epoch 27 Batch 200 Loss 0.0480\n",
            "Epoch 27 Batch 300 Loss 0.0478\n",
            "Epoch 27 Batch 400 Loss 0.0300\n",
            "Epoch 27 Batch 500 Loss 0.0357\n",
            "Epoch 27 Batch 600 Loss 0.0404\n",
            "Epoch 27 Batch 700 Loss 0.0531\n",
            "Epoch 27 Batch 800 Loss 0.0503\n",
            "Epoch 27 Batch 900 Loss 0.0282\n",
            "Epoch 27 Batch 1000 Loss 0.0708\n",
            "Epoch 27 Batch 1100 Loss 0.0748\n",
            "Epoch 27 Batch 1200 Loss 0.0442\n",
            "Epoch 27 Loss 0.0479\n",
            "Time taken for 1 epoch 96.48292374610901 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0258\n",
            "Epoch 28 Batch 100 Loss 0.0266\n",
            "Epoch 28 Batch 200 Loss 0.0780\n",
            "Epoch 28 Batch 300 Loss 0.0481\n",
            "Epoch 28 Batch 400 Loss 0.0516\n",
            "Epoch 28 Batch 500 Loss 0.0434\n",
            "Epoch 28 Batch 600 Loss 0.0393\n",
            "Epoch 28 Batch 700 Loss 0.0578\n",
            "Epoch 28 Batch 800 Loss 0.0370\n",
            "Epoch 28 Batch 900 Loss 0.0569\n",
            "Epoch 28 Batch 1000 Loss 0.0616\n",
            "Epoch 28 Batch 1100 Loss 0.0845\n",
            "Epoch 28 Batch 1200 Loss 0.0490\n",
            "Epoch 28 Loss 0.0475\n",
            "Time taken for 1 epoch 102.8598997592926 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0360\n",
            "Epoch 29 Batch 100 Loss 0.0506\n",
            "Epoch 29 Batch 200 Loss 0.0354\n",
            "Epoch 29 Batch 300 Loss 0.0349\n",
            "Epoch 29 Batch 400 Loss 0.0724\n",
            "Epoch 29 Batch 500 Loss 0.0381\n",
            "Epoch 29 Batch 600 Loss 0.0332\n",
            "Epoch 29 Batch 700 Loss 0.0366\n",
            "Epoch 29 Batch 800 Loss 0.0478\n",
            "Epoch 29 Batch 900 Loss 0.0413\n",
            "Epoch 29 Batch 1000 Loss 0.0569\n",
            "Epoch 29 Batch 1100 Loss 0.0366\n",
            "Epoch 29 Batch 1200 Loss 0.0434\n",
            "Epoch 29 Loss 0.0456\n",
            "Time taken for 1 epoch 96.49544668197632 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0205\n",
            "Epoch 30 Batch 100 Loss 0.0362\n",
            "Epoch 30 Batch 200 Loss 0.0260\n",
            "Epoch 30 Batch 300 Loss 0.0426\n",
            "Epoch 30 Batch 400 Loss 0.0343\n",
            "Epoch 30 Batch 500 Loss 0.0588\n",
            "Epoch 30 Batch 600 Loss 0.0426\n",
            "Epoch 30 Batch 700 Loss 0.0286\n",
            "Epoch 30 Batch 800 Loss 0.0451\n",
            "Epoch 30 Batch 900 Loss 0.0556\n",
            "Epoch 30 Batch 1000 Loss 0.0287\n",
            "Epoch 30 Batch 1100 Loss 0.0565\n",
            "Epoch 30 Batch 1200 Loss 0.0749\n",
            "Epoch 30 Loss 0.0456\n",
            "Time taken for 1 epoch 102.27445936203003 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0210\n",
            "Epoch 31 Batch 100 Loss 0.0270\n",
            "Epoch 31 Batch 200 Loss 0.0403\n",
            "Epoch 31 Batch 300 Loss 0.0469\n",
            "Epoch 31 Batch 400 Loss 0.0336\n",
            "Epoch 31 Batch 500 Loss 0.0501\n",
            "Epoch 31 Batch 600 Loss 0.0461\n",
            "Epoch 31 Batch 700 Loss 0.0486\n",
            "Epoch 31 Batch 800 Loss 0.0711\n",
            "Epoch 31 Batch 900 Loss 0.0511\n",
            "Epoch 31 Batch 1000 Loss 0.0575\n",
            "Epoch 31 Batch 1100 Loss 0.0526\n",
            "Epoch 31 Batch 1200 Loss 0.0397\n",
            "Epoch 31 Loss 0.0446\n",
            "Time taken for 1 epoch 96.59228706359863 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0429\n",
            "Epoch 32 Batch 100 Loss 0.0362\n",
            "Epoch 32 Batch 200 Loss 0.0240\n",
            "Epoch 32 Batch 300 Loss 0.0180\n",
            "Epoch 32 Batch 400 Loss 0.0586\n",
            "Epoch 32 Batch 500 Loss 0.0343\n",
            "Epoch 32 Batch 600 Loss 0.0369\n",
            "Epoch 32 Batch 700 Loss 0.0483\n",
            "Epoch 32 Batch 800 Loss 0.0428\n",
            "Epoch 32 Batch 900 Loss 0.0291\n",
            "Epoch 32 Batch 1000 Loss 0.0889\n",
            "Epoch 32 Batch 1100 Loss 0.0405\n",
            "Epoch 32 Batch 1200 Loss 0.0552\n",
            "Epoch 32 Loss 0.0442\n",
            "Time taken for 1 epoch 102.60451865196228 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0367\n",
            "Epoch 33 Batch 100 Loss 0.0463\n",
            "Epoch 33 Batch 200 Loss 0.0484\n",
            "Epoch 33 Batch 300 Loss 0.0427\n",
            "Epoch 33 Batch 400 Loss 0.0268\n",
            "Epoch 33 Batch 500 Loss 0.0530\n",
            "Epoch 33 Batch 600 Loss 0.0243\n",
            "Epoch 33 Batch 700 Loss 0.0349\n",
            "Epoch 33 Batch 800 Loss 0.0393\n",
            "Epoch 33 Batch 900 Loss 0.0335\n",
            "Epoch 33 Batch 1000 Loss 0.0406\n",
            "Epoch 33 Batch 1100 Loss 0.0546\n",
            "Epoch 33 Batch 1200 Loss 0.0525\n",
            "Epoch 33 Loss 0.0440\n",
            "Time taken for 1 epoch 96.65142560005188 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0356\n",
            "Epoch 34 Batch 100 Loss 0.0335\n",
            "Epoch 34 Batch 200 Loss 0.0242\n",
            "Epoch 34 Batch 300 Loss 0.0197\n",
            "Epoch 34 Batch 400 Loss 0.0408\n",
            "Epoch 34 Batch 500 Loss 0.0503\n",
            "Epoch 34 Batch 600 Loss 0.0261\n",
            "Epoch 34 Batch 700 Loss 0.0369\n",
            "Epoch 34 Batch 800 Loss 0.0471\n",
            "Epoch 34 Batch 900 Loss 0.0473\n",
            "Epoch 34 Batch 1000 Loss 0.0414\n",
            "Epoch 34 Batch 1100 Loss 0.0318\n",
            "Epoch 34 Batch 1200 Loss 0.0612\n",
            "Epoch 34 Loss 0.0429\n",
            "Time taken for 1 epoch 102.72722220420837 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0337\n",
            "Epoch 35 Batch 100 Loss 0.0191\n",
            "Epoch 35 Batch 200 Loss 0.0268\n",
            "Epoch 35 Batch 300 Loss 0.0318\n",
            "Epoch 35 Batch 400 Loss 0.0490\n",
            "Epoch 35 Batch 500 Loss 0.0474\n",
            "Epoch 35 Batch 600 Loss 0.0557\n",
            "Epoch 35 Batch 700 Loss 0.0384\n",
            "Epoch 35 Batch 800 Loss 0.0336\n",
            "Epoch 35 Batch 900 Loss 0.0588\n",
            "Epoch 35 Batch 1000 Loss 0.0470\n",
            "Epoch 35 Batch 1100 Loss 0.0591\n",
            "Epoch 35 Batch 1200 Loss 0.0398\n",
            "Epoch 35 Loss 0.0427\n",
            "Time taken for 1 epoch 96.63747000694275 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0380\n",
            "Epoch 36 Batch 100 Loss 0.0247\n",
            "Epoch 36 Batch 200 Loss 0.0423\n",
            "Epoch 36 Batch 300 Loss 0.0261\n",
            "Epoch 36 Batch 400 Loss 0.0389\n",
            "Epoch 36 Batch 500 Loss 0.0546\n",
            "Epoch 36 Batch 600 Loss 0.0492\n",
            "Epoch 36 Batch 700 Loss 0.0597\n",
            "Epoch 36 Batch 800 Loss 0.0249\n",
            "Epoch 36 Batch 900 Loss 0.0710\n",
            "Epoch 36 Batch 1000 Loss 0.0425\n",
            "Epoch 36 Batch 1100 Loss 0.0521\n",
            "Epoch 36 Batch 1200 Loss 0.0529\n",
            "Epoch 36 Loss 0.0433\n",
            "Time taken for 1 epoch 98.07888340950012 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0214\n",
            "Epoch 37 Batch 100 Loss 0.0188\n",
            "Epoch 37 Batch 200 Loss 0.0354\n",
            "Epoch 37 Batch 300 Loss 0.0364\n",
            "Epoch 37 Batch 400 Loss 0.0480\n",
            "Epoch 37 Batch 500 Loss 0.0431\n",
            "Epoch 37 Batch 600 Loss 0.0205\n",
            "Epoch 37 Batch 700 Loss 0.0441\n",
            "Epoch 37 Batch 800 Loss 0.0300\n",
            "Epoch 37 Batch 900 Loss 0.0538\n",
            "Epoch 37 Batch 1000 Loss 0.0474\n",
            "Epoch 37 Batch 1100 Loss 0.0424\n",
            "Epoch 37 Batch 1200 Loss 0.0354\n",
            "Epoch 37 Loss 0.0422\n",
            "Time taken for 1 epoch 96.5120542049408 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0250\n",
            "Epoch 38 Batch 100 Loss 0.0162\n",
            "Epoch 38 Batch 200 Loss 0.0090\n",
            "Epoch 38 Batch 300 Loss 0.0284\n",
            "Epoch 38 Batch 400 Loss 0.0395\n",
            "Epoch 38 Batch 500 Loss 0.0384\n",
            "Epoch 38 Batch 600 Loss 0.0378\n",
            "Epoch 38 Batch 700 Loss 0.0592\n",
            "Epoch 38 Batch 800 Loss 0.0506\n",
            "Epoch 38 Batch 900 Loss 0.0335\n",
            "Epoch 38 Batch 1000 Loss 0.0338\n",
            "Epoch 38 Batch 1100 Loss 0.0383\n",
            "Epoch 38 Batch 1200 Loss 0.0438\n",
            "Epoch 38 Loss 0.0416\n",
            "Time taken for 1 epoch 97.45167565345764 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0328\n",
            "Epoch 39 Batch 100 Loss 0.0291\n",
            "Epoch 39 Batch 200 Loss 0.0282\n",
            "Epoch 39 Batch 300 Loss 0.0306\n",
            "Epoch 39 Batch 400 Loss 0.0251\n",
            "Epoch 39 Batch 500 Loss 0.0485\n",
            "Epoch 39 Batch 600 Loss 0.0414\n",
            "Epoch 39 Batch 700 Loss 0.0387\n",
            "Epoch 39 Batch 800 Loss 0.0291\n",
            "Epoch 39 Batch 900 Loss 0.0428\n",
            "Epoch 39 Batch 1000 Loss 0.0483\n",
            "Epoch 39 Batch 1100 Loss 0.0716\n",
            "Epoch 39 Batch 1200 Loss 0.0418\n",
            "Epoch 39 Loss 0.0417\n",
            "Time taken for 1 epoch 96.39771223068237 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0183\n",
            "Epoch 40 Batch 100 Loss 0.0304\n",
            "Epoch 40 Batch 200 Loss 0.0418\n",
            "Epoch 40 Batch 300 Loss 0.0318\n",
            "Epoch 40 Batch 400 Loss 0.0267\n",
            "Epoch 40 Batch 500 Loss 0.0319\n",
            "Epoch 40 Batch 600 Loss 0.0361\n",
            "Epoch 40 Batch 700 Loss 0.0307\n",
            "Epoch 40 Batch 800 Loss 0.0600\n",
            "Epoch 40 Batch 900 Loss 0.0414\n",
            "Epoch 40 Batch 1000 Loss 0.0350\n",
            "Epoch 40 Batch 1100 Loss 0.0602\n",
            "Epoch 40 Batch 1200 Loss 0.0459\n",
            "Epoch 40 Loss 0.0410\n",
            "Time taken for 1 epoch 102.19638133049011 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0330\n",
            "Epoch 41 Batch 100 Loss 0.0199\n",
            "Epoch 41 Batch 200 Loss 0.0166\n",
            "Epoch 41 Batch 300 Loss 0.0398\n",
            "Epoch 41 Batch 400 Loss 0.0416\n",
            "Epoch 41 Batch 500 Loss 0.0222\n",
            "Epoch 41 Batch 600 Loss 0.0373\n",
            "Epoch 41 Batch 700 Loss 0.0335\n",
            "Epoch 41 Batch 800 Loss 0.0798\n",
            "Epoch 41 Batch 900 Loss 0.0302\n",
            "Epoch 41 Batch 1000 Loss 0.0457\n",
            "Epoch 41 Batch 1100 Loss 0.0479\n",
            "Epoch 41 Batch 1200 Loss 0.0491\n",
            "Epoch 41 Loss 0.0404\n",
            "Time taken for 1 epoch 96.51112127304077 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0303\n",
            "Epoch 42 Batch 100 Loss 0.0349\n",
            "Epoch 42 Batch 200 Loss 0.0375\n",
            "Epoch 42 Batch 300 Loss 0.0272\n",
            "Epoch 42 Batch 400 Loss 0.0160\n",
            "Epoch 42 Batch 500 Loss 0.0252\n",
            "Epoch 42 Batch 600 Loss 0.0627\n",
            "Epoch 42 Batch 700 Loss 0.0204\n",
            "Epoch 42 Batch 800 Loss 0.0363\n",
            "Epoch 42 Batch 900 Loss 0.0286\n",
            "Epoch 42 Batch 1000 Loss 0.0354\n",
            "Epoch 42 Batch 1100 Loss 0.0271\n",
            "Epoch 42 Batch 1200 Loss 0.0512\n",
            "Epoch 42 Loss 0.0406\n",
            "Time taken for 1 epoch 101.69653344154358 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0250\n",
            "Epoch 43 Batch 100 Loss 0.0333\n",
            "Epoch 43 Batch 200 Loss 0.0297\n",
            "Epoch 43 Batch 300 Loss 0.0382\n",
            "Epoch 43 Batch 400 Loss 0.0432\n",
            "Epoch 43 Batch 500 Loss 0.0528\n",
            "Epoch 43 Batch 600 Loss 0.0908\n",
            "Epoch 43 Batch 700 Loss 0.0426\n",
            "Epoch 43 Batch 800 Loss 0.0808\n",
            "Epoch 43 Batch 900 Loss 0.0427\n",
            "Epoch 43 Batch 1000 Loss 0.0570\n",
            "Epoch 43 Batch 1100 Loss 0.0387\n",
            "Epoch 43 Batch 1200 Loss 0.0492\n",
            "Epoch 43 Loss 0.0411\n",
            "Time taken for 1 epoch 97.08517289161682 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0443\n",
            "Epoch 44 Batch 100 Loss 0.0226\n",
            "Epoch 44 Batch 200 Loss 0.0262\n",
            "Epoch 44 Batch 300 Loss 0.0271\n",
            "Epoch 44 Batch 400 Loss 0.0331\n",
            "Epoch 44 Batch 500 Loss 0.0547\n",
            "Epoch 44 Batch 600 Loss 0.0432\n",
            "Epoch 44 Batch 700 Loss 0.0494\n",
            "Epoch 44 Batch 800 Loss 0.0551\n",
            "Epoch 44 Batch 900 Loss 0.0278\n",
            "Epoch 44 Batch 1000 Loss 0.0531\n",
            "Epoch 44 Batch 1100 Loss 0.0669\n",
            "Epoch 44 Batch 1200 Loss 0.0428\n",
            "Epoch 44 Loss 0.0397\n",
            "Time taken for 1 epoch 102.10774302482605 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0250\n",
            "Epoch 45 Batch 100 Loss 0.0160\n",
            "Epoch 45 Batch 200 Loss 0.0392\n",
            "Epoch 45 Batch 300 Loss 0.0444\n",
            "Epoch 45 Batch 400 Loss 0.0442\n",
            "Epoch 45 Batch 500 Loss 0.0536\n",
            "Epoch 45 Batch 600 Loss 0.0763\n",
            "Epoch 45 Batch 700 Loss 0.0274\n",
            "Epoch 45 Batch 800 Loss 0.0419\n",
            "Epoch 45 Batch 900 Loss 0.0450\n",
            "Epoch 45 Batch 1000 Loss 0.0274\n",
            "Epoch 45 Batch 1100 Loss 0.0523\n",
            "Epoch 45 Batch 1200 Loss 0.0469\n",
            "Epoch 45 Loss 0.0387\n",
            "Time taken for 1 epoch 96.68632364273071 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0198\n",
            "Epoch 46 Batch 100 Loss 0.0369\n",
            "Epoch 46 Batch 200 Loss 0.0223\n",
            "Epoch 46 Batch 300 Loss 0.0383\n",
            "Epoch 46 Batch 400 Loss 0.0608\n",
            "Epoch 46 Batch 500 Loss 0.0444\n",
            "Epoch 46 Batch 600 Loss 0.0259\n",
            "Epoch 46 Batch 700 Loss 0.0383\n",
            "Epoch 46 Batch 800 Loss 0.0361\n",
            "Epoch 46 Batch 900 Loss 0.0279\n",
            "Epoch 46 Batch 1000 Loss 0.0383\n",
            "Epoch 46 Batch 1100 Loss 0.0656\n",
            "Epoch 46 Batch 1200 Loss 0.0437\n",
            "Epoch 46 Loss 0.0392\n",
            "Time taken for 1 epoch 98.17755174636841 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0256\n",
            "Epoch 47 Batch 100 Loss 0.0215\n",
            "Epoch 47 Batch 200 Loss 0.0302\n",
            "Epoch 47 Batch 300 Loss 0.0231\n",
            "Epoch 47 Batch 400 Loss 0.0358\n",
            "Epoch 47 Batch 500 Loss 0.0418\n",
            "Epoch 47 Batch 600 Loss 0.0321\n",
            "Epoch 47 Batch 700 Loss 0.0437\n",
            "Epoch 47 Batch 800 Loss 0.0299\n",
            "Epoch 47 Batch 900 Loss 0.0418\n",
            "Epoch 47 Batch 1000 Loss 0.0446\n",
            "Epoch 47 Batch 1100 Loss 0.0323\n",
            "Epoch 47 Batch 1200 Loss 0.0278\n",
            "Epoch 47 Loss 0.0388\n",
            "Time taken for 1 epoch 96.38332033157349 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0227\n",
            "Epoch 48 Batch 100 Loss 0.0272\n",
            "Epoch 48 Batch 200 Loss 0.0269\n",
            "Epoch 48 Batch 300 Loss 0.0381\n",
            "Epoch 48 Batch 400 Loss 0.0351\n",
            "Epoch 48 Batch 500 Loss 0.0224\n",
            "Epoch 48 Batch 600 Loss 0.0429\n",
            "Epoch 48 Batch 700 Loss 0.0314\n",
            "Epoch 48 Batch 800 Loss 0.0479\n",
            "Epoch 48 Batch 900 Loss 0.0324\n",
            "Epoch 48 Batch 1000 Loss 0.0455\n",
            "Epoch 48 Batch 1100 Loss 0.0547\n",
            "Epoch 48 Batch 1200 Loss 0.0647\n",
            "Epoch 48 Loss 0.0394\n",
            "Time taken for 1 epoch 102.4483711719513 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0388\n",
            "Epoch 49 Batch 100 Loss 0.0446\n",
            "Epoch 49 Batch 200 Loss 0.0236\n",
            "Epoch 49 Batch 300 Loss 0.0286\n",
            "Epoch 49 Batch 400 Loss 0.0324\n",
            "Epoch 49 Batch 500 Loss 0.0444\n",
            "Epoch 49 Batch 600 Loss 0.0761\n",
            "Epoch 49 Batch 700 Loss 0.0334\n",
            "Epoch 49 Batch 800 Loss 0.0399\n",
            "Epoch 49 Batch 900 Loss 0.0388\n",
            "Epoch 49 Batch 1000 Loss 0.0471\n",
            "Epoch 49 Batch 1100 Loss 0.0878\n",
            "Epoch 49 Batch 1200 Loss 0.0510\n",
            "Epoch 49 Loss 0.0389\n",
            "Time taken for 1 epoch 96.54748153686523 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0166\n",
            "Epoch 50 Batch 100 Loss 0.0264\n",
            "Epoch 50 Batch 200 Loss 0.0268\n",
            "Epoch 50 Batch 300 Loss 0.0413\n",
            "Epoch 50 Batch 400 Loss 0.0258\n",
            "Epoch 50 Batch 500 Loss 0.0407\n",
            "Epoch 50 Batch 600 Loss 0.0430\n",
            "Epoch 50 Batch 700 Loss 0.0342\n",
            "Epoch 50 Batch 800 Loss 0.0312\n",
            "Epoch 50 Batch 900 Loss 0.0441\n",
            "Epoch 50 Batch 1000 Loss 0.0524\n",
            "Epoch 50 Batch 1100 Loss 0.0555\n",
            "Epoch 50 Batch 1200 Loss 0.0543\n",
            "Epoch 50 Loss 0.0387\n",
            "Time taken for 1 epoch 99.09927940368652 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#функция оценки аналогична циклу обучения, за исключением того, что здесь мы не используем принуждение учителя.\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "      attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "      attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "      result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "      if targ_lang.index_word[predicted_id] == '<end>':\n",
        "        return result, sentence, attention_plot\n",
        "\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "19j1V8vcj1Ao"
      },
      "id": "19j1V8vcj1Ao",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#функция для построения весов внимания\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZEy3jTgHj1DC"
      },
      "id": "ZEy3jTgHj1DC",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#функция перевода\n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "   # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "   # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "OYkrfSHSj1Fo"
      },
      "id": "OYkrfSHSj1Fo",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Восстановим последнюю контрольную точку и протестируем"
      ],
      "metadata": {
        "id": "pzonYbjN04AD"
      },
      "id": "pzonYbjN04AD"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1swKIUdK0ozs",
        "outputId": "ee86f6df-298e-4f6c-aa72-661d3c25a95d"
      },
      "id": "1swKIUdK0ozs",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7da518104e80>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Хороший переводчик хорошо пишет.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2VaWZ4Y0o2e",
        "outputId": "7aa960d3-f501-4ada-d643-7683f6b16c69"
      },
      "id": "h2VaWZ4Y0o2e",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> хороший переводчик хорошо пишет . <end>\n",
            "Predicted translation: it's a good work . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Как твои дела?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Vn3O630o5Q",
        "outputId": "d65be968-56f3-4cc0-912a-545c6be0baf8"
      },
      "id": "W5Vn3O630o5Q",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> как твои дела ? <end>\n",
            "Predicted translation: how are you doing ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Я никогда такого не делаю.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eazDl4nR1Bx5",
        "outputId": "9646ece7-52cf-4fa4-ac51-fb152fc7cdf4"
      },
      "id": "eazDl4nR1Bx5",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> я никогда такого не делаю . <end>\n",
            "Predicted translation: i never do that . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Вы пойдете в кино?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUS83sqi1B0h",
        "outputId": "2dbf60c8-4aef-48c8-d518-255d5348e9e1"
      },
      "id": "GUS83sqi1B0h",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> вы пойдете в кино ? <end>\n",
            "Predicted translation: will you see a movie ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Сегодня плохая погода.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIdrRzlJj1IA",
        "outputId": "6f34c65d-a392-4c77-a960-306fe6db4805"
      },
      "id": "XIdrRzlJj1IA",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> сегодня плохая погода . <end>\n",
            "Predicted translation: it's a bad today . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты работы нейросети переводчика можно признать удовлетворительными, примерно две трети из ста точности попаданий в контекст. Входными данными декодера на каждом временном шаге являются его предыдущие прогнозы, а также скрытое состояние и выходные данные кодера. Когда модель предсказывает конечный токен, прогнорз останавливается, а веса внимания сохраняются для каждого временного шага. Выходной сигнал энкодера рассчитывается только один раз для одного входа."
      ],
      "metadata": {
        "id": "t0i_O7-a3wz6"
      },
      "id": "t0i_O7-a3wz6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}