{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d747ec6",
   "metadata": {
    "id": "5d747ec6"
   },
   "source": [
    "### 5. Part-of-Speech разметка, NER, извлечение отношений\n",
    "\n",
    "Задание 1. Написать теггер на данных с русским языком\n",
    "проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "сравнить все реализованные методы, сделать выводы  \n",
    "\n",
    "\n",
    "Задание 2. Проверить, насколько хорошо работает NER\n",
    "Данные брать из Index of /pub/named_entities\n",
    "проверить NER из nltk/spacy/deeppavlov.\n",
    "написать свой NER, попробовать разные подходы.\n",
    "передаём в сетку токен и его соседей.\n",
    "передаём в сетку только токен.\n",
    "свой вариант.\n",
    "сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93461b5",
   "metadata": {
    "id": "f93461b5"
   },
   "source": [
    "#### Загрузим необходимые библиотеки и данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b1378d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42b1378d",
    "outputId": "e8818e13-8f00-43fd-e06f-383371bfd178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\n",
      "  Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae4387",
   "metadata": {
    "id": "21ae4387"
   },
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db67df4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db67df4a",
    "outputId": "4f1e90bf-c8bb-4b8e-efd4-c9f4f3636029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-05 14:00:12--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40736581 (39M) [text/plain]\n",
      "Saving to: ‘ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "ru_syntagrus-ud-tra 100%[===================>]  38.85M   256MB/s    in 0.2s    \n",
      "\n",
      "2023-10-05 14:00:12 (256 MB/s) - ‘ru_syntagrus-ud-train.conllu’ saved [40736581/40736581]\n",
      "\n",
      "--2023-10-05 14:00:12--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14704579 (14M) [text/plain]\n",
      "Saving to: ‘ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "ru_syntagrus-ud-dev 100%[===================>]  14.02M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2023-10-05 14:00:13 (228 MB/s) - ‘ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5b9ffc",
   "metadata": {
    "id": "dd5b9ffc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger, RegexpTagger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9986ce",
   "metadata": {
    "id": "0a9986ce"
   },
   "outputs": [],
   "source": [
    "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "JDQ9-ZFesYTu",
   "metadata": {
    "id": "JDQ9-ZFesYTu"
   },
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "\n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "\n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c338cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95c338cd",
    "outputId": "53281f54-f69f-404f-aa25-5be323d92a84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24516, 8906, 8906)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "znLJk2GEsp1t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znLJk2GEsp1t",
    "outputId": "1a28ceb7-a7f8-4da9-bafb-4b52ac015903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Анкета', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Начальник', 'NOUN'),\n",
       "  ('областного', 'ADJ'),\n",
       "  ('управления', 'NOUN'),\n",
       "  ('связи', 'NOUN'),\n",
       "  ('Семен', 'PROPN'),\n",
       "  ('Еремеевич', 'PROPN'),\n",
       "  ('был', 'AUX'),\n",
       "  ('человек', 'NOUN'),\n",
       "  ('простой', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('приходил', 'VERB'),\n",
       "  ('на', 'ADP'),\n",
       "  ('работу', 'NOUN'),\n",
       "  ('всегда', 'ADV'),\n",
       "  ('вовремя', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('здоровался', 'VERB'),\n",
       "  ('с', 'ADP'),\n",
       "  ('секретаршей', 'NOUN'),\n",
       "  ('за', 'ADP'),\n",
       "  ('руку', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('иногда', 'ADV'),\n",
       "  ('даже', 'PART'),\n",
       "  ('писал', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('стенгазету', 'NOUN'),\n",
       "  ('заметки', 'NOUN'),\n",
       "  ('под', 'ADP'),\n",
       "  ('псевдонимом', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('Муха', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yyHFhoJvs53x",
   "metadata": {
    "id": "yyHFhoJvs53x"
   },
   "source": [
    "#### Проверим работу всех теггеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Ct2pshNqsqYh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct2pshNqsqYh",
    "outputId": "52cf711c-13b7-4315-c0df-070a32d95e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Default Tagger: 0.0,\n",
      "Unigram Tagger: 0.824,\n",
      "Bigram Tagger: 0.60939,\n",
      "Trigram Tagger: 0.178,\n",
      "Bigram and Unigram Tagger: 0.82928,\n",
      "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_acc = default_tagger.evaluate(fdata_test)\n",
    "\n",
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'Accuracy:\\nDefault Tagger: {round(default_acc, 3)},\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
    "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
    "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XL6Qlh_RtKBw",
   "metadata": {
    "id": "XL6Qlh_RtKBw"
   },
   "source": [
    "Объединим работу всех теггеров с помощью функци"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5prGaSfns_rw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5prGaSfns_rw",
    "outputId": "94a1bc2b-571e-4f19-a199-60f74e21c604"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827905462595221"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def union_taggers(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN')\n",
    "tag = union_taggers(fdata_train,\n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff = backoff)\n",
    "\n",
    "tag.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KUauBoLttceF",
   "metadata": {
    "id": "KUauBoLttceF"
   },
   "source": [
    "#### Пишем кастомный tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "D_cMZxqns_uv",
   "metadata": {
    "id": "D_cMZxqns_uv"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "y_BUk-7LtXFt",
   "metadata": {
    "id": "y_BUk-7LtXFt"
   },
   "outputs": [],
   "source": [
    "#создадим списки слов и списки POS-разметки\n",
    "\n",
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "\n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gR0NAcD1tXIx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR0NAcD1tXIx",
    "outputId": "7f15fbb5-52b0-4b38-d07e-b13e4a976b41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета',\n",
       "  '.',\n",
       "  'Начальник',\n",
       "  'областного',\n",
       "  'управления',\n",
       "  'связи',\n",
       "  'Семен',\n",
       "  'Еремеевич',\n",
       "  'был',\n",
       "  'человек'],\n",
       " ['NOUN',\n",
       "  'PUNCT',\n",
       "  'NOUN',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'NOUN',\n",
       "  'PROPN',\n",
       "  'PROPN',\n",
       "  'AUX',\n",
       "  'NOUN'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:10], train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "zEjBxvkfs_xo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEjBxvkfs_xo",
    "outputId": "4c092405-3ec4-4f47-ab51-fab910a2a7e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  7, ...,  1, 11, 13])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "train_enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orei-kDPLsKR",
   "metadata": {
    "id": "orei-kDPLsKR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "P3r9_JrALsK9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3r9_JrALsK9",
    "outputId": "52a27f17-7933-47d8-abcc-527f7f6063ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  1, ...,  0,  7, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_labels = le.transform(test_label)\n",
    "test_enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "w8KD_pFTIc9-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8KD_pFTIc9-",
    "outputId": "ce36812c-20ea-4712-8e6d-f4d6618ec849"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#получившиеся классы\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I1CUxz-vKXG8",
   "metadata": {
    "id": "I1CUxz-vKXG8"
   },
   "source": [
    "Проведем веткоризацию и обучим модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "I4RUoim3IdFW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4RUoim3IdFW",
    "outputId": "c18ff40b-65a7-4c5c-9cad-a41ca7138944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 27.9 µs\n",
      "CountVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.94      0.92      0.93     15103\n",
      "         ADP       0.98      1.00      0.99     13717\n",
      "         ADV       0.91      0.93      0.92      7783\n",
      "         AUX       0.82      0.96      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.83      0.79      0.81      4265\n",
      "        INTJ       0.39      0.29      0.33        24\n",
      "        NOUN       0.94      0.97      0.96     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.86      0.89      0.88      1734\n",
      "        PART       0.95      0.77      0.85      5125\n",
      "        PRON       0.90      0.84      0.87      7444\n",
      "       PROPN       0.84      0.66      0.73      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.75      0.97      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.97      0.96      0.96     17110\n",
      "           X       0.43      0.07      0.13       134\n",
      "\n",
      "    accuracy                           0.94    153590\n",
      "   macro avg       0.86      0.81      0.82    153590\n",
      "weighted avg       0.94      0.94      0.94    153590\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.91      0.92     15103\n",
      "         ADP       0.99      1.00      0.99     13717\n",
      "         ADV       0.91      0.89      0.90      7783\n",
      "         AUX       0.82      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.89      0.70      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.91      0.97      0.94     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.90      0.87      1734\n",
      "        PART       0.94      0.78      0.85      5125\n",
      "        PRON       0.86      0.88      0.87      7444\n",
      "       PROPN       0.81      0.54      0.65      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.76      0.97      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.95      0.93      0.94     17110\n",
      "           X       0.24      0.13      0.17       134\n",
      "\n",
      "    accuracy                           0.93    153590\n",
      "   macro avg       0.82      0.79      0.80    153590\n",
      "weighted avg       0.93      0.93      0.93    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.82      0.80      0.81     15103\n",
      "         ADP       0.97      0.99      0.98     13717\n",
      "         ADV       0.81      0.79      0.80      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.87      1.00      0.93      5672\n",
      "         DET       0.83      0.76      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.81      0.89      0.85     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.80      0.81      1734\n",
      "        PART       0.96      0.74      0.83      5125\n",
      "        PRON       0.84      0.87      0.86      7444\n",
      "       PROPN       0.65      0.38      0.48      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.80      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.86      0.80      0.83     17110\n",
      "           X       0.26      0.04      0.08       134\n",
      "\n",
      "    accuracy                           0.87    153590\n",
      "   macro avg       0.78      0.73      0.75    153590\n",
      "weighted avg       0.87      0.87      0.87    153590\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.41      0.57     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.91      0.77      0.84      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.89      0.20      0.33      5672\n",
      "         DET       0.88      0.62      0.73      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.87      0.55      0.68      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.83      0.81      0.82      7444\n",
      "       PROPN       0.92      0.16      0.27      5473\n",
      "       PUNCT       0.37      1.00      0.54     29186\n",
      "       SCONJ       0.72      0.84      0.77      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.43      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.64    153590\n",
      "   macro avg       0.67      0.47      0.51    153590\n",
      "weighted avg       0.83      0.64      0.65    153590\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.41      0.57     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.94      0.78      0.85      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.88      0.20      0.33      5672\n",
      "         DET       0.91      0.61      0.73      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.88      0.55      0.68      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.79      0.83      0.81      7444\n",
      "       PROPN       0.90      0.16      0.28      5473\n",
      "       PUNCT       0.37      1.00      0.54     29186\n",
      "       SCONJ       0.76      0.86      0.80      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.47      0.63     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.64    153590\n",
      "   macro avg       0.67      0.48      0.52    153590\n",
      "weighted avg       0.83      0.64      0.65    153590\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.41      0.20      0.27     15103\n",
      "         ADP       0.83      0.47      0.60     13717\n",
      "         ADV       0.56      0.62      0.59      7783\n",
      "         AUX       0.72      0.86      0.78      1390\n",
      "       CCONJ       0.88      0.18      0.29      5672\n",
      "         DET       0.49      0.65      0.56      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.25      0.53      0.34     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.39      0.43      0.41      1734\n",
      "        PART       0.84      0.75      0.79      5125\n",
      "        PRON       0.68      0.68      0.68      7444\n",
      "       PROPN       0.28      0.08      0.12      5473\n",
      "       PUNCT       0.00      0.00      0.00     29186\n",
      "       SCONJ       0.66      0.96      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.45      0.25      0.32     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.36    153590\n",
      "   macro avg       0.41      0.37      0.36    153590\n",
      "weighted avg       0.39      0.36      0.34    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.86      0.86      0.86     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.87      0.82      0.84      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.83      0.77      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.85      0.92      0.89     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.85      0.84      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.85      0.87      0.86      7444\n",
      "       PROPN       0.69      0.41      0.52      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.89      0.87      0.88     17110\n",
      "           X       0.30      0.04      0.08       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.80      0.75      0.76    153590\n",
      "weighted avg       0.89      0.90      0.89    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.88      0.84      0.86      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.88      0.98      0.93      5672\n",
      "         DET       0.85      0.76      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.87      0.93      0.90     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.83      0.84      1734\n",
      "        PART       0.94      0.76      0.84      5125\n",
      "        PRON       0.83      0.87      0.85      7444\n",
      "       PROPN       0.73      0.42      0.53      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.82      0.90        62\n",
      "        VERB       0.89      0.88      0.89     17110\n",
      "           X       0.53      0.06      0.11       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.82      0.76      0.77    153590\n",
      "weighted avg       0.90      0.90      0.90    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.89      0.89     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.90      0.84      0.87      7783\n",
      "         AUX       0.82      0.97      0.89      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.88      0.72      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.88      0.95      0.91     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.84      0.87      0.85      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.83      0.90      0.86      7444\n",
      "       PROPN       0.79      0.46      0.58      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.92      0.91      0.91     17110\n",
      "           X       0.29      0.07      0.12       134\n",
      "\n",
      "    accuracy                           0.91    153590\n",
      "   macro avg       0.81      0.77      0.78    153590\n",
      "weighted avg       0.91      0.91      0.91    153590\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.47      0.25      0.32     15103\n",
      "         ADP       0.89      0.47      0.62     13717\n",
      "         ADV       0.66      0.67      0.66      7783\n",
      "         AUX       0.75      0.94      0.84      1390\n",
      "       CCONJ       0.89      0.18      0.31      5672\n",
      "         DET       0.64      0.57      0.60      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.60      0.57      0.59     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.50      0.48      0.49      1734\n",
      "        PART       0.91      0.74      0.82      5125\n",
      "        PRON       0.71      0.79      0.75      7444\n",
      "       PROPN       0.37      0.09      0.15      5473\n",
      "       PUNCT       0.48      1.00      0.65     29186\n",
      "       SCONJ       0.76      0.90      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.54      0.30      0.38     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.58    153590\n",
      "   macro avg       0.51      0.44      0.44    153590\n",
      "weighted avg       0.61      0.58      0.55    153590\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.53      0.28      0.36     15103\n",
      "         ADP       0.91      0.47      0.62     13717\n",
      "         ADV       0.75      0.69      0.72      7783\n",
      "         AUX       0.77      0.94      0.85      1390\n",
      "       CCONJ       0.93      0.18      0.30      5672\n",
      "         DET       0.71      0.53      0.61      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.64      0.59      0.61     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.55      0.53      0.54      1734\n",
      "        PART       0.88      0.78      0.83      5125\n",
      "        PRON       0.74      0.82      0.78      7444\n",
      "       PROPN       0.43      0.11      0.18      5473\n",
      "       PUNCT       0.46      1.00      0.63     29186\n",
      "       SCONJ       0.73      0.95      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.59      0.32      0.41     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.59    153590\n",
      "   macro avg       0.54      0.45      0.46    153590\n",
      "weighted avg       0.64      0.59      0.57    153590\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.60      0.31      0.41     15103\n",
      "         ADP       0.92      0.48      0.63     13717\n",
      "         ADV       0.80      0.70      0.75      7783\n",
      "         AUX       0.81      0.86      0.83      1390\n",
      "       CCONJ       0.85      0.20      0.33      5672\n",
      "         DET       0.77      0.55      0.64      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.70      0.58      0.64     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.63      0.49      0.55      1734\n",
      "        PART       0.96      0.72      0.82      5125\n",
      "        PRON       0.74      0.84      0.79      7444\n",
      "       PROPN       0.51      0.14      0.22      5473\n",
      "       PUNCT       0.43      1.00      0.60     29186\n",
      "       SCONJ       0.76      0.91      0.83      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.65      0.35      0.46     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.60    153590\n",
      "   macro avg       0.56      0.45      0.47    153590\n",
      "weighted avg       0.67      0.60      0.58    153590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "vectorizers = [CountVectorizer(ngram_range=(1, 5), analyzer='char'),\n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'),\n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000)]\n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 5), analyzer='word'),\n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='word'),\n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000)]\n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "\n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels, pred)\n",
    "    accuracy_scores.append(acc)\n",
    "\n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Kep_e82MEeH",
   "metadata": {
    "id": "1Kep_e82MEeH"
   },
   "source": [
    "Соберем данные обучения в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Kr2puGpoKNbT",
   "metadata": {
    "id": "Kr2puGpoKNbT"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "PDyc_3yDKNdy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "PDyc_3yDKNdy",
    "outputId": "2fa23118-cd6b-4c22-cb1f-894920c5a3a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e1499a0a-9f57-43e4-8aa4-48fcbffbe223\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.938119</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.926624</td>\n",
       "      <td>0.929012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.912540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.898547</td>\n",
       "      <td>0.902070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.892840</td>\n",
       "      <td>0.896243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.866679</td>\n",
       "      <td>0.870441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.654096</td>\n",
       "      <td>0.643336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.648576</td>\n",
       "      <td>0.637971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.598294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.567710</td>\n",
       "      <td>0.592669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.550497</td>\n",
       "      <td>0.578866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.341387</td>\n",
       "      <td>0.360648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1499a0a-9f57-43e4-8aa4-48fcbffbe223')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e1499a0a-9f57-43e4-8aa4-48fcbffbe223 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e1499a0a-9f57-43e4-8aa4-48fcbffbe223');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-e3b2ba86-196c-461e-ac9e-a4520c8dd807\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3b2ba86-196c-461e-ac9e-a4520c8dd807')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-e3b2ba86-196c-461e-ac9e-a4520c8dd807 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                 Vectorizer  f1_score  Accuracy\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.938119  0.939469\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.926624  0.929012\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.909308  0.912540\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.898547  0.902070\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.892840  0.896243\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.866679  0.870441\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.654096  0.643336\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.648576  0.637971\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.581928  0.598294\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.567710  0.592669\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.550497  0.578866\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.341387  0.360648"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'f1_score': f1_scores, 'Accuracy': accuracy_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6oHgd1Ocyv",
   "metadata": {
    "id": "dc6oHgd1Ocyv"
   },
   "source": [
    "Вывод: векторайзеры на N-граммах отработали лучше, символьные показали наилучший результат, чем словарные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3wzqbxjOyEP",
   "metadata": {
    "id": "O3wzqbxjOyEP"
   },
   "source": [
    "### Задание 2. Проверить, насколько хорошо работает NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T7rFN9POPwbS",
   "metadata": {
    "id": "T7rFN9POPwbS"
   },
   "source": [
    "Установим и имоптируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "r8OJGwmSMQuv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8OJGwmSMQuv",
    "outputId": "636a0e45-df45-4164-f4e0-4e6fc09ff2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting corus\n",
      "  Downloading corus-0.10.0-py3-none-any.whl (83 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m61.4/83.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: corus\n",
      "Successfully installed corus-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "CQFyofskIdMP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQFyofskIdMP",
    "outputId": "f5d1659f-a75d-41b7-ffd2-d15a8b00a97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting razdel\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: razdel\n",
      "Successfully installed razdel-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "yPZvoNB9PcXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPZvoNB9PcXf",
    "outputId": "98e7818a-3edd-404b-b452-3eca9d49c216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.16.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.15.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: cloudpathlib, weasel, spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.6.1\n",
      "    Uninstalling spacy-3.6.1:\n",
      "      Successfully uninstalled spacy-3.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpathlib-0.15.1 spacy-3.7.1 weasel-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "zu2lN8Z-PcbC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zu2lN8Z-PcbC",
    "outputId": "02eb8a0c-8cb9-4b14-d489-f31cff54ddf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 14:29:39.842366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 14:29:43.907384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.7.1                         \n",
      "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
      "Platform         Linux-5.15.120+-x86_64-with-glibc2.35\n",
      "Python version   3.10.12                       \n",
      "Pipelines        en_core_web_sm (3.6.0)        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ph0OhM89PceZ",
   "metadata": {
    "id": "ph0OhM89PceZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "import pyconll\n",
    "import corus\n",
    "from corus import load_ne5\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "Ra_fpVH5PsBA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ra_fpVH5PsBA",
    "outputId": "2d4b645c-ba76-4565-d716-8d476a05fefa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#подтянем словари\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "Q65k32aSPsDu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q65k32aSPsDu",
    "outputId": "3a752bb9-a462-4d30-8e92-e4d9fd4d43e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01UVJaXHPsG4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01UVJaXHPsG4",
    "outputId": "adab6bad-423a-4a0f-d890-f9656a35c053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-05 14:32:34--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
      "Resolving www.labinform.ru (www.labinform.ru)... 95.181.230.181\n",
      "Connecting to www.labinform.ru (www.labinform.ru)|95.181.230.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1899530 (1.8M) [application/zip]\n",
      "Saving to: ‘collection5.zip’\n",
      "\n",
      "collection5.zip     100%[===================>]   1.81M  2.28MB/s    in 0.8s    \n",
      "\n",
      "2023-10-05 14:32:36 (2.28 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "-pcGIF4AQT2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pcGIF4AQT2d",
    "outputId": "38d0ceb1-3bf5-4200-84e9-127c53eeb5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  collection5.zip\n",
      "   creating: Collection5/\n",
      "  inflating: Collection5/001.ann     \n",
      "  inflating: Collection5/001.txt     \n",
      "  inflating: Collection5/002.ann     \n",
      "  inflating: Collection5/002.txt     \n",
      "  inflating: Collection5/003.ann     \n",
      "  inflating: Collection5/003.txt     \n",
      "  inflating: Collection5/004.ann     \n",
      "  inflating: Collection5/004.txt     \n",
      "  inflating: Collection5/005.ann     \n",
      "  inflating: Collection5/005.txt     \n",
      "  inflating: Collection5/006.ann     \n",
      "  inflating: Collection5/006.txt     \n",
      "  inflating: Collection5/007.ann     \n",
      "  inflating: Collection5/007.txt     \n",
      "  inflating: Collection5/008.ann     \n",
      "  inflating: Collection5/008.txt     \n",
      "  inflating: Collection5/009.ann     \n",
      "  inflating: Collection5/009.txt     \n",
      "  inflating: Collection5/010.ann     \n",
      "  inflating: Collection5/010.txt     \n",
      "  inflating: Collection5/011.ann     \n",
      "  inflating: Collection5/011.txt     \n",
      "  inflating: Collection5/012.ann     \n",
      "  inflating: Collection5/012.txt     \n",
      "  inflating: Collection5/013.ann     \n",
      "  inflating: Collection5/013.txt     \n",
      "  inflating: Collection5/014.ann     \n",
      "  inflating: Collection5/014.txt     \n",
      "  inflating: Collection5/015 (!).ann  \n",
      "  inflating: Collection5/015 (!).txt  \n",
      "  inflating: Collection5/016.ann     \n",
      "  inflating: Collection5/016.txt     \n",
      "  inflating: Collection5/017.ann     \n",
      "  inflating: Collection5/017.txt     \n",
      "  inflating: Collection5/018.ann     \n",
      "  inflating: Collection5/018.txt     \n",
      "  inflating: Collection5/019.ann     \n",
      "  inflating: Collection5/019.txt     \n",
      "  inflating: Collection5/020.ann     \n",
      "  inflating: Collection5/020.txt     \n",
      "  inflating: Collection5/021.ann     \n",
      "  inflating: Collection5/021.txt     \n",
      "  inflating: Collection5/022.ann     \n",
      "  inflating: Collection5/022.txt     \n",
      "  inflating: Collection5/023.ann     \n",
      "  inflating: Collection5/023.txt     \n",
      "  inflating: Collection5/025.ann     \n",
      "  inflating: Collection5/025.txt     \n",
      "  inflating: Collection5/026.ann     \n",
      "  inflating: Collection5/026.txt     \n",
      "  inflating: Collection5/027.ann     \n",
      "  inflating: Collection5/027.txt     \n",
      "  inflating: Collection5/028.ann     \n",
      "  inflating: Collection5/028.txt     \n",
      "  inflating: Collection5/029.ann     \n",
      "  inflating: Collection5/029.txt     \n",
      "  inflating: Collection5/030.ann     \n",
      "  inflating: Collection5/030.txt     \n",
      "  inflating: Collection5/031.ann     \n",
      "  inflating: Collection5/031.txt     \n",
      "  inflating: Collection5/032.ann     \n",
      "  inflating: Collection5/032.txt     \n",
      "  inflating: Collection5/033.ann     \n",
      "  inflating: Collection5/033.txt     \n",
      "  inflating: Collection5/034.ann     \n",
      "  inflating: Collection5/034.txt     \n",
      "  inflating: Collection5/035.ann     \n",
      "  inflating: Collection5/035.txt     \n",
      "  inflating: Collection5/036.ann     \n",
      "  inflating: Collection5/036.txt     \n",
      "  inflating: Collection5/037.ann     \n",
      "  inflating: Collection5/037.txt     \n",
      "  inflating: Collection5/038.ann     \n",
      "  inflating: Collection5/038.txt     \n",
      "  inflating: Collection5/039.ann     \n",
      "  inflating: Collection5/039.txt     \n",
      "  inflating: Collection5/03_12_12a.ann  \n",
      "  inflating: Collection5/03_12_12a.txt  \n",
      "  inflating: Collection5/03_12_12b.ann  \n",
      "  inflating: Collection5/03_12_12b.txt  \n",
      "  inflating: Collection5/03_12_12c.ann  \n",
      "  inflating: Collection5/03_12_12c.txt  \n",
      "  inflating: Collection5/03_12_12d.ann  \n",
      "  inflating: Collection5/03_12_12d.txt  \n",
      "  inflating: Collection5/03_12_12g.ann  \n",
      "  inflating: Collection5/03_12_12g.txt  \n",
      "  inflating: Collection5/03_12_12h.ann  \n",
      "  inflating: Collection5/03_12_12h.txt  \n",
      "  inflating: Collection5/040.ann     \n",
      "  inflating: Collection5/040.txt     \n",
      "  inflating: Collection5/041.ann     \n",
      "  inflating: Collection5/041.txt     \n",
      "  inflating: Collection5/042.ann     \n",
      "  inflating: Collection5/042.txt     \n",
      "  inflating: Collection5/043.ann     \n",
      "  inflating: Collection5/043.txt     \n",
      "  inflating: Collection5/044.ann     \n",
      "  inflating: Collection5/044.txt     \n",
      "  inflating: Collection5/045.ann     \n",
      "  inflating: Collection5/045.txt     \n",
      "  inflating: Collection5/046.ann     \n",
      "  inflating: Collection5/046.txt     \n",
      "  inflating: Collection5/047.ann     \n",
      "  inflating: Collection5/047.txt     \n",
      "  inflating: Collection5/048.ann     \n",
      "  inflating: Collection5/048.txt     \n",
      "  inflating: Collection5/049.ann     \n",
      "  inflating: Collection5/049.txt     \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.ann  \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.txt  \n",
      "  inflating: Collection5/04_03_13a_sorokin.ann  \n",
      "  inflating: Collection5/04_03_13a_sorokin.txt  \n",
      "  inflating: Collection5/04_12_12b.ann  \n",
      "  inflating: Collection5/04_12_12b.txt  \n",
      "  inflating: Collection5/04_12_12d.ann  \n",
      "  inflating: Collection5/04_12_12d.txt  \n",
      "  inflating: Collection5/04_12_12f.ann  \n",
      "  inflating: Collection5/04_12_12f.txt  \n",
      "  inflating: Collection5/04_12_12g.ann  \n",
      "  inflating: Collection5/04_12_12g.txt  \n",
      "  inflating: Collection5/04_12_12h_corr.ann  \n",
      "  inflating: Collection5/04_12_12h_corr.txt  \n",
      "  inflating: Collection5/050.ann     \n",
      "  inflating: Collection5/050.txt     \n",
      "  inflating: Collection5/051.ann     \n",
      "  inflating: Collection5/051.txt     \n",
      "  inflating: Collection5/052.ann     \n",
      "  inflating: Collection5/052.txt     \n",
      "  inflating: Collection5/053.ann     \n",
      "  inflating: Collection5/053.txt     \n",
      "  inflating: Collection5/054.ann     \n",
      "  inflating: Collection5/054.txt     \n",
      "  inflating: Collection5/055.ann     \n",
      "  inflating: Collection5/055.txt     \n",
      "  inflating: Collection5/056.ann     \n",
      "  inflating: Collection5/056.txt     \n",
      "  inflating: Collection5/057.ann     \n",
      "  inflating: Collection5/057.txt     \n",
      "  inflating: Collection5/058.ann     \n",
      "  inflating: Collection5/058.txt     \n",
      "  inflating: Collection5/059.ann     \n",
      "  inflating: Collection5/059.txt     \n",
      "  inflating: Collection5/060.ann     \n",
      "  inflating: Collection5/060.txt     \n",
      "  inflating: Collection5/061.ann     \n",
      "  inflating: Collection5/061.txt     \n",
      "  inflating: Collection5/062.ann     \n",
      "  inflating: Collection5/062.txt     \n",
      "  inflating: Collection5/063.ann     \n",
      "  inflating: Collection5/063.txt     \n",
      "  inflating: Collection5/064.ann     \n",
      "  inflating: Collection5/064.txt     \n",
      "  inflating: Collection5/065.ann     \n",
      "  inflating: Collection5/065.txt     \n",
      "  inflating: Collection5/066.ann     \n",
      "  inflating: Collection5/066.txt     \n",
      "  inflating: Collection5/067.ann     \n",
      "  inflating: Collection5/067.txt     \n",
      "  inflating: Collection5/068.ann     \n",
      "  inflating: Collection5/068.txt     \n",
      "  inflating: Collection5/069.ann     \n",
      "  inflating: Collection5/069.txt     \n",
      "  inflating: Collection5/070.ann     \n",
      "  inflating: Collection5/070.txt     \n",
      "  inflating: Collection5/071.ann     \n",
      "  inflating: Collection5/071.txt     \n",
      "  inflating: Collection5/072.ann     \n",
      "  inflating: Collection5/072.txt     \n",
      "  inflating: Collection5/073.ann     \n",
      "  inflating: Collection5/073.txt     \n",
      "  inflating: Collection5/074.ann     \n",
      "  inflating: Collection5/074.txt     \n",
      "  inflating: Collection5/075.ann     \n",
      "  inflating: Collection5/075.txt     \n",
      "  inflating: Collection5/076.ann     \n",
      "  inflating: Collection5/076.txt     \n",
      "  inflating: Collection5/077.ann     \n",
      "  inflating: Collection5/077.txt     \n",
      "  inflating: Collection5/078.ann     \n",
      "  inflating: Collection5/078.txt     \n",
      "  inflating: Collection5/079.ann     \n",
      "  inflating: Collection5/079.txt     \n",
      "  inflating: Collection5/080.ann     \n",
      "  inflating: Collection5/080.txt     \n",
      "  inflating: Collection5/081.ann     \n",
      "  inflating: Collection5/081.txt     \n",
      "  inflating: Collection5/082.ann     \n",
      "  inflating: Collection5/082.txt     \n",
      "  inflating: Collection5/083.ann     \n",
      "  inflating: Collection5/083.txt     \n",
      "  inflating: Collection5/084.ann     \n",
      "  inflating: Collection5/084.txt     \n",
      "  inflating: Collection5/085.ann     \n",
      "  inflating: Collection5/085.txt     \n",
      "  inflating: Collection5/086.ann     \n",
      "  inflating: Collection5/086.txt     \n",
      "  inflating: Collection5/087.ann     \n",
      "  inflating: Collection5/087.txt     \n",
      "  inflating: Collection5/088.ann     \n",
      "  inflating: Collection5/088.txt     \n",
      "  inflating: Collection5/089.ann     \n",
      "  inflating: Collection5/089.txt     \n",
      "  inflating: Collection5/090.ann     \n",
      "  inflating: Collection5/090.txt     \n",
      "  inflating: Collection5/091.ann     \n",
      "  inflating: Collection5/091.txt     \n",
      "  inflating: Collection5/092.ann     \n",
      "  inflating: Collection5/092.txt     \n",
      "  inflating: Collection5/093.ann     \n",
      "  inflating: Collection5/093.txt     \n",
      "  inflating: Collection5/094.ann     \n",
      "  inflating: Collection5/094.txt     \n",
      "  inflating: Collection5/095.ann     \n",
      "  inflating: Collection5/095.txt     \n",
      "  inflating: Collection5/096.ann     \n",
      "  inflating: Collection5/096.txt     \n",
      "  inflating: Collection5/097.ann     \n",
      "  inflating: Collection5/097.txt     \n",
      "  inflating: Collection5/098.ann     \n",
      "  inflating: Collection5/098.txt     \n",
      "  inflating: Collection5/099.ann     \n",
      "  inflating: Collection5/099.txt     \n",
      "  inflating: Collection5/09_01_13.ann  \n",
      "  inflating: Collection5/09_01_13.txt  \n",
      "  inflating: Collection5/09_01_13a.ann  \n",
      "  inflating: Collection5/09_01_13a.txt  \n",
      "  inflating: Collection5/09_01_13c.ann  \n",
      "  inflating: Collection5/09_01_13c.txt  \n",
      "  inflating: Collection5/09_01_13d.ann  \n",
      "  inflating: Collection5/09_01_13d.txt  \n",
      "  inflating: Collection5/09_01_13e.ann  \n",
      "  inflating: Collection5/09_01_13e.txt  \n",
      "  inflating: Collection5/09_01_13h.ann  \n",
      "  inflating: Collection5/09_01_13h.txt  \n",
      "  inflating: Collection5/09_01_13i.ann  \n",
      "  inflating: Collection5/09_01_13i.txt  \n",
      "  inflating: Collection5/100.ann     \n",
      "  inflating: Collection5/100.txt     \n",
      "  inflating: Collection5/1000.ann    \n",
      "  inflating: Collection5/1000.txt    \n",
      "  inflating: Collection5/1001.ann    \n",
      "  inflating: Collection5/1001.txt    \n",
      "  inflating: Collection5/1002.ann    \n",
      "  inflating: Collection5/1002.txt    \n",
      "  inflating: Collection5/1003.ann    \n",
      "  inflating: Collection5/1003.txt    \n",
      "  inflating: Collection5/1004.ann    \n",
      "  inflating: Collection5/1004.txt    \n",
      "  inflating: Collection5/1005.ann    \n",
      "  inflating: Collection5/1005.txt    \n",
      "  inflating: Collection5/1006.ann    \n",
      "  inflating: Collection5/1006.txt    \n",
      "  inflating: Collection5/1007.ann    \n",
      "  inflating: Collection5/1007.txt    \n",
      "  inflating: Collection5/1008.ann    \n",
      "  inflating: Collection5/1008.txt    \n",
      "  inflating: Collection5/1009.ann    \n",
      "  inflating: Collection5/1009.txt    \n",
      "  inflating: Collection5/101.ann     \n",
      "  inflating: Collection5/101.txt     \n",
      "  inflating: Collection5/1010.ann    \n",
      "  inflating: Collection5/1010.txt    \n",
      "  inflating: Collection5/1011.ann    \n",
      "  inflating: Collection5/1011.txt    \n",
      "  inflating: Collection5/1012.ann    \n",
      "  inflating: Collection5/1012.txt    \n",
      "  inflating: Collection5/1013.ann    \n",
      "  inflating: Collection5/1013.txt    \n",
      "  inflating: Collection5/1014.ann    \n",
      "  inflating: Collection5/1014.txt    \n",
      "  inflating: Collection5/1015.ann    \n",
      "  inflating: Collection5/1015.txt    \n",
      "  inflating: Collection5/1016.ann    \n",
      "  inflating: Collection5/1016.txt    \n",
      "  inflating: Collection5/1017.ann    \n",
      "  inflating: Collection5/1017.txt    \n",
      "  inflating: Collection5/1018.ann    \n",
      "  inflating: Collection5/1018.txt    \n",
      "  inflating: Collection5/1019.ann    \n",
      "  inflating: Collection5/1019.txt    \n",
      "  inflating: Collection5/102.ann     \n",
      "  inflating: Collection5/102.txt     \n",
      "  inflating: Collection5/1020.ann    \n",
      "  inflating: Collection5/1020.txt    \n",
      "  inflating: Collection5/1021.ann    \n",
      "  inflating: Collection5/1021.txt    \n",
      "  inflating: Collection5/1022.ann    \n",
      "  inflating: Collection5/1022.txt    \n",
      "  inflating: Collection5/1023.ann    \n",
      "  inflating: Collection5/1023.txt    \n",
      "  inflating: Collection5/1024.ann    \n",
      "  inflating: Collection5/1024.txt    \n",
      "  inflating: Collection5/1025.ann    \n",
      "  inflating: Collection5/1025.txt    \n",
      "  inflating: Collection5/1026.ann    \n",
      "  inflating: Collection5/1026.txt    \n",
      "  inflating: Collection5/1027.ann    \n",
      "  inflating: Collection5/1027.txt    \n",
      "  inflating: Collection5/1028.ann    \n",
      "  inflating: Collection5/1028.txt    \n",
      "  inflating: Collection5/1029.ann    \n",
      "  inflating: Collection5/1029.txt    \n",
      "  inflating: Collection5/103.ann     \n",
      "  inflating: Collection5/103.txt     \n",
      "  inflating: Collection5/1030.ann    \n",
      "  inflating: Collection5/1030.txt    \n",
      "  inflating: Collection5/1031.ann    \n",
      "  inflating: Collection5/1031.txt    \n",
      "  inflating: Collection5/1032.ann    \n",
      "  inflating: Collection5/1032.txt    \n",
      "  inflating: Collection5/1033.ann    \n",
      "  inflating: Collection5/1033.txt    \n",
      "  inflating: Collection5/1034.ann    \n",
      "  inflating: Collection5/1034.txt    \n",
      "  inflating: Collection5/1035.ann    \n",
      "  inflating: Collection5/1035.txt    \n",
      "  inflating: Collection5/1036.ann    \n",
      "  inflating: Collection5/1036.txt    \n",
      "  inflating: Collection5/1037.ann    \n",
      "  inflating: Collection5/1037.txt    \n",
      "  inflating: Collection5/1038.ann    \n",
      "  inflating: Collection5/1038.txt    \n",
      "  inflating: Collection5/1039.ann    \n",
      "  inflating: Collection5/1039.txt    \n",
      "  inflating: Collection5/104.ann     \n",
      "  inflating: Collection5/104.txt     \n",
      "  inflating: Collection5/1040.ann    \n",
      "  inflating: Collection5/1040.txt    \n",
      "  inflating: Collection5/1041.ann    \n",
      "  inflating: Collection5/1041.txt    \n",
      "  inflating: Collection5/1042.ann    \n",
      "  inflating: Collection5/1042.txt    \n",
      "  inflating: Collection5/1043.ann    \n",
      "  inflating: Collection5/1043.txt    \n",
      "  inflating: Collection5/1044.ann    \n",
      "  inflating: Collection5/1044.txt    \n",
      "  inflating: Collection5/1045.ann    \n",
      "  inflating: Collection5/1045.txt    \n",
      "  inflating: Collection5/1046.ann    \n",
      "  inflating: Collection5/1046.txt    \n",
      "  inflating: Collection5/1047.ann    \n",
      "  inflating: Collection5/1047.txt    \n",
      "  inflating: Collection5/1048.ann    \n",
      "  inflating: Collection5/1048.txt    \n",
      "  inflating: Collection5/1049.ann    \n",
      "  inflating: Collection5/1049.txt    \n",
      "  inflating: Collection5/105.ann     \n",
      "  inflating: Collection5/105.txt     \n",
      "  inflating: Collection5/1050.ann    \n",
      "  inflating: Collection5/1050.txt    \n",
      "  inflating: Collection5/106.ann     \n",
      "  inflating: Collection5/106.txt     \n",
      "  inflating: Collection5/107.ann     \n",
      "  inflating: Collection5/107.txt     \n",
      "  inflating: Collection5/108.ann     \n",
      "  inflating: Collection5/108.txt     \n",
      "  inflating: Collection5/109.ann     \n",
      "  inflating: Collection5/109.txt     \n",
      "  inflating: Collection5/10_01_13a.ann  \n",
      "  inflating: Collection5/10_01_13a.txt  \n",
      "  inflating: Collection5/10_01_13d.ann  \n",
      "  inflating: Collection5/10_01_13d.txt  \n",
      "  inflating: Collection5/10_01_13i.ann  \n",
      "  inflating: Collection5/10_01_13i.txt  \n",
      "  inflating: Collection5/110.ann     \n",
      "  inflating: Collection5/110.txt     \n",
      "  inflating: Collection5/1100.ann    \n",
      "  inflating: Collection5/1100.txt    \n",
      "  inflating: Collection5/1101.ann    \n",
      "  inflating: Collection5/1101.txt    \n",
      "  inflating: Collection5/1102.ann    \n",
      "  inflating: Collection5/1102.txt    \n",
      "  inflating: Collection5/1103.ann    \n",
      "  inflating: Collection5/1103.txt    \n",
      "  inflating: Collection5/1104.ann    \n",
      "  inflating: Collection5/1104.txt    \n",
      "  inflating: Collection5/1105.ann    \n",
      "  inflating: Collection5/1105.txt    \n",
      "  inflating: Collection5/1106.ann    \n",
      "  inflating: Collection5/1106.txt    \n",
      "  inflating: Collection5/1107.ann    \n",
      "  inflating: Collection5/1107.txt    \n",
      "  inflating: Collection5/1108.ann    \n",
      "  inflating: Collection5/1108.txt    \n",
      "  inflating: Collection5/1109.ann    \n",
      "  inflating: Collection5/1109.txt    \n",
      "  inflating: Collection5/111.ann     \n",
      "  inflating: Collection5/111.txt     \n",
      "  inflating: Collection5/1110.ann    \n",
      "  inflating: Collection5/1110.txt    \n",
      "  inflating: Collection5/1111.ann    \n",
      "  inflating: Collection5/1111.txt    \n",
      "  inflating: Collection5/1112.ann    \n",
      "  inflating: Collection5/1112.txt    \n",
      "  inflating: Collection5/1113.ann    \n",
      "  inflating: Collection5/1113.txt    \n",
      "  inflating: Collection5/1114.ann    \n",
      "  inflating: Collection5/1114.txt    \n",
      "  inflating: Collection5/1115.ann    \n",
      "  inflating: Collection5/1115.txt    \n",
      "  inflating: Collection5/1116.ann    \n",
      "  inflating: Collection5/1116.txt    \n",
      "  inflating: Collection5/1117.ann    \n",
      "  inflating: Collection5/1117.txt    \n",
      "  inflating: Collection5/1118.ann    \n",
      "  inflating: Collection5/1118.txt    \n",
      "  inflating: Collection5/1119.ann    \n",
      "  inflating: Collection5/1119.txt    \n",
      "  inflating: Collection5/112.ann     \n",
      "  inflating: Collection5/112.txt     \n",
      "  inflating: Collection5/1120.ann    \n",
      "  inflating: Collection5/1120.txt    \n",
      "  inflating: Collection5/1121.ann    \n",
      "  inflating: Collection5/1121.txt    \n",
      "  inflating: Collection5/1122.ann    \n",
      "  inflating: Collection5/1122.txt    \n",
      "  inflating: Collection5/1123.ann    \n",
      "  inflating: Collection5/1123.txt    \n",
      "  inflating: Collection5/1124.ann    \n",
      "  inflating: Collection5/1124.txt    \n",
      "  inflating: Collection5/1125.ann    \n",
      "  inflating: Collection5/1125.txt    \n",
      "  inflating: Collection5/1126.ann    \n",
      "  inflating: Collection5/1126.txt    \n",
      "  inflating: Collection5/1127.ann    \n",
      "  inflating: Collection5/1127.txt    \n",
      "  inflating: Collection5/1128.ann    \n",
      "  inflating: Collection5/1128.txt    \n",
      "  inflating: Collection5/113.ann     \n",
      "  inflating: Collection5/113.txt     \n",
      "  inflating: Collection5/1130.ann    \n",
      "  inflating: Collection5/1130.txt    \n",
      "  inflating: Collection5/1131.ann    \n",
      "  inflating: Collection5/1131.txt    \n",
      "  inflating: Collection5/1132.ann    \n",
      "  inflating: Collection5/1132.txt    \n",
      "  inflating: Collection5/1133.ann    \n",
      "  inflating: Collection5/1133.txt    \n",
      "  inflating: Collection5/1134.ann    \n",
      "  inflating: Collection5/1134.txt    \n",
      "  inflating: Collection5/1135.ann    \n",
      "  inflating: Collection5/1135.txt    \n",
      "  inflating: Collection5/1136.ann    \n",
      "  inflating: Collection5/1136.txt    \n",
      "  inflating: Collection5/1137.ann    \n",
      "  inflating: Collection5/1137.txt    \n",
      "  inflating: Collection5/1138.ann    \n",
      "  inflating: Collection5/1138.txt    \n",
      "  inflating: Collection5/1139.ann    \n",
      "  inflating: Collection5/1139.txt    \n",
      "  inflating: Collection5/114.ann     \n",
      "  inflating: Collection5/114.txt     \n",
      "  inflating: Collection5/1140.ann    \n",
      "  inflating: Collection5/1140.txt    \n",
      "  inflating: Collection5/1141.ann    \n",
      "  inflating: Collection5/1141.txt    \n",
      "  inflating: Collection5/1142.ann    \n",
      "  inflating: Collection5/1142.txt    \n",
      "  inflating: Collection5/1143.ann    \n",
      "  inflating: Collection5/1143.txt    \n",
      "  inflating: Collection5/1144.ann    \n",
      "  inflating: Collection5/1144.txt    \n",
      "  inflating: Collection5/1145.ann    \n",
      "  inflating: Collection5/1145.txt    \n",
      "  inflating: Collection5/1146.ann    \n",
      "  inflating: Collection5/1146.txt    \n",
      "  inflating: Collection5/1147.ann    \n",
      "  inflating: Collection5/1147.txt    \n",
      "  inflating: Collection5/1148.ann    \n",
      "  inflating: Collection5/1148.txt    \n",
      "  inflating: Collection5/1149.ann    \n",
      "  inflating: Collection5/1149.txt    \n",
      "  inflating: Collection5/115.ann     \n",
      "  inflating: Collection5/115.txt     \n",
      "  inflating: Collection5/1150.ann    \n",
      "  inflating: Collection5/1150.txt    \n",
      "  inflating: Collection5/1151.ann    \n",
      "  inflating: Collection5/1151.txt    \n",
      "  inflating: Collection5/1152.ann    \n",
      "  inflating: Collection5/1152.txt    \n",
      "  inflating: Collection5/1153.ann    \n",
      "  inflating: Collection5/1153.txt    \n",
      "  inflating: Collection5/1154.ann    \n",
      "  inflating: Collection5/1154.txt    \n",
      "  inflating: Collection5/1155.ann    \n",
      "  inflating: Collection5/1155.txt    \n",
      "  inflating: Collection5/1156.ann    \n",
      "  inflating: Collection5/1156.txt    \n",
      "  inflating: Collection5/1157.ann    \n",
      "  inflating: Collection5/1157.txt    \n",
      "  inflating: Collection5/1158.ann    \n",
      "  inflating: Collection5/1158.txt    \n",
      "  inflating: Collection5/1159.ann    \n",
      "  inflating: Collection5/1159.txt    \n",
      "  inflating: Collection5/116.ann     \n",
      "  inflating: Collection5/116.txt     \n",
      "  inflating: Collection5/1160.ann    \n",
      "  inflating: Collection5/1160.txt    \n",
      "  inflating: Collection5/1161.ann    \n",
      "  inflating: Collection5/1161.txt    \n",
      "  inflating: Collection5/1162.ann    \n",
      "  inflating: Collection5/1162.txt    \n",
      "  inflating: Collection5/1163.ann    \n",
      "  inflating: Collection5/1163.txt    \n",
      "  inflating: Collection5/1164.ann    \n",
      "  inflating: Collection5/1164.txt    \n",
      "  inflating: Collection5/1165.ann    \n",
      "  inflating: Collection5/1165.txt    \n",
      "  inflating: Collection5/1166.ann    \n",
      "  inflating: Collection5/1166.txt    \n",
      "  inflating: Collection5/1167.ann    \n",
      "  inflating: Collection5/1167.txt    \n",
      "  inflating: Collection5/1168.ann    \n",
      "  inflating: Collection5/1168.txt    \n",
      "  inflating: Collection5/1169.ann    \n",
      "  inflating: Collection5/1169.txt    \n",
      "  inflating: Collection5/117.ann     \n",
      "  inflating: Collection5/117.txt     \n",
      "  inflating: Collection5/1170.ann    \n",
      "  inflating: Collection5/1170.txt    \n",
      "  inflating: Collection5/1171.ann    \n",
      "  inflating: Collection5/1171.txt    \n",
      "  inflating: Collection5/1172.ann    \n",
      "  inflating: Collection5/1172.txt    \n",
      "  inflating: Collection5/1173.ann    \n",
      "  inflating: Collection5/1173.txt    \n",
      "  inflating: Collection5/1174.ann    \n",
      "  inflating: Collection5/1174.txt    \n",
      "  inflating: Collection5/1175.ann    \n",
      "  inflating: Collection5/1175.txt    \n",
      "  inflating: Collection5/1176.ann    \n",
      "  inflating: Collection5/1176.txt    \n",
      "  inflating: Collection5/1177.ann    \n",
      "  inflating: Collection5/1177.txt    \n",
      "  inflating: Collection5/1178.ann    \n",
      "  inflating: Collection5/1178.txt    \n",
      "  inflating: Collection5/1179.ann    \n",
      "  inflating: Collection5/1179.txt    \n",
      "  inflating: Collection5/118.ann     \n",
      "  inflating: Collection5/118.txt     \n",
      "  inflating: Collection5/1180.ann    \n",
      "  inflating: Collection5/1180.txt    \n",
      "  inflating: Collection5/1181.ann    \n",
      "  inflating: Collection5/1181.txt    \n",
      "  inflating: Collection5/1182.ann    \n",
      "  inflating: Collection5/1182.txt    \n",
      "  inflating: Collection5/1183.ann    \n",
      "  inflating: Collection5/1183.txt    \n",
      "  inflating: Collection5/1184.ann    \n",
      "  inflating: Collection5/1184.txt    \n",
      "  inflating: Collection5/1185.ann    \n",
      "  inflating: Collection5/1185.txt    \n",
      "  inflating: Collection5/1186.ann    \n",
      "  inflating: Collection5/1186.txt    \n",
      "  inflating: Collection5/1187.ann    \n",
      "  inflating: Collection5/1187.txt    \n",
      "  inflating: Collection5/1188.ann    \n",
      "  inflating: Collection5/1188.txt    \n",
      "  inflating: Collection5/1189.ann    \n",
      "  inflating: Collection5/1189.txt    \n",
      "  inflating: Collection5/119.ann     \n",
      "  inflating: Collection5/119.txt     \n",
      "  inflating: Collection5/1190.ann    \n",
      "  inflating: Collection5/1190.txt    \n",
      "  inflating: Collection5/1191.ann    \n",
      "  inflating: Collection5/1191.txt    \n",
      "  inflating: Collection5/1192.ann    \n",
      "  inflating: Collection5/1192.txt    \n",
      "  inflating: Collection5/1193.ann    \n",
      "  inflating: Collection5/1193.txt    \n",
      "  inflating: Collection5/1194.ann    \n",
      "  inflating: Collection5/1194.txt    \n",
      "  inflating: Collection5/1195.ann    \n",
      "  inflating: Collection5/1195.txt    \n",
      "  inflating: Collection5/1196.ann    \n",
      "  inflating: Collection5/1196.txt    \n",
      "  inflating: Collection5/1197.ann    \n",
      "  inflating: Collection5/1197.txt    \n",
      "  inflating: Collection5/1198.ann    \n",
      "  inflating: Collection5/1198.txt    \n",
      "  inflating: Collection5/1199.ann    \n",
      "  inflating: Collection5/1199.txt    \n",
      "  inflating: Collection5/11_01_13b.ann  \n",
      "  inflating: Collection5/11_01_13b.txt  \n",
      "  inflating: Collection5/11_01_13e.ann  \n",
      "  inflating: Collection5/11_01_13e.txt  \n",
      "  inflating: Collection5/120.ann     \n",
      "  inflating: Collection5/120.txt     \n",
      "  inflating: Collection5/1200.ann    \n",
      "  inflating: Collection5/1200.txt    \n",
      "  inflating: Collection5/121.ann     \n",
      "  inflating: Collection5/121.txt     \n",
      "  inflating: Collection5/122.ann     \n",
      "  inflating: Collection5/122.txt     \n",
      "  inflating: Collection5/123.ann     \n",
      "  inflating: Collection5/123.txt     \n",
      "  inflating: Collection5/124.ann     \n",
      "  inflating: Collection5/124.txt     \n",
      "  inflating: Collection5/125.ann     \n",
      "  inflating: Collection5/125.txt     \n",
      "  inflating: Collection5/126.ann     \n",
      "  inflating: Collection5/126.txt     \n",
      "  inflating: Collection5/127.ann     \n",
      "  inflating: Collection5/127.txt     \n",
      "  inflating: Collection5/128.ann     \n",
      "  inflating: Collection5/128.txt     \n",
      "  inflating: Collection5/129.ann     \n",
      "  inflating: Collection5/129.txt     \n",
      "  inflating: Collection5/130.ann     \n",
      "  inflating: Collection5/130.txt     \n",
      "  inflating: Collection5/131.ann     \n",
      "  inflating: Collection5/131.txt     \n",
      "  inflating: Collection5/132.ann     \n",
      "  inflating: Collection5/132.txt     \n",
      "  inflating: Collection5/133.ann     \n",
      "  inflating: Collection5/133.txt     \n",
      "  inflating: Collection5/134.ann     \n",
      "  inflating: Collection5/134.txt     \n",
      "  inflating: Collection5/135.ann     \n",
      "  inflating: Collection5/135.txt     \n",
      "  inflating: Collection5/136.ann     \n",
      "  inflating: Collection5/136.txt     \n",
      "  inflating: Collection5/137.ann     \n",
      "  inflating: Collection5/137.txt     \n",
      "  inflating: Collection5/138.ann     \n",
      "  inflating: Collection5/138.txt     \n",
      "  inflating: Collection5/139.ann     \n",
      "  inflating: Collection5/139.txt     \n",
      "  inflating: Collection5/140.ann     \n",
      "  inflating: Collection5/140.txt     \n",
      "  inflating: Collection5/141.ann     \n",
      "  inflating: Collection5/141.txt     \n",
      "  inflating: Collection5/142.ann     \n",
      "  inflating: Collection5/142.txt     \n",
      "  inflating: Collection5/143.ann     \n",
      "  inflating: Collection5/143.txt     \n",
      "  inflating: Collection5/144.ann     \n",
      "  inflating: Collection5/144.txt     \n",
      "  inflating: Collection5/145.ann     \n",
      "  inflating: Collection5/145.txt     \n",
      "  inflating: Collection5/146.ann     \n",
      "  inflating: Collection5/146.txt     \n",
      "  inflating: Collection5/147.ann     \n",
      "  inflating: Collection5/147.txt     \n",
      "  inflating: Collection5/148.ann     \n",
      "  inflating: Collection5/148.txt     \n",
      "  inflating: Collection5/149.ann     \n",
      "  inflating: Collection5/149.txt     \n",
      "  inflating: Collection5/14_01_13c.ann  \n",
      "  inflating: Collection5/14_01_13c.txt  \n",
      "  inflating: Collection5/14_01_13g.ann  \n",
      "  inflating: Collection5/14_01_13g.txt  \n",
      "  inflating: Collection5/14_01_13i.ann  \n",
      "  inflating: Collection5/14_01_13i.txt  \n",
      "  inflating: Collection5/150.ann     \n",
      "  inflating: Collection5/150.txt     \n",
      "  inflating: Collection5/151.ann     \n",
      "  inflating: Collection5/151.txt     \n",
      "  inflating: Collection5/152.ann     \n",
      "  inflating: Collection5/152.txt     \n",
      "  inflating: Collection5/153.ann     \n",
      "  inflating: Collection5/153.txt     \n",
      "  inflating: Collection5/154.ann     \n",
      "  inflating: Collection5/154.txt     \n",
      "  inflating: Collection5/155.ann     \n",
      "  inflating: Collection5/155.txt     \n",
      "  inflating: Collection5/156.ann     \n",
      "  inflating: Collection5/156.txt     \n",
      "  inflating: Collection5/157.ann     \n",
      "  inflating: Collection5/157.txt     \n",
      "  inflating: Collection5/158.ann     \n",
      "  inflating: Collection5/158.txt     \n",
      "  inflating: Collection5/159.ann     \n",
      "  inflating: Collection5/159.txt     \n",
      "  inflating: Collection5/15_01_13a.ann  \n",
      "  inflating: Collection5/15_01_13a.txt  \n",
      "  inflating: Collection5/15_01_13b.ann  \n",
      "  inflating: Collection5/15_01_13b.txt  \n",
      "  inflating: Collection5/15_01_13e.ann  \n",
      "  inflating: Collection5/15_01_13e.txt  \n",
      "  inflating: Collection5/15_01_13f.ann  \n",
      "  inflating: Collection5/15_01_13f.txt  \n",
      "  inflating: Collection5/160.ann     \n",
      "  inflating: Collection5/160.txt     \n",
      "  inflating: Collection5/161.ann     \n",
      "  inflating: Collection5/161.txt     \n",
      "  inflating: Collection5/162.ann     \n",
      "  inflating: Collection5/162.txt     \n",
      "  inflating: Collection5/163.ann     \n",
      "  inflating: Collection5/163.txt     \n",
      "  inflating: Collection5/164.ann     \n",
      "  inflating: Collection5/164.txt     \n",
      "  inflating: Collection5/165.ann     \n",
      "  inflating: Collection5/165.txt     \n",
      "  inflating: Collection5/166.ann     \n",
      "  inflating: Collection5/166.txt     \n",
      "  inflating: Collection5/167.ann     \n",
      "  inflating: Collection5/167.txt     \n",
      "  inflating: Collection5/168.ann     \n",
      "  inflating: Collection5/168.txt     \n",
      "  inflating: Collection5/169.ann     \n",
      "  inflating: Collection5/169.txt     \n",
      "  inflating: Collection5/170.ann     \n",
      "  inflating: Collection5/170.txt     \n",
      "  inflating: Collection5/171.ann     \n",
      "  inflating: Collection5/171.txt     \n",
      "  inflating: Collection5/172.ann     \n",
      "  inflating: Collection5/172.txt     \n",
      "  inflating: Collection5/173.ann     \n",
      "  inflating: Collection5/173.txt     \n",
      "  inflating: Collection5/174.ann     \n",
      "  inflating: Collection5/174.txt     \n",
      "  inflating: Collection5/175.ann     \n",
      "  inflating: Collection5/175.txt     \n",
      "  inflating: Collection5/176.ann     \n",
      "  inflating: Collection5/176.txt     \n",
      "  inflating: Collection5/177.ann     \n",
      "  inflating: Collection5/177.txt     \n",
      "  inflating: Collection5/178.ann     \n",
      "  inflating: Collection5/178.txt     \n",
      "  inflating: Collection5/179.ann     \n",
      "  inflating: Collection5/179.txt     \n",
      "  inflating: Collection5/180.ann     \n",
      "  inflating: Collection5/180.txt     \n",
      "  inflating: Collection5/181.ann     \n",
      "  inflating: Collection5/181.txt     \n",
      "  inflating: Collection5/182.ann     \n",
      "  inflating: Collection5/182.txt     \n",
      "  inflating: Collection5/183.ann     \n",
      "  inflating: Collection5/183.txt     \n",
      "  inflating: Collection5/184.ann     \n",
      "  inflating: Collection5/184.txt     \n",
      "  inflating: Collection5/185.ann     \n",
      "  inflating: Collection5/185.txt     \n",
      "  inflating: Collection5/186.ann     \n",
      "  inflating: Collection5/186.txt     \n",
      "  inflating: Collection5/187.ann     \n",
      "  inflating: Collection5/187.txt     \n",
      "  inflating: Collection5/188.ann     \n",
      "  inflating: Collection5/188.txt     \n",
      "  inflating: Collection5/189.ann     \n",
      "  inflating: Collection5/189.txt     \n",
      "  inflating: Collection5/190.ann     \n",
      "  inflating: Collection5/190.txt     \n",
      "  inflating: Collection5/191.ann     \n",
      "  inflating: Collection5/191.txt     \n",
      "  inflating: Collection5/192.ann     \n",
      "  inflating: Collection5/192.txt     \n",
      "  inflating: Collection5/193.ann     \n",
      "  inflating: Collection5/193.txt     \n",
      "  inflating: Collection5/194.ann     \n",
      "  inflating: Collection5/194.txt     \n",
      "  inflating: Collection5/195.ann     \n",
      "  inflating: Collection5/195.txt     \n",
      "  inflating: Collection5/196.ann     \n",
      "  inflating: Collection5/196.txt     \n",
      "  inflating: Collection5/197.ann     \n",
      "  inflating: Collection5/197.txt     \n",
      "  inflating: Collection5/198.ann     \n",
      "  inflating: Collection5/198.txt     \n",
      "  inflating: Collection5/199.ann     \n",
      "  inflating: Collection5/199.txt     \n",
      "  inflating: Collection5/19_11_12d.ann  \n",
      "  inflating: Collection5/19_11_12d.txt  \n",
      "  inflating: Collection5/19_11_12h.ann  \n",
      "  inflating: Collection5/19_11_12h.txt  \n",
      "  inflating: Collection5/200.ann     \n",
      "  inflating: Collection5/200.txt     \n",
      "  inflating: Collection5/2001.ann    \n",
      "  inflating: Collection5/2001.txt    \n",
      "  inflating: Collection5/2002.ann    \n",
      "  inflating: Collection5/2002.txt    \n",
      "  inflating: Collection5/2003.ann    \n",
      "  inflating: Collection5/2003.txt    \n",
      "  inflating: Collection5/2004.ann    \n",
      "  inflating: Collection5/2004.txt    \n",
      "  inflating: Collection5/2005.ann    \n",
      "  inflating: Collection5/2005.txt    \n",
      "  inflating: Collection5/2006.ann    \n",
      "  inflating: Collection5/2006.txt    \n",
      "  inflating: Collection5/2007.ann    \n",
      "  inflating: Collection5/2007.txt    \n",
      "  inflating: Collection5/2008.ann    \n",
      "  inflating: Collection5/2008.txt    \n",
      "  inflating: Collection5/2009.ann    \n",
      "  inflating: Collection5/2009.txt    \n",
      "  inflating: Collection5/201.ann     \n",
      "  inflating: Collection5/201.txt     \n",
      "  inflating: Collection5/2010.ann    \n",
      "  inflating: Collection5/2010.txt    \n",
      "  inflating: Collection5/2011.ann    \n",
      "  inflating: Collection5/2011.txt    \n",
      "  inflating: Collection5/2012.ann    \n",
      "  inflating: Collection5/2012.txt    \n",
      "  inflating: Collection5/2013.ann    \n",
      "  inflating: Collection5/2013.txt    \n",
      "  inflating: Collection5/2014.ann    \n",
      "  inflating: Collection5/2014.txt    \n",
      "  inflating: Collection5/2015.ann    \n",
      "  inflating: Collection5/2015.txt    \n",
      "  inflating: Collection5/2016.ann    \n",
      "  inflating: Collection5/2016.txt    \n",
      "  inflating: Collection5/2017.ann    \n",
      "  inflating: Collection5/2017.txt    \n",
      "  inflating: Collection5/2018.ann    \n",
      "  inflating: Collection5/2018.txt    \n",
      "  inflating: Collection5/2019.ann    \n",
      "  inflating: Collection5/2019.txt    \n",
      "  inflating: Collection5/202.ann     \n",
      "  inflating: Collection5/202.txt     \n",
      "  inflating: Collection5/2020.ann    \n",
      "  inflating: Collection5/2020.txt    \n",
      "  inflating: Collection5/2021.ann    \n",
      "  inflating: Collection5/2021.txt    \n",
      "  inflating: Collection5/2022.ann    \n",
      "  inflating: Collection5/2022.txt    \n",
      "  inflating: Collection5/2023.ann    \n",
      "  inflating: Collection5/2023.txt    \n",
      "  inflating: Collection5/2024.ann    \n",
      "  inflating: Collection5/2024.txt    \n",
      "  inflating: Collection5/2025.ann    \n",
      "  inflating: Collection5/2025.txt    \n",
      "  inflating: Collection5/2026.ann    \n",
      "  inflating: Collection5/2026.txt    \n",
      "  inflating: Collection5/2027.ann    \n",
      "  inflating: Collection5/2027.txt    \n",
      "  inflating: Collection5/2028.ann    \n",
      "  inflating: Collection5/2028.txt    \n",
      "  inflating: Collection5/2029.ann    \n",
      "  inflating: Collection5/2029.txt    \n",
      "  inflating: Collection5/203.ann     \n",
      "  inflating: Collection5/203.txt     \n",
      "  inflating: Collection5/2030.ann    \n",
      "  inflating: Collection5/2030.txt    \n",
      "  inflating: Collection5/2031.ann    \n",
      "  inflating: Collection5/2031.txt    \n",
      "  inflating: Collection5/2032.ann    \n",
      "  inflating: Collection5/2032.txt    \n",
      "  inflating: Collection5/2034.ann    \n",
      "  inflating: Collection5/2034.txt    \n",
      "  inflating: Collection5/2035.ann    \n",
      "  inflating: Collection5/2035.txt    \n",
      "  inflating: Collection5/2036.ann    \n",
      "  inflating: Collection5/2036.txt    \n",
      "  inflating: Collection5/2037.ann    \n",
      "  inflating: Collection5/2037.txt    \n",
      "  inflating: Collection5/2038.ann    \n",
      "  inflating: Collection5/2038.txt    \n",
      "  inflating: Collection5/2039.ann    \n",
      "  inflating: Collection5/2039.txt    \n",
      "  inflating: Collection5/204.ann     \n",
      "  inflating: Collection5/204.txt     \n",
      "  inflating: Collection5/2040.ann    \n",
      "  inflating: Collection5/2040.txt    \n",
      "  inflating: Collection5/2041.ann    \n",
      "  inflating: Collection5/2041.txt    \n",
      "  inflating: Collection5/2042.ann    \n",
      "  inflating: Collection5/2042.txt    \n",
      "  inflating: Collection5/2043.ann    \n",
      "  inflating: Collection5/2043.txt    \n",
      "  inflating: Collection5/2044.ann    \n",
      "  inflating: Collection5/2044.txt    \n",
      "  inflating: Collection5/2045.ann    \n",
      "  inflating: Collection5/2045.txt    \n",
      "  inflating: Collection5/2046.ann    \n",
      "  inflating: Collection5/2046.txt    \n",
      "  inflating: Collection5/2047.ann    \n",
      "  inflating: Collection5/2047.txt    \n",
      "  inflating: Collection5/2048.ann    \n",
      "  inflating: Collection5/2048.txt    \n",
      "  inflating: Collection5/2049.ann    \n",
      "  inflating: Collection5/2049.txt    \n",
      "  inflating: Collection5/205.ann     \n",
      "  inflating: Collection5/205.txt     \n",
      "  inflating: Collection5/2050.ann    \n",
      "  inflating: Collection5/2050.txt    \n",
      "  inflating: Collection5/206.ann     \n",
      "  inflating: Collection5/206.txt     \n",
      "  inflating: Collection5/207.ann     \n",
      "  inflating: Collection5/207.txt     \n",
      "  inflating: Collection5/208.ann     \n",
      "  inflating: Collection5/208.txt     \n",
      "  inflating: Collection5/209.ann     \n",
      "  inflating: Collection5/209.txt     \n",
      "  inflating: Collection5/20_11_12a.ann  \n",
      "  inflating: Collection5/20_11_12a.txt  \n",
      "  inflating: Collection5/20_11_12b.ann  \n",
      "  inflating: Collection5/20_11_12b.txt  \n",
      "  inflating: Collection5/20_11_12c.ann  \n",
      "  inflating: Collection5/20_11_12c.txt  \n",
      "  inflating: Collection5/20_11_12d.ann  \n",
      "  inflating: Collection5/20_11_12d.txt  \n",
      "  inflating: Collection5/20_11_12i.ann  \n",
      "  inflating: Collection5/20_11_12i.txt  \n",
      "  inflating: Collection5/210.ann     \n",
      "  inflating: Collection5/210.txt     \n",
      "  inflating: Collection5/211.ann     \n",
      "  inflating: Collection5/211.txt     \n",
      "  inflating: Collection5/212.ann     \n",
      "  inflating: Collection5/212.txt     \n",
      "  inflating: Collection5/213.ann     \n",
      "  inflating: Collection5/213.txt     \n",
      "  inflating: Collection5/214.ann     \n",
      "  inflating: Collection5/214.txt     \n",
      "  inflating: Collection5/215.ann     \n",
      "  inflating: Collection5/215.txt     \n",
      "  inflating: Collection5/216.ann     \n",
      "  inflating: Collection5/216.txt     \n",
      "  inflating: Collection5/217.ann     \n",
      "  inflating: Collection5/217.txt     \n",
      "  inflating: Collection5/218.ann     \n",
      "  inflating: Collection5/218.txt     \n",
      "  inflating: Collection5/219.ann     \n",
      "  inflating: Collection5/219.txt     \n",
      "  inflating: Collection5/21_11_12c.ann  \n",
      "  inflating: Collection5/21_11_12c.txt  \n",
      "  inflating: Collection5/21_11_12h.ann  \n",
      "  inflating: Collection5/21_11_12h.txt  \n",
      "  inflating: Collection5/21_11_12i.ann  \n",
      "  inflating: Collection5/21_11_12i.txt  \n",
      "  inflating: Collection5/21_11_12j.ann  \n",
      "  inflating: Collection5/21_11_12j.txt  \n",
      "  inflating: Collection5/220.ann     \n",
      "  inflating: Collection5/220.txt     \n",
      "  inflating: Collection5/221.ann     \n",
      "  inflating: Collection5/221.txt     \n",
      "  inflating: Collection5/222.ann     \n",
      "  inflating: Collection5/222.txt     \n",
      "  inflating: Collection5/223.ann     \n",
      "  inflating: Collection5/223.txt     \n",
      "  inflating: Collection5/224.ann     \n",
      "  inflating: Collection5/224.txt     \n",
      "  inflating: Collection5/225.ann     \n",
      "  inflating: Collection5/225.txt     \n",
      "  inflating: Collection5/226.ann     \n",
      "  inflating: Collection5/226.txt     \n",
      "  inflating: Collection5/227.ann     \n",
      "  inflating: Collection5/227.txt     \n",
      "  inflating: Collection5/228.ann     \n",
      "  inflating: Collection5/228.txt     \n",
      "  inflating: Collection5/229.ann     \n",
      "  inflating: Collection5/229.txt     \n",
      "  inflating: Collection5/22_11_12a.ann  \n",
      "  inflating: Collection5/22_11_12a.txt  \n",
      "  inflating: Collection5/22_11_12c.ann  \n",
      "  inflating: Collection5/22_11_12c.txt  \n",
      "  inflating: Collection5/22_11_12d.ann  \n",
      "  inflating: Collection5/22_11_12d.txt  \n",
      "  inflating: Collection5/22_11_12g.ann  \n",
      "  inflating: Collection5/22_11_12g.txt  \n",
      "  inflating: Collection5/22_11_12h.ann  \n",
      "  inflating: Collection5/22_11_12h.txt  \n",
      "  inflating: Collection5/22_11_12i.ann  \n",
      "  inflating: Collection5/22_11_12i.txt  \n",
      "  inflating: Collection5/22_11_12j.ann  \n",
      "  inflating: Collection5/22_11_12j.txt  \n",
      "  inflating: Collection5/230.ann     \n",
      "  inflating: Collection5/230.txt     \n",
      "  inflating: Collection5/231.ann     \n",
      "  inflating: Collection5/231.txt     \n",
      "  inflating: Collection5/232.ann     \n",
      "  inflating: Collection5/232.txt     \n",
      "  inflating: Collection5/233.ann     \n",
      "  inflating: Collection5/233.txt     \n",
      "  inflating: Collection5/234.ann     \n",
      "  inflating: Collection5/234.txt     \n",
      "  inflating: Collection5/235.ann     \n",
      "  inflating: Collection5/235.txt     \n",
      "  inflating: Collection5/236.ann     \n",
      "  inflating: Collection5/236.txt     \n",
      "  inflating: Collection5/237.ann     \n",
      "  inflating: Collection5/237.txt     \n",
      "  inflating: Collection5/238.ann     \n",
      "  inflating: Collection5/238.txt     \n",
      "  inflating: Collection5/239.ann     \n",
      "  inflating: Collection5/239.txt     \n",
      "  inflating: Collection5/23_11_12a.ann  \n",
      "  inflating: Collection5/23_11_12a.txt  \n",
      "  inflating: Collection5/23_11_12b.ann  \n",
      "  inflating: Collection5/23_11_12b.txt  \n",
      "  inflating: Collection5/23_11_12c.ann  \n",
      "  inflating: Collection5/23_11_12c.txt  \n",
      "  inflating: Collection5/23_11_12d.ann  \n",
      "  inflating: Collection5/23_11_12d.txt  \n",
      "  inflating: Collection5/23_11_12e.ann  \n",
      "  inflating: Collection5/23_11_12e.txt  \n",
      "  inflating: Collection5/23_11_12f.ann  \n",
      "  inflating: Collection5/23_11_12f.txt  \n",
      "  inflating: Collection5/240.ann     \n",
      "  inflating: Collection5/240.txt     \n",
      "  inflating: Collection5/241.ann     \n",
      "  inflating: Collection5/241.txt     \n",
      "  inflating: Collection5/242.ann     \n",
      "  inflating: Collection5/242.txt     \n",
      "  inflating: Collection5/243.ann     \n",
      "  inflating: Collection5/243.txt     \n",
      "  inflating: Collection5/244.ann     \n",
      "  inflating: Collection5/244.txt     \n",
      "  inflating: Collection5/245.ann     \n",
      "  inflating: Collection5/245.txt     \n",
      "  inflating: Collection5/246.ann     \n",
      "  inflating: Collection5/246.txt     \n",
      "  inflating: Collection5/247.ann     \n",
      "  inflating: Collection5/247.txt     \n",
      "  inflating: Collection5/248.ann     \n",
      "  inflating: Collection5/248.txt     \n",
      "  inflating: Collection5/249.ann     \n",
      "  inflating: Collection5/249.txt     \n",
      "  inflating: Collection5/250.ann     \n",
      "  inflating: Collection5/250.txt     \n",
      "  inflating: Collection5/251.ann     \n",
      "  inflating: Collection5/251.txt     \n",
      "  inflating: Collection5/252.ann     \n",
      "  inflating: Collection5/252.txt     \n",
      "  inflating: Collection5/253.ann     \n",
      "  inflating: Collection5/253.txt     \n",
      "  inflating: Collection5/254.ann     \n",
      "  inflating: Collection5/254.txt     \n",
      "  inflating: Collection5/255.ann     \n",
      "  inflating: Collection5/255.txt     \n",
      "  inflating: Collection5/256.ann     \n",
      "  inflating: Collection5/256.txt     \n",
      "  inflating: Collection5/257.ann     \n",
      "  inflating: Collection5/257.txt     \n",
      "  inflating: Collection5/258.ann     \n",
      "  inflating: Collection5/258.txt     \n",
      "  inflating: Collection5/259.ann     \n",
      "  inflating: Collection5/259.txt     \n",
      "  inflating: Collection5/25_12_12a.ann  \n",
      "  inflating: Collection5/25_12_12a.txt  \n",
      "  inflating: Collection5/25_12_12c.ann  \n",
      "  inflating: Collection5/25_12_12c.txt  \n",
      "  inflating: Collection5/25_12_12d.ann  \n",
      "  inflating: Collection5/25_12_12d.txt  \n",
      "  inflating: Collection5/25_12_12e.ann  \n",
      "  inflating: Collection5/25_12_12e.txt  \n",
      "  inflating: Collection5/260.ann     \n",
      "  inflating: Collection5/260.txt     \n",
      "  inflating: Collection5/261.ann     \n",
      "  inflating: Collection5/261.txt     \n",
      "  inflating: Collection5/262.ann     \n",
      "  inflating: Collection5/262.txt     \n",
      "  inflating: Collection5/263.ann     \n",
      "  inflating: Collection5/263.txt     \n",
      "  inflating: Collection5/264.ann     \n",
      "  inflating: Collection5/264.txt     \n",
      "  inflating: Collection5/265.ann     \n",
      "  inflating: Collection5/265.txt     \n",
      "  inflating: Collection5/266.ann     \n",
      "  inflating: Collection5/266.txt     \n",
      "  inflating: Collection5/267.ann     \n",
      "  inflating: Collection5/267.txt     \n",
      "  inflating: Collection5/268.ann     \n",
      "  inflating: Collection5/268.txt     \n",
      "  inflating: Collection5/269.ann     \n",
      "  inflating: Collection5/269.txt     \n",
      "  inflating: Collection5/26_11_12b.ann  \n",
      "  inflating: Collection5/26_11_12b.txt  \n",
      "  inflating: Collection5/26_11_12c.ann  \n",
      "  inflating: Collection5/26_11_12c.txt  \n",
      "  inflating: Collection5/26_11_12e.ann  \n",
      "  inflating: Collection5/26_11_12e.txt  \n",
      "  inflating: Collection5/26_11_12f.ann  \n",
      "  inflating: Collection5/26_11_12f.txt  \n",
      "  inflating: Collection5/270.ann     \n",
      "  inflating: Collection5/270.txt     \n",
      "  inflating: Collection5/271.ann     \n",
      "  inflating: Collection5/271.txt     \n",
      "  inflating: Collection5/272.ann     \n",
      "  inflating: Collection5/272.txt     \n",
      "  inflating: Collection5/273.ann     \n",
      "  inflating: Collection5/273.txt     \n",
      "  inflating: Collection5/274.ann     \n",
      "  inflating: Collection5/274.txt     \n",
      "  inflating: Collection5/275.ann     \n",
      "  inflating: Collection5/275.txt     \n",
      "  inflating: Collection5/276.ann     \n",
      "  inflating: Collection5/276.txt     \n",
      "  inflating: Collection5/277.ann     \n",
      "  inflating: Collection5/277.txt     \n",
      "  inflating: Collection5/278.ann     \n",
      "  inflating: Collection5/278.txt     \n",
      "  inflating: Collection5/279.ann     \n",
      "  inflating: Collection5/279.txt     \n",
      "  inflating: Collection5/27_11_12a.ann  \n",
      "  inflating: Collection5/27_11_12a.txt  \n",
      "  inflating: Collection5/27_11_12c.ann  \n",
      "  inflating: Collection5/27_11_12c.txt  \n",
      "  inflating: Collection5/27_11_12d.ann  \n",
      "  inflating: Collection5/27_11_12d.txt  \n",
      "  inflating: Collection5/27_11_12e.ann  \n",
      "  inflating: Collection5/27_11_12e.txt  \n",
      "  inflating: Collection5/27_11_12j.ann  \n",
      "  inflating: Collection5/27_11_12j.txt  \n",
      "  inflating: Collection5/280.ann     \n",
      "  inflating: Collection5/280.txt     \n",
      "  inflating: Collection5/281.ann     \n",
      "  inflating: Collection5/281.txt     \n",
      "  inflating: Collection5/282.ann     \n",
      "  inflating: Collection5/282.txt     \n",
      "  inflating: Collection5/283.ann     \n",
      "  inflating: Collection5/283.txt     \n",
      "  inflating: Collection5/284.ann     \n",
      "  inflating: Collection5/284.txt     \n",
      "  inflating: Collection5/285.ann     \n",
      "  inflating: Collection5/285.txt     \n",
      "  inflating: Collection5/286.ann     \n",
      "  inflating: Collection5/286.txt     \n",
      "  inflating: Collection5/287.ann     \n",
      "  inflating: Collection5/287.txt     \n",
      "  inflating: Collection5/288.ann     \n",
      "  inflating: Collection5/288.txt     \n",
      "  inflating: Collection5/289.ann     \n",
      "  inflating: Collection5/289.txt     \n",
      "  inflating: Collection5/28_11_12a.ann  \n",
      "  inflating: Collection5/28_11_12a.txt  \n",
      "  inflating: Collection5/28_11_12f.ann  \n",
      "  inflating: Collection5/28_11_12f.txt  \n",
      "  inflating: Collection5/28_11_12g.ann  \n",
      "  inflating: Collection5/28_11_12g.txt  \n",
      "  inflating: Collection5/28_11_12h.ann  \n",
      "  inflating: Collection5/28_11_12h.txt  \n",
      "  inflating: Collection5/28_11_12i.ann  \n",
      "  inflating: Collection5/28_11_12i.txt  \n",
      "  inflating: Collection5/28_11_12j.ann  \n",
      "  inflating: Collection5/28_11_12j.txt  \n",
      "  inflating: Collection5/290.ann     \n",
      "  inflating: Collection5/290.txt     \n",
      "  inflating: Collection5/291.ann     \n",
      "  inflating: Collection5/291.txt     \n",
      "  inflating: Collection5/292.ann     \n",
      "  inflating: Collection5/292.txt     \n",
      "  inflating: Collection5/293.ann     \n",
      "  inflating: Collection5/293.txt     \n",
      "  inflating: Collection5/294.ann     \n",
      "  inflating: Collection5/294.txt     \n",
      "  inflating: Collection5/295.ann     \n",
      "  inflating: Collection5/295.txt     \n",
      "  inflating: Collection5/296.ann     \n",
      "  inflating: Collection5/296.txt     \n",
      "  inflating: Collection5/297.ann     \n",
      "  inflating: Collection5/297.txt     \n",
      "  inflating: Collection5/298.ann     \n",
      "  inflating: Collection5/298.txt     \n",
      "  inflating: Collection5/299.ann     \n",
      "  inflating: Collection5/299.txt     \n",
      "  inflating: Collection5/29_11_12a.ann  \n",
      "  inflating: Collection5/29_11_12a.txt  \n",
      "  inflating: Collection5/29_11_12b.ann  \n",
      "  inflating: Collection5/29_11_12b.txt  \n",
      "  inflating: Collection5/300.ann     \n",
      "  inflating: Collection5/300.txt     \n",
      "  inflating: Collection5/301.ann     \n",
      "  inflating: Collection5/301.txt     \n",
      "  inflating: Collection5/302.ann     \n",
      "  inflating: Collection5/302.txt     \n",
      "  inflating: Collection5/303.ann     \n",
      "  inflating: Collection5/303.txt     \n",
      "  inflating: Collection5/304.ann     \n",
      "  inflating: Collection5/304.txt     \n",
      "  inflating: Collection5/305.ann     \n",
      "  inflating: Collection5/305.txt     \n",
      "  inflating: Collection5/306.ann     \n",
      "  inflating: Collection5/306.txt     \n",
      "  inflating: Collection5/307.ann     \n",
      "  inflating: Collection5/307.txt     \n",
      "  inflating: Collection5/308.ann     \n",
      "  inflating: Collection5/308.txt     \n",
      "  inflating: Collection5/309.ann     \n",
      "  inflating: Collection5/309.txt     \n",
      "  inflating: Collection5/30_11_12b.ann  \n",
      "  inflating: Collection5/30_11_12b.txt  \n",
      "  inflating: Collection5/30_11_12h.ann  \n",
      "  inflating: Collection5/30_11_12h.txt  \n",
      "  inflating: Collection5/30_11_12i.ann  \n",
      "  inflating: Collection5/30_11_12i.txt  \n",
      "  inflating: Collection5/310.ann     \n",
      "  inflating: Collection5/310.txt     \n",
      "  inflating: Collection5/311.ann     \n",
      "  inflating: Collection5/311.txt     \n",
      "  inflating: Collection5/312.ann     \n",
      "  inflating: Collection5/312.txt     \n",
      "  inflating: Collection5/313.ann     \n",
      "  inflating: Collection5/313.txt     \n",
      "  inflating: Collection5/314.ann     \n",
      "  inflating: Collection5/314.txt     \n",
      "  inflating: Collection5/315.ann     \n",
      "  inflating: Collection5/315.txt     \n",
      "  inflating: Collection5/316.ann     \n",
      "  inflating: Collection5/316.txt     \n",
      "  inflating: Collection5/317.ann     \n",
      "  inflating: Collection5/317.txt     \n",
      "  inflating: Collection5/318.ann     \n",
      "  inflating: Collection5/318.txt     \n",
      "  inflating: Collection5/319.ann     \n",
      "  inflating: Collection5/319.txt     \n",
      "  inflating: Collection5/320.ann     \n",
      "  inflating: Collection5/320.txt     \n",
      "  inflating: Collection5/321.ann     \n",
      "  inflating: Collection5/321.txt     \n",
      "  inflating: Collection5/322.ann     \n",
      "  inflating: Collection5/322.txt     \n",
      "  inflating: Collection5/323.ann     \n",
      "  inflating: Collection5/323.txt     \n",
      "  inflating: Collection5/324.ann     \n",
      "  inflating: Collection5/324.txt     \n",
      "  inflating: Collection5/325.ann     \n",
      "  inflating: Collection5/325.txt     \n",
      "  inflating: Collection5/326.ann     \n",
      "  inflating: Collection5/326.txt     \n",
      "  inflating: Collection5/327.ann     \n",
      "  inflating: Collection5/327.txt     \n",
      "  inflating: Collection5/328.ann     \n",
      "  inflating: Collection5/328.txt     \n",
      "  inflating: Collection5/329.ann     \n",
      "  inflating: Collection5/329.txt     \n",
      "  inflating: Collection5/330.ann     \n",
      "  inflating: Collection5/330.txt     \n",
      "  inflating: Collection5/331.ann     \n",
      "  inflating: Collection5/331.txt     \n",
      "  inflating: Collection5/332.ann     \n",
      "  inflating: Collection5/332.txt     \n",
      "  inflating: Collection5/333.ann     \n",
      "  inflating: Collection5/333.txt     \n",
      "  inflating: Collection5/334.ann     \n",
      "  inflating: Collection5/334.txt     \n",
      "  inflating: Collection5/335.ann     \n",
      "  inflating: Collection5/335.txt     \n",
      "  inflating: Collection5/336.ann     \n",
      "  inflating: Collection5/336.txt     \n",
      "  inflating: Collection5/337.ann     \n",
      "  inflating: Collection5/337.txt     \n",
      "  inflating: Collection5/338.ann     \n",
      "  inflating: Collection5/338.txt     \n",
      "  inflating: Collection5/339.ann     \n",
      "  inflating: Collection5/339.txt     \n",
      "  inflating: Collection5/340.ann     \n",
      "  inflating: Collection5/340.txt     \n",
      "  inflating: Collection5/341.ann     \n",
      "  inflating: Collection5/341.txt     \n",
      "  inflating: Collection5/342.ann     \n",
      "  inflating: Collection5/342.txt     \n",
      "  inflating: Collection5/343.ann     \n",
      "  inflating: Collection5/343.txt     \n",
      "  inflating: Collection5/344.ann     \n",
      "  inflating: Collection5/344.txt     \n",
      "  inflating: Collection5/345.ann     \n",
      "  inflating: Collection5/345.txt     \n",
      "  inflating: Collection5/346.ann     \n",
      "  inflating: Collection5/346.txt     \n",
      "  inflating: Collection5/347.ann     \n",
      "  inflating: Collection5/347.txt     \n",
      "  inflating: Collection5/348.ann     \n",
      "  inflating: Collection5/348.txt     \n",
      "  inflating: Collection5/349.ann     \n",
      "  inflating: Collection5/349.txt     \n",
      "  inflating: Collection5/350.ann     \n",
      "  inflating: Collection5/350.txt     \n",
      "  inflating: Collection5/351.ann     \n",
      "  inflating: Collection5/351.txt     \n",
      "  inflating: Collection5/352.ann     \n",
      "  inflating: Collection5/352.txt     \n",
      "  inflating: Collection5/353.ann     \n",
      "  inflating: Collection5/353.txt     \n",
      "  inflating: Collection5/354.ann     \n",
      "  inflating: Collection5/354.txt     \n",
      "  inflating: Collection5/355.ann     \n",
      "  inflating: Collection5/355.txt     \n",
      "  inflating: Collection5/356.ann     \n",
      "  inflating: Collection5/356.txt     \n",
      "  inflating: Collection5/357.ann     \n",
      "  inflating: Collection5/357.txt     \n",
      "  inflating: Collection5/358.ann     \n",
      "  inflating: Collection5/358.txt     \n",
      "  inflating: Collection5/359.ann     \n",
      "  inflating: Collection5/359.txt     \n",
      "  inflating: Collection5/360.ann     \n",
      "  inflating: Collection5/360.txt     \n",
      "  inflating: Collection5/361.ann     \n",
      "  inflating: Collection5/361.txt     \n",
      "  inflating: Collection5/362.ann     \n",
      "  inflating: Collection5/362.txt     \n",
      "  inflating: Collection5/363.ann     \n",
      "  inflating: Collection5/363.txt     \n",
      "  inflating: Collection5/364.ann     \n",
      "  inflating: Collection5/364.txt     \n",
      "  inflating: Collection5/365.ann     \n",
      "  inflating: Collection5/365.txt     \n",
      "  inflating: Collection5/366.ann     \n",
      "  inflating: Collection5/366.txt     \n",
      "  inflating: Collection5/367.ann     \n",
      "  inflating: Collection5/367.txt     \n",
      "  inflating: Collection5/368.ann     \n",
      "  inflating: Collection5/368.txt     \n",
      "  inflating: Collection5/369.ann     \n",
      "  inflating: Collection5/369.txt     \n",
      "  inflating: Collection5/370.ann     \n",
      "  inflating: Collection5/370.txt     \n",
      "  inflating: Collection5/371.ann     \n",
      "  inflating: Collection5/371.txt     \n",
      "  inflating: Collection5/372.ann     \n",
      "  inflating: Collection5/372.txt     \n",
      "  inflating: Collection5/373.ann     \n",
      "  inflating: Collection5/373.txt     \n",
      "  inflating: Collection5/374.ann     \n",
      "  inflating: Collection5/374.txt     \n",
      "  inflating: Collection5/375.ann     \n",
      "  inflating: Collection5/375.txt     \n",
      "  inflating: Collection5/376.ann     \n",
      "  inflating: Collection5/376.txt     \n",
      "  inflating: Collection5/377.ann     \n",
      "  inflating: Collection5/377.txt     \n",
      "  inflating: Collection5/378.ann     \n",
      "  inflating: Collection5/378.txt     \n",
      "  inflating: Collection5/379.ann     \n",
      "  inflating: Collection5/379.txt     \n",
      "  inflating: Collection5/380.ann     \n",
      "  inflating: Collection5/380.txt     \n",
      "  inflating: Collection5/381.ann     \n",
      "  inflating: Collection5/381.txt     \n",
      "  inflating: Collection5/382.ann     \n",
      "  inflating: Collection5/382.txt     \n",
      "  inflating: Collection5/383.ann     \n",
      "  inflating: Collection5/383.txt     \n",
      "  inflating: Collection5/384.ann     \n",
      "  inflating: Collection5/384.txt     \n",
      "  inflating: Collection5/385.ann     \n",
      "  inflating: Collection5/385.txt     \n",
      "  inflating: Collection5/386.ann     \n",
      "  inflating: Collection5/386.txt     \n",
      "  inflating: Collection5/387.ann     \n",
      "  inflating: Collection5/387.txt     \n",
      "  inflating: Collection5/388.ann     \n",
      "  inflating: Collection5/388.txt     \n",
      "  inflating: Collection5/389.ann     \n",
      "  inflating: Collection5/389.txt     \n",
      "  inflating: Collection5/390.ann     \n",
      "  inflating: Collection5/390.txt     \n",
      "  inflating: Collection5/391.ann     \n",
      "  inflating: Collection5/391.txt     \n",
      "  inflating: Collection5/392.ann     \n",
      "  inflating: Collection5/392.txt     \n",
      "  inflating: Collection5/393.ann     \n",
      "  inflating: Collection5/393.txt     \n",
      "  inflating: Collection5/394.ann     \n",
      "  inflating: Collection5/394.txt     \n",
      "  inflating: Collection5/395.ann     \n",
      "  inflating: Collection5/395.txt     \n",
      "  inflating: Collection5/396.ann     \n",
      "  inflating: Collection5/396.txt     \n",
      "  inflating: Collection5/397.ann     \n",
      "  inflating: Collection5/397.txt     \n",
      "  inflating: Collection5/398.ann     \n",
      "  inflating: Collection5/398.txt     \n",
      "  inflating: Collection5/399.ann     \n",
      "  inflating: Collection5/399.txt     \n",
      "  inflating: Collection5/400.ann     \n",
      "  inflating: Collection5/400.txt     \n",
      "  inflating: Collection5/401.ann     \n",
      "  inflating: Collection5/401.txt     \n",
      "  inflating: Collection5/402.ann     \n",
      "  inflating: Collection5/402.txt     \n",
      "  inflating: Collection5/403.ann     \n",
      "  inflating: Collection5/403.txt     \n",
      "  inflating: Collection5/404.ann     \n",
      "  inflating: Collection5/404.txt     \n",
      "  inflating: Collection5/405.ann     \n",
      "  inflating: Collection5/405.txt     \n",
      "  inflating: Collection5/406.ann     \n",
      "  inflating: Collection5/406.txt     \n",
      "  inflating: Collection5/407.ann     \n",
      "  inflating: Collection5/407.txt     \n",
      "  inflating: Collection5/408.ann     \n",
      "  inflating: Collection5/408.txt     \n",
      "  inflating: Collection5/409.ann     \n",
      "  inflating: Collection5/409.txt     \n",
      "  inflating: Collection5/410.ann     \n",
      "  inflating: Collection5/410.txt     \n",
      "  inflating: Collection5/411.ann     \n",
      "  inflating: Collection5/411.txt     \n",
      "  inflating: Collection5/412.ann     \n",
      "  inflating: Collection5/412.txt     \n",
      "  inflating: Collection5/413.ann     \n",
      "  inflating: Collection5/413.txt     \n",
      "  inflating: Collection5/414.ann     \n",
      "  inflating: Collection5/414.txt     \n",
      "  inflating: Collection5/415.ann     \n",
      "  inflating: Collection5/415.txt     \n",
      "  inflating: Collection5/416.ann     \n",
      "  inflating: Collection5/416.txt     \n",
      "  inflating: Collection5/417.ann     \n",
      "  inflating: Collection5/417.txt     \n",
      "  inflating: Collection5/418.ann     \n",
      "  inflating: Collection5/418.txt     \n",
      "  inflating: Collection5/419.ann     \n",
      "  inflating: Collection5/419.txt     \n",
      "  inflating: Collection5/420.ann     \n",
      "  inflating: Collection5/420.txt     \n",
      "  inflating: Collection5/421.ann     \n",
      "  inflating: Collection5/421.txt     \n",
      "  inflating: Collection5/422.ann     \n",
      "  inflating: Collection5/422.txt     \n",
      "  inflating: Collection5/423.ann     \n",
      "  inflating: Collection5/423.txt     \n",
      "  inflating: Collection5/424.ann     \n",
      "  inflating: Collection5/424.txt     \n",
      "  inflating: Collection5/425.ann     \n",
      "  inflating: Collection5/425.txt     \n",
      "  inflating: Collection5/426.ann     \n",
      "  inflating: Collection5/426.txt     \n",
      "  inflating: Collection5/427.ann     \n",
      "  inflating: Collection5/427.txt     \n",
      "  inflating: Collection5/428.ann     \n",
      "  inflating: Collection5/428.txt     \n",
      "  inflating: Collection5/429.ann     \n",
      "  inflating: Collection5/429.txt     \n",
      "  inflating: Collection5/430.ann     \n",
      "  inflating: Collection5/430.txt     \n",
      "  inflating: Collection5/431.ann     \n",
      "  inflating: Collection5/431.txt     \n",
      "  inflating: Collection5/432.ann     \n",
      "  inflating: Collection5/432.txt     \n",
      "  inflating: Collection5/433.ann     \n",
      "  inflating: Collection5/433.txt     \n",
      "  inflating: Collection5/434.ann     \n",
      "  inflating: Collection5/434.txt     \n",
      "  inflating: Collection5/435.ann     \n",
      "  inflating: Collection5/435.txt     \n",
      "  inflating: Collection5/436.ann     \n",
      "  inflating: Collection5/436.txt     \n",
      "  inflating: Collection5/437.ann     \n",
      "  inflating: Collection5/437.txt     \n",
      "  inflating: Collection5/438.ann     \n",
      "  inflating: Collection5/438.txt     \n",
      "  inflating: Collection5/439.ann     \n",
      "  inflating: Collection5/439.txt     \n",
      "  inflating: Collection5/440.ann     \n",
      "  inflating: Collection5/440.txt     \n",
      "  inflating: Collection5/441.ann     \n",
      "  inflating: Collection5/441.txt     \n",
      "  inflating: Collection5/442.ann     \n",
      "  inflating: Collection5/442.txt     \n",
      "  inflating: Collection5/443.ann     \n",
      "  inflating: Collection5/443.txt     \n",
      "  inflating: Collection5/444.ann     \n",
      "  inflating: Collection5/444.txt     \n",
      "  inflating: Collection5/445.ann     \n",
      "  inflating: Collection5/445.txt     \n",
      "  inflating: Collection5/446.ann     \n",
      "  inflating: Collection5/446.txt     \n",
      "  inflating: Collection5/447.ann     \n",
      "  inflating: Collection5/447.txt     \n",
      "  inflating: Collection5/448.ann     \n",
      "  inflating: Collection5/448.txt     \n",
      "  inflating: Collection5/449.ann     \n",
      "  inflating: Collection5/449.txt     \n",
      "  inflating: Collection5/450.ann     \n",
      "  inflating: Collection5/450.txt     \n",
      "  inflating: Collection5/451.ann     \n",
      "  inflating: Collection5/451.txt     \n",
      "  inflating: Collection5/452.ann     \n",
      "  inflating: Collection5/452.txt     \n",
      "  inflating: Collection5/453.ann     \n",
      "  inflating: Collection5/453.txt     \n",
      "  inflating: Collection5/454.ann     \n",
      "  inflating: Collection5/454.txt     \n",
      "  inflating: Collection5/455.ann     \n",
      "  inflating: Collection5/455.txt     \n",
      "  inflating: Collection5/457.ann     \n",
      "  inflating: Collection5/457.txt     \n",
      "  inflating: Collection5/458.ann     \n",
      "  inflating: Collection5/458.txt     \n",
      "  inflating: Collection5/459.ann     \n",
      "  inflating: Collection5/459.txt     \n",
      "  inflating: Collection5/460.ann     \n",
      "  inflating: Collection5/460.txt     \n",
      "  inflating: Collection5/461.ann     \n",
      "  inflating: Collection5/461.txt     \n",
      "  inflating: Collection5/462.ann     \n",
      "  inflating: Collection5/462.txt     \n",
      "  inflating: Collection5/463.ann     \n",
      "  inflating: Collection5/463.txt     \n",
      "  inflating: Collection5/464.ann     \n",
      "  inflating: Collection5/464.txt     \n",
      "  inflating: Collection5/465.ann     \n",
      "  inflating: Collection5/465.txt     \n",
      "  inflating: Collection5/466.ann     \n",
      "  inflating: Collection5/466.txt     \n",
      "  inflating: Collection5/467.ann     \n",
      "  inflating: Collection5/467.txt     \n",
      "  inflating: Collection5/468.ann     \n",
      "  inflating: Collection5/468.txt     \n",
      "  inflating: Collection5/469.ann     \n",
      "  inflating: Collection5/469.txt     \n",
      "  inflating: Collection5/470.ann     \n",
      "  inflating: Collection5/470.txt     \n",
      "  inflating: Collection5/471.ann     \n",
      "  inflating: Collection5/471.txt     \n",
      "  inflating: Collection5/472.ann     \n",
      "  inflating: Collection5/472.txt     \n",
      "  inflating: Collection5/473.ann     \n",
      "  inflating: Collection5/473.txt     \n",
      "  inflating: Collection5/474.ann     \n",
      "  inflating: Collection5/474.txt     \n",
      "  inflating: Collection5/475.ann     \n",
      "  inflating: Collection5/475.txt     \n",
      "  inflating: Collection5/476.ann     \n",
      "  inflating: Collection5/476.txt     \n",
      "  inflating: Collection5/477.ann     \n",
      "  inflating: Collection5/477.txt     \n",
      "  inflating: Collection5/478.ann     \n",
      "  inflating: Collection5/478.txt     \n",
      "  inflating: Collection5/479.ann     \n",
      "  inflating: Collection5/479.txt     \n",
      "  inflating: Collection5/480.ann     \n",
      "  inflating: Collection5/480.txt     \n",
      "  inflating: Collection5/481.ann     \n",
      "  inflating: Collection5/481.txt     \n",
      "  inflating: Collection5/482.ann     \n",
      "  inflating: Collection5/482.txt     \n",
      "  inflating: Collection5/483.ann     \n",
      "  inflating: Collection5/483.txt     \n",
      "  inflating: Collection5/484.ann     \n",
      "  inflating: Collection5/484.txt     \n",
      "  inflating: Collection5/485.ann     \n",
      "  inflating: Collection5/485.txt     \n",
      "  inflating: Collection5/486.ann     \n",
      "  inflating: Collection5/486.txt     \n",
      "  inflating: Collection5/487.ann     \n",
      "  inflating: Collection5/487.txt     \n",
      "  inflating: Collection5/488.ann     \n",
      "  inflating: Collection5/488.txt     \n",
      "  inflating: Collection5/489.ann     \n",
      "  inflating: Collection5/489.txt     \n",
      "  inflating: Collection5/490.ann     \n",
      "  inflating: Collection5/490.txt     \n",
      "  inflating: Collection5/491.ann     \n",
      "  inflating: Collection5/491.txt     \n",
      "  inflating: Collection5/492.ann     \n",
      "  inflating: Collection5/492.txt     \n",
      "  inflating: Collection5/493.ann     \n",
      "  inflating: Collection5/493.txt     \n",
      "  inflating: Collection5/494.ann     \n",
      "  inflating: Collection5/494.txt     \n",
      "  inflating: Collection5/495.ann     \n",
      "  inflating: Collection5/495.txt     \n",
      "  inflating: Collection5/496.ann     \n",
      "  inflating: Collection5/496.txt     \n",
      "  inflating: Collection5/497.ann     \n",
      "  inflating: Collection5/497.txt     \n",
      "  inflating: Collection5/498.ann     \n",
      "  inflating: Collection5/498.txt     \n",
      "  inflating: Collection5/499.ann     \n",
      "  inflating: Collection5/499.txt     \n",
      "  inflating: Collection5/500.ann     \n",
      "  inflating: Collection5/500.txt     \n",
      "  inflating: Collection5/501.ann     \n",
      "  inflating: Collection5/501.txt     \n",
      "  inflating: Collection5/502.ann     \n",
      "  inflating: Collection5/502.txt     \n",
      "  inflating: Collection5/503.ann     \n",
      "  inflating: Collection5/503.txt     \n",
      "  inflating: Collection5/504.ann     \n",
      "  inflating: Collection5/504.txt     \n",
      "  inflating: Collection5/505.ann     \n",
      "  inflating: Collection5/505.txt     \n",
      "  inflating: Collection5/506.ann     \n",
      "  inflating: Collection5/506.txt     \n",
      "  inflating: Collection5/507.ann     \n",
      "  inflating: Collection5/507.txt     \n",
      "  inflating: Collection5/508.ann     \n",
      "  inflating: Collection5/508.txt     \n",
      "  inflating: Collection5/509.ann     \n",
      "  inflating: Collection5/509.txt     \n",
      "  inflating: Collection5/510.ann     \n",
      "  inflating: Collection5/510.txt     \n",
      "  inflating: Collection5/511.ann     \n",
      "  inflating: Collection5/511.txt     \n",
      "  inflating: Collection5/512.ann     \n",
      "  inflating: Collection5/512.txt     \n",
      "  inflating: Collection5/513.ann     \n",
      "  inflating: Collection5/513.txt     \n",
      "  inflating: Collection5/514.ann     \n",
      "  inflating: Collection5/514.txt     \n",
      "  inflating: Collection5/515.ann     \n",
      "  inflating: Collection5/515.txt     \n",
      "  inflating: Collection5/516.ann     \n",
      "  inflating: Collection5/516.txt     \n",
      "  inflating: Collection5/517.ann     \n",
      "  inflating: Collection5/517.txt     \n",
      "  inflating: Collection5/518.ann     \n",
      "  inflating: Collection5/518.txt     \n",
      "  inflating: Collection5/519.ann     \n",
      "  inflating: Collection5/519.txt     \n",
      "  inflating: Collection5/520.ann     \n",
      "  inflating: Collection5/520.txt     \n",
      "  inflating: Collection5/521.ann     \n",
      "  inflating: Collection5/521.txt     \n",
      "  inflating: Collection5/522.ann     \n",
      "  inflating: Collection5/522.txt     \n",
      "  inflating: Collection5/523.ann     \n",
      "  inflating: Collection5/523.txt     \n",
      "  inflating: Collection5/524.ann     \n",
      "  inflating: Collection5/524.txt     \n",
      "  inflating: Collection5/525.ann     \n",
      "  inflating: Collection5/525.txt     \n",
      "  inflating: Collection5/526.ann     \n",
      "  inflating: Collection5/526.txt     \n",
      "  inflating: Collection5/527.ann     \n",
      "  inflating: Collection5/527.txt     \n",
      "  inflating: Collection5/528.ann     \n",
      "  inflating: Collection5/528.txt     \n",
      "  inflating: Collection5/529.ann     \n",
      "  inflating: Collection5/529.txt     \n",
      "  inflating: Collection5/530.ann     \n",
      "  inflating: Collection5/530.txt     \n",
      "  inflating: Collection5/531.ann     \n",
      "  inflating: Collection5/531.txt     \n",
      "  inflating: Collection5/532.ann     \n",
      "  inflating: Collection5/532.txt     \n",
      "  inflating: Collection5/533 (!).ann  \n",
      "  inflating: Collection5/533 (!).txt  \n",
      "  inflating: Collection5/534.ann     \n",
      "  inflating: Collection5/534.txt     \n",
      "  inflating: Collection5/535.ann     \n",
      "  inflating: Collection5/535.txt     \n",
      "  inflating: Collection5/536.ann     \n",
      "  inflating: Collection5/536.txt     \n",
      "  inflating: Collection5/537.ann     \n",
      "  inflating: Collection5/537.txt     \n",
      "  inflating: Collection5/538.ann     \n",
      "  inflating: Collection5/538.txt     \n",
      "  inflating: Collection5/539.ann     \n",
      "  inflating: Collection5/539.txt     \n",
      "  inflating: Collection5/540.ann     \n",
      "  inflating: Collection5/540.txt     \n",
      "  inflating: Collection5/541.ann     \n",
      "  inflating: Collection5/541.txt     \n",
      "  inflating: Collection5/542.ann     \n",
      "  inflating: Collection5/542.txt     \n",
      "  inflating: Collection5/543.ann     \n",
      "  inflating: Collection5/543.txt     \n",
      "  inflating: Collection5/544.ann     \n",
      "  inflating: Collection5/544.txt     \n",
      "  inflating: Collection5/545.ann     \n",
      "  inflating: Collection5/545.txt     \n",
      "  inflating: Collection5/546.ann     \n",
      "  inflating: Collection5/546.txt     \n",
      "  inflating: Collection5/547.ann     \n",
      "  inflating: Collection5/547.txt     \n",
      "  inflating: Collection5/548.ann     \n",
      "  inflating: Collection5/548.txt     \n",
      "  inflating: Collection5/549.ann     \n",
      "  inflating: Collection5/549.txt     \n",
      "  inflating: Collection5/550.ann     \n",
      "  inflating: Collection5/550.txt     \n",
      "  inflating: Collection5/551.ann     \n",
      "  inflating: Collection5/551.txt     \n",
      "  inflating: Collection5/552.ann     \n",
      "  inflating: Collection5/552.txt     \n",
      "  inflating: Collection5/553.ann     \n",
      "  inflating: Collection5/553.txt     \n",
      "  inflating: Collection5/554.ann     \n",
      "  inflating: Collection5/554.txt     \n",
      "  inflating: Collection5/555 (!).ann  \n",
      "  inflating: Collection5/555 (!).txt  \n",
      "  inflating: Collection5/556.ann     \n",
      "  inflating: Collection5/556.txt     \n",
      "  inflating: Collection5/557.ann     \n",
      "  inflating: Collection5/557.txt     \n",
      "  inflating: Collection5/558.ann     \n",
      "  inflating: Collection5/558.txt     \n",
      "  inflating: Collection5/559.ann     \n",
      "  inflating: Collection5/559.txt     \n",
      "  inflating: Collection5/560.ann     \n",
      "  inflating: Collection5/560.txt     \n",
      "  inflating: Collection5/561.ann     \n",
      "  inflating: Collection5/561.txt     \n",
      "  inflating: Collection5/562.ann     \n",
      "  inflating: Collection5/562.txt     \n",
      "  inflating: Collection5/563.ann     \n",
      "  inflating: Collection5/563.txt     \n",
      "  inflating: Collection5/564.ann     \n",
      "  inflating: Collection5/564.txt     \n",
      "  inflating: Collection5/565.ann     \n",
      "  inflating: Collection5/565.txt     \n",
      "  inflating: Collection5/567.ann     \n",
      "  inflating: Collection5/567.txt     \n",
      "  inflating: Collection5/568.ann     \n",
      "  inflating: Collection5/568.txt     \n",
      "  inflating: Collection5/569.ann     \n",
      "  inflating: Collection5/569.txt     \n",
      "  inflating: Collection5/570.ann     \n",
      "  inflating: Collection5/570.txt     \n",
      "  inflating: Collection5/571.ann     \n",
      "  inflating: Collection5/571.txt     \n",
      "  inflating: Collection5/572.ann     \n",
      "  inflating: Collection5/572.txt     \n",
      "  inflating: Collection5/574.ann     \n",
      "  inflating: Collection5/574.txt     \n",
      "  inflating: Collection5/575.ann     \n",
      "  inflating: Collection5/575.txt     \n",
      "  inflating: Collection5/576.ann     \n",
      "  inflating: Collection5/576.txt     \n",
      "  inflating: Collection5/577.ann     \n",
      "  inflating: Collection5/577.txt     \n",
      "  inflating: Collection5/578.ann     \n",
      "  inflating: Collection5/578.txt     \n",
      "  inflating: Collection5/579.ann     \n",
      "  inflating: Collection5/579.txt     \n",
      "  inflating: Collection5/581.ann     \n",
      "  inflating: Collection5/581.txt     \n",
      "  inflating: Collection5/582.ann     \n",
      "  inflating: Collection5/582.txt     \n",
      "  inflating: Collection5/583.ann     \n",
      "  inflating: Collection5/583.txt     \n",
      "  inflating: Collection5/584 (!).ann  \n",
      "  inflating: Collection5/584 (!).txt  \n",
      "  inflating: Collection5/585.ann     \n",
      "  inflating: Collection5/585.txt     \n",
      "  inflating: Collection5/586.ann     \n",
      "  inflating: Collection5/586.txt     \n",
      "  inflating: Collection5/587.ann     \n",
      "  inflating: Collection5/587.txt     \n",
      "  inflating: Collection5/588.ann     \n",
      "  inflating: Collection5/588.txt     \n",
      "  inflating: Collection5/589.ann     \n",
      "  inflating: Collection5/589.txt     \n",
      "  inflating: Collection5/590.ann     \n",
      "  inflating: Collection5/590.txt     \n",
      "  inflating: Collection5/591.ann     \n",
      "  inflating: Collection5/591.txt     \n",
      "  inflating: Collection5/592.ann     \n",
      "  inflating: Collection5/592.txt     \n",
      "  inflating: Collection5/593.ann     \n",
      "  inflating: Collection5/593.txt     \n",
      "  inflating: Collection5/594.ann     \n",
      "  inflating: Collection5/594.txt     \n",
      "  inflating: Collection5/595.ann     \n",
      "  inflating: Collection5/595.txt     \n",
      "  inflating: Collection5/596.ann     \n",
      "  inflating: Collection5/596.txt     \n",
      "  inflating: Collection5/597.ann     \n",
      "  inflating: Collection5/597.txt     \n",
      "  inflating: Collection5/598 (!).ann  \n",
      "  inflating: Collection5/598 (!).txt  \n",
      "  inflating: Collection5/599.ann     \n",
      "  inflating: Collection5/599.txt     \n",
      "  inflating: Collection5/600.ann     \n",
      "  inflating: Collection5/600.txt     \n",
      "  inflating: Collection5/601.ann     \n",
      "  inflating: Collection5/601.txt     \n",
      "  inflating: Collection5/602.ann     \n",
      "  inflating: Collection5/602.txt     \n",
      "  inflating: Collection5/610.ann     \n",
      "  inflating: Collection5/610.txt     \n",
      "  inflating: Collection5/611.ann     \n",
      "  inflating: Collection5/611.txt     \n",
      "  inflating: Collection5/612.ann     \n",
      "  inflating: Collection5/612.txt     \n",
      "  inflating: Collection5/613.ann     \n",
      "  inflating: Collection5/613.txt     \n",
      "  inflating: Collection5/614.ann     \n",
      "  inflating: Collection5/614.txt     \n",
      "  inflating: Collection5/615.ann     \n",
      "  inflating: Collection5/615.txt     \n",
      "  inflating: Collection5/616.ann     \n",
      "  inflating: Collection5/616.txt     \n",
      "  inflating: Collection5/617.ann     \n",
      "  inflating: Collection5/617.txt     \n",
      "  inflating: Collection5/618.ann     \n",
      "  inflating: Collection5/618.txt     \n",
      "  inflating: Collection5/619.ann     \n",
      "  inflating: Collection5/619.txt     \n",
      "  inflating: Collection5/620.ann     \n",
      "  inflating: Collection5/620.txt     \n",
      "  inflating: Collection5/621.ann     \n",
      "  inflating: Collection5/621.txt     \n",
      "  inflating: Collection5/622.ann     \n",
      "  inflating: Collection5/622.txt     \n",
      "  inflating: Collection5/623.ann     \n",
      "  inflating: Collection5/623.txt     \n",
      "  inflating: Collection5/624.ann     \n",
      "  inflating: Collection5/624.txt     \n",
      "  inflating: Collection5/625.ann     \n",
      "  inflating: Collection5/625.txt     \n",
      "  inflating: Collection5/626.ann     \n",
      "  inflating: Collection5/626.txt     \n",
      "  inflating: Collection5/627.ann     \n",
      "  inflating: Collection5/627.txt     \n",
      "  inflating: Collection5/628.ann     \n",
      "  inflating: Collection5/628.txt     \n",
      "  inflating: Collection5/629.ann     \n",
      "  inflating: Collection5/629.txt     \n",
      "  inflating: Collection5/630.ann     \n",
      "  inflating: Collection5/630.txt     \n",
      "  inflating: Collection5/631.ann     \n",
      "  inflating: Collection5/631.txt     \n",
      "  inflating: Collection5/632.ann     \n",
      "  inflating: Collection5/632.txt     \n",
      "  inflating: Collection5/633.ann     \n",
      "  inflating: Collection5/633.txt     \n",
      "  inflating: Collection5/abdulatipov.ann  \n",
      "  inflating: Collection5/abdulatipov.txt  \n",
      "  inflating: Collection5/artjakov.ann  \n",
      "  inflating: Collection5/artjakov.txt  \n",
      "  inflating: Collection5/Avtovaz.ann  \n",
      "  inflating: Collection5/Avtovaz.txt  \n",
      "  inflating: Collection5/blokhin.ann  \n",
      "  inflating: Collection5/blokhin.txt  \n",
      "  inflating: Collection5/chaves.ann  \n",
      "  inflating: Collection5/chaves.txt  \n",
      "  inflating: Collection5/chirkunov.ann  \n",
      "  inflating: Collection5/chirkunov.txt  \n",
      "  inflating: Collection5/kamchatka.ann  \n",
      "  inflating: Collection5/kamchatka.txt  \n",
      "  inflating: Collection5/klinton.ann  \n",
      "  inflating: Collection5/klinton.txt  \n",
      "  inflating: Collection5/kuleshov.ann  \n",
      "  inflating: Collection5/kuleshov.txt  \n",
      "  inflating: Collection5/last_01.ann  \n",
      "  inflating: Collection5/last_01.txt  \n",
      "  inflating: Collection5/last_02.ann  \n",
      "  inflating: Collection5/last_02.txt  \n",
      "  inflating: Collection5/last_03.ann  \n",
      "  inflating: Collection5/last_03.txt  \n",
      "  inflating: Collection5/last_04.ann  \n",
      "  inflating: Collection5/last_04.txt  \n",
      "  inflating: Collection5/last_05.ann  \n",
      "  inflating: Collection5/last_05.txt  \n",
      "  inflating: Collection5/last_06.ann  \n",
      "  inflating: Collection5/last_06.txt  \n",
      "  inflating: Collection5/last_07_new.ann  \n",
      "  inflating: Collection5/last_07_new.txt  \n",
      "  inflating: Collection5/last_08.ann  \n",
      "  inflating: Collection5/last_08.txt  \n",
      "  inflating: Collection5/last_09.ann  \n",
      "  inflating: Collection5/last_09.txt  \n",
      "  inflating: Collection5/last_10.ann  \n",
      "  inflating: Collection5/last_10.txt  \n",
      "  inflating: Collection5/last_11.ann  \n",
      "  inflating: Collection5/last_11.txt  \n",
      "  inflating: Collection5/last_12.ann  \n",
      "  inflating: Collection5/last_12.txt  \n",
      "  inflating: Collection5/last_13.ann  \n",
      "  inflating: Collection5/last_13.txt  \n",
      "  inflating: Collection5/last_14.ann  \n",
      "  inflating: Collection5/last_14.txt  \n",
      "  inflating: Collection5/last_15.ann  \n",
      "  inflating: Collection5/last_15.txt  \n",
      "  inflating: Collection5/last_16.ann  \n",
      "  inflating: Collection5/last_16.txt  \n",
      "  inflating: Collection5/last_17.ann  \n",
      "  inflating: Collection5/last_17.txt  \n",
      "  inflating: Collection5/last_18.ann  \n",
      "  inflating: Collection5/last_18.txt  \n",
      "  inflating: Collection5/last_19.ann  \n",
      "  inflating: Collection5/last_19.txt  \n",
      "  inflating: Collection5/last_20.ann  \n",
      "  inflating: Collection5/last_20.txt  \n",
      "  inflating: Collection5/last_21.ann  \n",
      "  inflating: Collection5/last_21.txt  \n",
      "  inflating: Collection5/last_22.ann  \n",
      "  inflating: Collection5/last_22.txt  \n",
      "  inflating: Collection5/last_23.ann  \n",
      "  inflating: Collection5/last_23.txt  \n",
      "  inflating: Collection5/last_24.ann  \n",
      "  inflating: Collection5/last_24.txt  \n",
      "  inflating: Collection5/last_25.ann  \n",
      "  inflating: Collection5/last_25.txt  \n",
      "  inflating: Collection5/last_26.ann  \n",
      "  inflating: Collection5/last_26.txt  \n",
      "  inflating: Collection5/last_27.ann  \n",
      "  inflating: Collection5/last_27.txt  \n",
      "  inflating: Collection5/last_28.ann  \n",
      "  inflating: Collection5/last_28.txt  \n",
      "  inflating: Collection5/last_29.ann  \n",
      "  inflating: Collection5/last_29.txt  \n",
      "  inflating: Collection5/last_30_new.ann  \n",
      "  inflating: Collection5/last_30_new.txt  \n",
      "  inflating: Collection5/last_31.ann  \n",
      "  inflating: Collection5/last_31.txt  \n",
      "  inflating: Collection5/last_32.ann  \n",
      "  inflating: Collection5/last_32.txt  \n",
      "  inflating: Collection5/last_33.ann  \n",
      "  inflating: Collection5/last_33.txt  \n",
      "  inflating: Collection5/last_34.ann  \n",
      "  inflating: Collection5/last_34.txt  \n",
      "  inflating: Collection5/last_35.ann  \n",
      "  inflating: Collection5/last_35.txt  \n",
      "  inflating: Collection5/last_36.ann  \n",
      "  inflating: Collection5/last_36.txt  \n",
      "  inflating: Collection5/last_37.ann  \n",
      "  inflating: Collection5/last_37.txt  \n",
      "  inflating: Collection5/last_38.ann  \n",
      "  inflating: Collection5/last_38.txt  \n",
      "  inflating: Collection5/last_39.ann  \n",
      "  inflating: Collection5/last_39.txt  \n",
      "  inflating: Collection5/last_40.ann  \n",
      "  inflating: Collection5/last_40.txt  \n",
      "  inflating: Collection5/last_41.ann  \n",
      "  inflating: Collection5/last_41.txt  \n",
      "  inflating: Collection5/last_42.ann  \n",
      "  inflating: Collection5/last_42.txt  \n",
      "  inflating: Collection5/last_43.ann  \n",
      "  inflating: Collection5/last_43.txt  \n",
      "  inflating: Collection5/last_44.ann  \n",
      "  inflating: Collection5/last_44.txt  \n",
      "  inflating: Collection5/last_45.ann  \n",
      "  inflating: Collection5/last_45.txt  \n",
      "  inflating: Collection5/last_46.ann  \n",
      "  inflating: Collection5/last_46.txt  \n",
      "  inflating: Collection5/last_47.ann  \n",
      "  inflating: Collection5/last_47.txt  \n",
      "  inflating: Collection5/last_48.ann  \n",
      "  inflating: Collection5/last_48.txt  \n",
      "  inflating: Collection5/last_49.ann  \n",
      "  inflating: Collection5/last_49.txt  \n",
      "  inflating: Collection5/last_50.ann  \n",
      "  inflating: Collection5/last_50.txt  \n",
      "  inflating: Collection5/last_51.ann  \n",
      "  inflating: Collection5/last_51.txt  \n",
      "  inflating: Collection5/last_52.ann  \n",
      "  inflating: Collection5/last_52.txt  \n",
      "  inflating: Collection5/last_53.ann  \n",
      "  inflating: Collection5/last_53.txt  \n",
      "  inflating: Collection5/last_54.ann  \n",
      "  inflating: Collection5/last_54.txt  \n",
      "  inflating: Collection5/last_55.ann  \n",
      "  inflating: Collection5/last_55.txt  \n",
      "  inflating: Collection5/last_56.ann  \n",
      "  inflating: Collection5/last_56.txt  \n",
      "  inflating: Collection5/last_57.ann  \n",
      "  inflating: Collection5/last_57.txt  \n",
      "  inflating: Collection5/last_58.ann  \n",
      "  inflating: Collection5/last_58.txt  \n",
      "  inflating: Collection5/last_59.ann  \n",
      "  inflating: Collection5/last_59.txt  \n",
      "  inflating: Collection5/last_60.ann  \n",
      "  inflating: Collection5/last_60.txt  \n",
      "  inflating: Collection5/last_61.ann  \n",
      "  inflating: Collection5/last_61.txt  \n",
      "  inflating: Collection5/last_62.ann  \n",
      "  inflating: Collection5/last_62.txt  \n",
      "  inflating: Collection5/last_63.ann  \n",
      "  inflating: Collection5/last_63.txt  \n",
      "  inflating: Collection5/last_64.ann  \n",
      "  inflating: Collection5/last_64.txt  \n",
      "  inflating: Collection5/last_65.ann  \n",
      "  inflating: Collection5/last_65.txt  \n",
      "  inflating: Collection5/last_66.ann  \n",
      "  inflating: Collection5/last_66.txt  \n",
      "  inflating: Collection5/last_67.ann  \n",
      "  inflating: Collection5/last_67.txt  \n",
      "  inflating: Collection5/last_68.ann  \n",
      "  inflating: Collection5/last_68.txt  \n",
      "  inflating: Collection5/last_69.ann  \n",
      "  inflating: Collection5/last_69.txt  \n",
      "  inflating: Collection5/last_70.ann  \n",
      "  inflating: Collection5/last_70.txt  \n",
      "  inflating: Collection5/last_71.ann  \n",
      "  inflating: Collection5/last_71.txt  \n",
      "  inflating: Collection5/last_72.ann  \n",
      "  inflating: Collection5/last_72.txt  \n",
      "  inflating: Collection5/last_73.ann  \n",
      "  inflating: Collection5/last_73.txt  \n",
      "  inflating: Collection5/last_74.ann  \n",
      "  inflating: Collection5/last_74.txt  \n",
      "  inflating: Collection5/last_75.ann  \n",
      "  inflating: Collection5/last_75.txt  \n",
      "  inflating: Collection5/lenoblast.ann  \n",
      "  inflating: Collection5/lenoblast.txt  \n",
      "  inflating: Collection5/maykl dzhekson.ann  \n",
      "  inflating: Collection5/maykl dzhekson.txt  \n",
      "  inflating: Collection5/mvd.ann     \n",
      "  inflating: Collection5/mvd.txt     \n",
      "  inflating: Collection5/mvd2.ann    \n",
      "  inflating: Collection5/mvd2.txt    \n",
      "  inflating: Collection5/rosobrnadzor.ann  \n",
      "  inflating: Collection5/rosobrnadzor.txt  \n",
      "  inflating: Collection5/ryadovoy chelah.ann  \n",
      "  inflating: Collection5/ryadovoy chelah.txt  \n",
      "  inflating: Collection5/semenenko.ann  \n",
      "  inflating: Collection5/semenenko.txt  \n",
      "  inflating: Collection5/shojgu1.ann  \n",
      "  inflating: Collection5/shojgu1.txt  \n",
      "  inflating: Collection5/shojgu3.ann  \n",
      "  inflating: Collection5/shojgu3.txt  \n",
      "  inflating: Collection5/shojgu4.ann  \n",
      "  inflating: Collection5/shojgu4.txt  \n",
      "  inflating: Collection5/shojgu6.ann  \n",
      "  inflating: Collection5/shojgu6.txt  \n",
      "  inflating: Collection5/si_tzjanpin.ann  \n",
      "  inflating: Collection5/si_tzjanpin.txt  \n",
      "  inflating: Collection5/sobjanin2.ann  \n",
      "  inflating: Collection5/sobjanin2.txt  \n",
      "  inflating: Collection5/turkmenija.ann  \n",
      "  inflating: Collection5/turkmenija.txt  \n",
      "  inflating: Collection5/uchitel.ann  \n",
      "  inflating: Collection5/uchitel.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip collection5.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djNnrfsCQcBe",
   "metadata": {
    "id": "djNnrfsCQcBe"
   },
   "source": [
    "Теперь посмотим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "IDAlV9IhQYES",
   "metadata": {
    "id": "IDAlV9IhQYES"
   },
   "outputs": [],
   "source": [
    "records = load_ne5('Collection5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "QThR7BNdQYLb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "QThR7BNdQYLb",
    "outputId": "182c0dfe-1d88-4cbc-ebef-5e9a55a54629"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'В Финляндии сменился премьер-министр Парламент Финляндии в среду, 22 июня, проголосовал по кандидатуре премьер-министра страны. Как сообщает финская телерадиовещательная компания YLE, им стал бывший министр финансов Юрки Катайнен (Jyrki Katainen). Глава правительства был избран спустя более чем два месяца после парламентских выборов. Как сообщает Agence France-Presse, для Финляндии этот срок является рекордным. Победителем на выборах, прошедших 17 апреля, стала Коалиционная партия, во главе которой стоит Катайнен. Вместе с ней в коалицию также войдут Социал-демократическая партия, Союз левых сил, \"Зеленые\", Шведская народная партия, а также Христианские демократы. В то же время националистическая партия \"Истинные финны\" от участия в правительстве отказалась. В среду кандидатуру Катайнена на посту премьер-министр утвердит президент Финляндии Тарья Халонен. '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = next(records)\n",
    "text = document.text\n",
    "\n",
    "text = re.sub('\\r\\n\\r\\n',' ',text)\n",
    "text = re.sub('\\r\\n',' ',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "LD7LyvyORU9M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LD7LyvyORU9M",
    "outputId": "5bbe9fef-d63e-4989-fb9a-080eaaa6c8d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "-fmKx4HMQYOQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fmKx4HMQYOQ",
    "outputId": "fc5c73c6-b61d-4b8f-d76b-1264c373fbc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'JJ'),\n",
       " ('Финляндии', 'NNP'),\n",
       " ('сменился', 'NNP'),\n",
       " ('премьер-министр', 'JJ'),\n",
       " ('Парламент', 'NNP'),\n",
       " ('Финляндии', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('среду', 'NNP'),\n",
       " (',', ','),\n",
       " ('22', 'CD'),\n",
       " ('июня', 'NN'),\n",
       " (',', ','),\n",
       " ('проголосовал', 'NNP'),\n",
       " ('по', 'NNP'),\n",
       " ('кандидатуре', 'NNP'),\n",
       " ('премьер-министра', 'JJ'),\n",
       " ('страны', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Как', 'VB'),\n",
       " ('сообщает', 'JJ'),\n",
       " ('финская', 'NNP'),\n",
       " ('телерадиовещательная', 'NNP'),\n",
       " ('компания', 'NNP'),\n",
       " ('YLE', 'NNP'),\n",
       " (',', ','),\n",
       " ('им', 'NNP'),\n",
       " ('стал', 'NNP'),\n",
       " ('бывший', 'NNP'),\n",
       " ('министр', 'NNP'),\n",
       " ('финансов', 'NNP'),\n",
       " ('Юрки', 'NNP'),\n",
       " ('Катайнен', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Jyrki', 'NNP'),\n",
       " ('Katainen', 'NNP'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('Глава', 'JJ'),\n",
       " ('правительства', 'JJ'),\n",
       " ('был', 'NN'),\n",
       " ('избран', 'NNP'),\n",
       " ('спустя', 'NNP'),\n",
       " ('более', 'NNP'),\n",
       " ('чем', 'NNP'),\n",
       " ('два', 'NNP'),\n",
       " ('месяца', 'NNP'),\n",
       " ('после', 'NNP'),\n",
       " ('парламентских', 'NNP'),\n",
       " ('выборов', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Как', 'VB'),\n",
       " ('сообщает', 'JJ'),\n",
       " ('Agence', 'NNP'),\n",
       " ('France-Presse', 'NNP'),\n",
       " (',', ','),\n",
       " ('для', 'NNP'),\n",
       " ('Финляндии', 'NNP'),\n",
       " ('этот', 'NNP'),\n",
       " ('срок', 'NNP'),\n",
       " ('является', 'NNP'),\n",
       " ('рекордным', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Победителем', 'VB'),\n",
       " ('на', 'JJ'),\n",
       " ('выборах', 'NNP'),\n",
       " (',', ','),\n",
       " ('прошедших', 'VBD'),\n",
       " ('17', 'CD'),\n",
       " ('апреля', 'NN'),\n",
       " (',', ','),\n",
       " ('стала', 'NNP'),\n",
       " ('Коалиционная', 'NNP'),\n",
       " ('партия', 'NNP'),\n",
       " (',', ','),\n",
       " ('во', 'NNP'),\n",
       " ('главе', 'NNP'),\n",
       " ('которой', 'NNP'),\n",
       " ('стоит', 'NNP'),\n",
       " ('Катайнен', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Вместе', 'VB'),\n",
       " ('с', 'JJ'),\n",
       " ('ней', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('коалицию', 'NNP'),\n",
       " ('также', 'NNP'),\n",
       " ('войдут', 'NNP'),\n",
       " ('Социал-демократическая', 'JJ'),\n",
       " ('партия', 'NNP'),\n",
       " (',', ','),\n",
       " ('Союз', 'NNP'),\n",
       " ('левых', 'NNP'),\n",
       " ('сил', 'NNP'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('Зеленые', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('Шведская', 'NNP'),\n",
       " ('народная', 'NNP'),\n",
       " ('партия', 'NNP'),\n",
       " (',', ','),\n",
       " ('а', 'NNP'),\n",
       " ('также', 'NNP'),\n",
       " ('Христианские', 'NNP'),\n",
       " ('демократы', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('В', 'VB'),\n",
       " ('то', 'JJ'),\n",
       " ('же', 'NNP'),\n",
       " ('время', 'NNP'),\n",
       " ('националистическая', 'NNP'),\n",
       " ('партия', 'NNP'),\n",
       " ('``', '``'),\n",
       " ('Истинные', 'FW'),\n",
       " ('финны', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('от', 'CC'),\n",
       " ('участия', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('правительстве', 'NNP'),\n",
       " ('отказалась', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('В', 'VB'),\n",
       " ('среду', 'JJ'),\n",
       " ('кандидатуру', 'NNP'),\n",
       " ('Катайнена', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('посту', 'NNP'),\n",
       " ('премьер-министр', 'JJ'),\n",
       " ('утвердит', 'NNP'),\n",
       " ('президент', 'NNP'),\n",
       " ('Финляндии', 'NNP'),\n",
       " ('Тарья', 'NNP'),\n",
       " ('Халонен', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.text = text\n",
    "# Предварительная очистка статей в словаре\n",
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "034Nya8fQoPV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "034Nya8fQoPV",
    "outputId": "e12eb6f4-65e6-40e5-8244-0cf86120cc9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Jyrki Katainen', 'PERSON'),\n",
       " ('Парламент Финляндии', 'PERSON'),\n",
       " ('Союз', 'PERSON'),\n",
       " ('Финляндии', 'PERSON'),\n",
       " ('Шведская', 'PERSON')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "JRiPAg5-QoSd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRiPAg5-QoSd",
    "outputId": "e9b26e19-7c9d-4ec3-c1c6-8a00a6ab7686"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ne5Span(\n",
       "     index='T1',\n",
       "     type='GEOPOLIT',\n",
       "     start=2,\n",
       "     stop=11,\n",
       "     text='Финляндии'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T2',\n",
       "     type='GEOPOLIT',\n",
       "     start=50,\n",
       "     stop=59,\n",
       "     text='Финляндии'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T3',\n",
       "     type='MEDIA',\n",
       "     start=182,\n",
       "     stop=185,\n",
       "     text='YLE'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T4',\n",
       "     type='PER',\n",
       "     start=219,\n",
       "     stop=249,\n",
       "     text='Юрки Катайнен (Jyrki Katainen)'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T5',\n",
       "     type='MEDIA',\n",
       "     start=355,\n",
       "     stop=375,\n",
       "     text='Agence France-Presse'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T6',\n",
       "     type='GEOPOLIT',\n",
       "     start=381,\n",
       "     stop=390,\n",
       "     text='Финляндии'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T7',\n",
       "     type='ORG',\n",
       "     start=475,\n",
       "     stop=494,\n",
       "     text='Коалиционная партия'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T8',\n",
       "     type='PER',\n",
       "     start=519,\n",
       "     stop=527,\n",
       "     text='Катайнен'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T9',\n",
       "     type='ORG',\n",
       "     start=566,\n",
       "     stop=595,\n",
       "     text='Социал-демократическая партия'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T10',\n",
       "     type='ORG',\n",
       "     start=597,\n",
       "     stop=611,\n",
       "     text='Союз левых сил'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T11',\n",
       "     type='ORG',\n",
       "     start=614,\n",
       "     stop=621,\n",
       "     text='Зеленые'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T12',\n",
       "     type='ORG',\n",
       "     start=624,\n",
       "     stop=648,\n",
       "     text='Шведская народная партия'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T13',\n",
       "     type='ORG',\n",
       "     start=658,\n",
       "     stop=680,\n",
       "     text='Христианские демократы'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T14',\n",
       "     type='ORG',\n",
       "     start=723,\n",
       "     stop=737,\n",
       "     text='Истинные финны'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T15',\n",
       "     type='PER',\n",
       "     start=801,\n",
       "     stop=810,\n",
       "     text='Катайнена'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T16',\n",
       "     type='GEOPOLIT',\n",
       "     start=855,\n",
       "     stop=864,\n",
       "     text='Финляндии'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T17',\n",
       "     type='PER',\n",
       "     start=865,\n",
       "     stop=878,\n",
       "     text='Тарья Халонен'\n",
       " )]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gjJRYqulR-Yy",
   "metadata": {
    "id": "gjJRYqulR-Yy"
   },
   "source": [
    "Поработаем со SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eN85tnfNQYQ3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eN85tnfNQYQ3",
    "outputId": "dfe8ece8-562a-45ed-db69-ff02f2a9ccb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 14:40:55.436191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.7.0) (3.7.1)\n",
      "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3-1.2.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt-ng>=0.6 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading docopt_ng-0.9.0-py3-none-any.whl (16 kB)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.3)\n",
      "Installing collected packages: pymorphy3-dicts-ru, dawg-python, docopt-ng, pymorphy3, ru-core-news-sm\n",
      "Successfully installed dawg-python-0.7.2 docopt-ng-0.9.0 pymorphy3-1.2.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "auiaxIwfR49l",
   "metadata": {
    "id": "auiaxIwfR49l"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import ru_core_news_sm\n",
    "from spacy.lang.ru.examples import sentences\n",
    "from spacy.lang.ru import Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZetjDNoISeX4",
   "metadata": {
    "id": "ZetjDNoISeX4"
   },
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "Mtv0dqNYSPgG",
   "metadata": {
    "id": "Mtv0dqNYSPgG"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "MNRphTdzSPi_",
   "metadata": {
    "id": "MNRphTdzSPi_"
   },
   "outputs": [],
   "source": [
    "news_bb = text\n",
    "article = nlp(news_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ObLpl0lSTCQ6",
   "metadata": {
    "id": "ObLpl0lSTCQ6"
   },
   "source": [
    "Подсветим теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bm7g1PNbR5AW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "bm7g1PNbR5AW",
    "outputId": "288cf3b4-1dc0-4277-b188-28f0f0418d5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">В \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Финляндии\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " сменился премьер-министр \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Парламент\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Финляндии\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в среду, 22 июня, проголосовал по кандидатуре премьер-министра страны. Как сообщает финская телерадиовещательная компания \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    YLE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", им стал бывший министр финансов \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Юрки Катайнен (Jyrki Katainen)\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". Глава правительства был избран спустя более чем два месяца после парламентских выборов. Как сообщает \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Agence France-Presse\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", для \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Финляндии\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " этот срок является рекордным. Победителем на выборах, прошедших 17 апреля, стала \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Коалиционная партия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", во главе которой стоит \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Катайнен\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". Вместе с ней в коалицию также войдут \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Социал-демократическая партия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Союз левых сил\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", &quot;\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Зеленые\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "&quot;, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Шведская народная партия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", а также Христианские демократы. В то же время националистическая партия &quot;\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Истинные финны\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "&quot; от участия в правительстве отказалась. В среду кандидатуру \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Катайнена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " на посту премьер-министр утвердит президент \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Финляндии\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Тарья Халонен\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2drS7kp3TS5w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2drS7kp3TS5w",
    "outputId": "0812fa6c-82b2-4eb1-c219-e288bd128b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В ADP case\n",
      "Финляндии PROPN obl\n",
      "сменился VERB ROOT\n",
      "премьер NOUN nsubj\n",
      "- NOUN nsubj\n",
      "министр NOUN nsubj\n",
      "Парламент PROPN appos\n",
      "Финляндии PROPN nmod\n",
      "в ADP case\n",
      "среду NOUN obl\n",
      ", PUNCT punct\n",
      "22 ADJ nmod\n",
      "июня NOUN flat\n",
      ", PUNCT punct\n",
      "проголосовал VERB conj\n",
      "по ADP case\n",
      "кандидатуре NOUN obl\n",
      "премьер NOUN nmod\n",
      "- NOUN nmod\n",
      "министра NOUN nmod\n",
      "страны NOUN nmod\n",
      ". PUNCT punct\n",
      "Как SCONJ mark\n",
      "сообщает VERB parataxis\n",
      "финская ADJ amod\n",
      "телерадиовещательная ADJ amod\n",
      "компания NOUN nsubj\n",
      "YLE PROPN appos\n",
      ", PUNCT punct\n",
      "им PRON xcomp\n",
      "стал VERB ROOT\n",
      "бывший ADJ amod\n",
      "министр NOUN nsubj\n",
      "финансов NOUN nmod\n",
      "Юрки PROPN appos\n",
      "Катайнен PROPN flat:name\n",
      "( PUNCT punct\n",
      "Jyrki PROPN appos\n",
      "Katainen PROPN flat:name\n",
      ") PUNCT punct\n",
      ". PUNCT punct\n",
      "Глава NOUN nsubj:pass\n",
      "правительства NOUN nmod\n",
      "был AUX aux:pass\n",
      "избран VERB ROOT\n",
      "спустя ADP case\n",
      "более ADV case\n",
      "чем SCONJ fixed\n",
      "два NUM nummod:gov\n",
      "месяца NOUN obl\n",
      "после ADP case\n",
      "парламентских ADJ amod\n",
      "выборов NOUN nmod\n",
      ". PUNCT punct\n",
      "Как SCONJ mark\n",
      "сообщает VERB parataxis\n",
      "Agence X nsubj\n",
      "France X flat:foreign\n",
      "- X flat:foreign\n",
      "Presse X flat:foreign\n",
      ", PUNCT punct\n",
      "для ADP case\n",
      "Финляндии PROPN obl\n",
      "этот DET det\n",
      "срок NOUN nsubj\n",
      "является VERB ROOT\n",
      "рекордным ADJ xcomp\n",
      ". PUNCT punct\n",
      "Победителем NOUN xcomp\n",
      "на ADP case\n",
      "выборах NOUN nmod\n",
      ", PUNCT punct\n",
      "прошедших VERB acl\n",
      "17 ADJ obl\n",
      "апреля NOUN flat\n",
      ", PUNCT punct\n",
      "стала VERB ROOT\n",
      "Коалиционная ADJ amod\n",
      "партия NOUN nsubj\n",
      ", PUNCT punct\n",
      "во ADP case\n",
      "главе NOUN fixed\n",
      "которой PRON obl\n",
      "стоит VERB acl:relcl\n",
      "Катайнен PROPN nsubj\n",
      ". PUNCT punct\n",
      "Вместе ADV advmod\n",
      "с ADP case\n",
      "ней PRON obl\n",
      "в ADP case\n",
      "коалицию NOUN obl\n",
      "также ADV advmod\n",
      "войдут VERB ROOT\n",
      "Социал ADJ amod\n",
      "- ADJ amod\n",
      "демократическая ADJ amod\n",
      "партия NOUN nsubj\n",
      ", PUNCT punct\n",
      "Союз PROPN conj\n",
      "левых ADJ amod\n",
      "сил NOUN nmod\n",
      ", PUNCT punct\n",
      "\" PUNCT punct\n",
      "Зеленые ADJ conj\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "Шведская ADJ amod\n",
      "народная ADJ amod\n",
      "партия NOUN conj\n",
      ", PUNCT punct\n",
      "а CCONJ cc\n",
      "также ADV fixed\n",
      "Христианские ADJ amod\n",
      "демократы NOUN conj\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "то DET det\n",
      "же PART advmod\n",
      "время NOUN obl\n",
      "националистическая ADJ amod\n",
      "партия NOUN nsubj\n",
      "\" PUNCT punct\n",
      "Истинные ADJ amod\n",
      "финны NOUN appos\n",
      "\" PUNCT punct\n",
      "от ADP case\n",
      "участия NOUN obl\n",
      "в ADP case\n",
      "правительстве NOUN nmod\n",
      "отказалась VERB ROOT\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "среду NOUN obl\n",
      "кандидатуру NOUN obj\n",
      "Катайнена PROPN nmod\n",
      "на ADP case\n",
      "посту NOUN nmod\n",
      "премьер NOUN appos\n",
      "- NOUN nsubj\n",
      "министр NOUN obj\n",
      "утвердит VERB ROOT\n",
      "президент NOUN nsubj\n",
      "Финляндии PROPN nmod\n",
      "Тарья PROPN appos\n",
      "Халонен PROPN flat:name\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "#Покажем списком токены и части речи\n",
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T8tTebnvUINs",
   "metadata": {
    "id": "T8tTebnvUINs"
   },
   "source": [
    "Написать свой NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "qU4HSdg7Tq1e",
   "metadata": {
    "id": "qU4HSdg7Tq1e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from razdel import tokenize\n",
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "qZc25idrTq4G",
   "metadata": {
    "id": "qZc25idrTq4G"
   },
   "outputs": [],
   "source": [
    "def get_classification_report(y_test_true, y_test_pred):\n",
    "    print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "GSbg2xQqTq8S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSbg2xQqTq8S",
    "outputId": "29650784-cf25-48ac-bbf7-8da6a1e5244a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='315',\n",
       "    text='В Финляндии сменился премьер-министр\\r\\n\\r\\nПарламент Финляндии в среду, 22 июня, проголосовал по кандидатуре премьер-министра страны. Как сообщает финская телерадиовещательная компания YLE, им стал бывший министр финансов Юрки Катайнен (Jyrki Katainen).\\r\\n\\r\\nГлава правительства был избран спустя более чем два месяца после парламентских выборов. Как сообщает Agence France-Presse, для Финляндии этот срок является рекордным.\\r\\n\\r\\nПобедителем на выборах, прошедших 17 апреля, стала Коалиционная партия, во главе которой стоит Катайнен. Вместе с ней в коалицию также войдут Социал-демократическая партия, Союз левых сил, \"Зеленые\", Шведская народная партия, а также Христианские демократы. В то же время националистическая партия \"Истинные финны\" от участия в правительстве отказалась.\\r\\n\\r\\nВ среду кандидатуру Катайнена на посту премьер-министр утвердит президент Финляндии Тарья Халонен. ',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='GEOPOLIT',\n",
       "         start=2,\n",
       "         stop=11,\n",
       "         text='Финляндии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='GEOPOLIT',\n",
       "         start=50,\n",
       "         stop=59,\n",
       "         text='Финляндии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='MEDIA',\n",
       "         start=182,\n",
       "         stop=185,\n",
       "         text='YLE'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='PER',\n",
       "         start=219,\n",
       "         stop=249,\n",
       "         text='Юрки Катайнен (Jyrki Katainen)'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='MEDIA',\n",
       "         start=355,\n",
       "         stop=375,\n",
       "         text='Agence France-Presse'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='GEOPOLIT',\n",
       "         start=381,\n",
       "         stop=390,\n",
       "         text='Финляндии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='ORG',\n",
       "         start=475,\n",
       "         stop=494,\n",
       "         text='Коалиционная партия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='PER',\n",
       "         start=519,\n",
       "         stop=527,\n",
       "         text='Катайнен'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='ORG',\n",
       "         start=566,\n",
       "         stop=595,\n",
       "         text='Социал-демократическая партия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='ORG',\n",
       "         start=597,\n",
       "         stop=611,\n",
       "         text='Союз левых сил'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='ORG',\n",
       "         start=614,\n",
       "         stop=621,\n",
       "         text='Зеленые'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='ORG',\n",
       "         start=624,\n",
       "         stop=648,\n",
       "         text='Шведская народная партия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='ORG',\n",
       "         start=658,\n",
       "         stop=680,\n",
       "         text='Христианские демократы'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='ORG',\n",
       "         start=723,\n",
       "         stop=737,\n",
       "         text='Истинные финны'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='PER',\n",
       "         start=801,\n",
       "         stop=810,\n",
       "         text='Катайнена'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='GEOPOLIT',\n",
       "         start=855,\n",
       "         stop=864,\n",
       "         text='Финляндии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='PER',\n",
       "         start=865,\n",
       "         stop=878,\n",
       "         text='Тарья Халонен'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Возьмем корпус текстов\n",
    "records = load_ne5('Collection5/')\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "MdK2QDQPUSGm",
   "metadata": {
    "id": "MdK2QDQPUSGm"
   },
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "LpcHKGATUSJR",
   "metadata": {
    "id": "LpcHKGATUSJR"
   },
   "outputs": [],
   "source": [
    "#Сделаем новый датафрейм\n",
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "zMGktXtSUSMB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMGktXtSUSMB",
    "outputId": "25ba3d3b-3796-4afe-fa5f-87116a1be8b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219110\n",
       "PER          21190\n",
       "ORG          13636\n",
       "LOC           4568\n",
       "GEOPOLIT      4352\n",
       "MEDIA         2479\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6gOGpMBUU7Ev",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6gOGpMBUU7Ev",
    "outputId": "6a16f9e5-64e4-44d7-af1d-60d013f78a75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b52f54f8-1793-42b1-b7cd-71d93baa3b34\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Путин</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>освободил</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>от</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b52f54f8-1793-42b1-b7cd-71d93baa3b34')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b52f54f8-1793-42b1-b7cd-71d93baa3b34 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b52f54f8-1793-42b1-b7cd-71d93baa3b34');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ac3cd758-56c1-4696-b9ac-5bb12e0966bb\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac3cd758-56c1-4696-b9ac-5bb12e0966bb')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ac3cd758-56c1-4696-b9ac-5bb12e0966bb button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        word  tag\n",
       "0          В  PER\n",
       "1          .  PER\n",
       "2      Путин  PER\n",
       "3  освободил  OUT\n",
       "4         от  OUT"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "q5DBtnZWU7H3",
   "metadata": {
    "id": "q5DBtnZWU7H3"
   },
   "outputs": [],
   "source": [
    "#Распределим на трейн и тест\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "LhfA_0e0U7Kk",
   "metadata": {
    "id": "LhfA_0e0U7Kk"
   },
   "outputs": [],
   "source": [
    "#Зададим целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bzpaED_nVhke",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzpaED_nVhke",
    "outputId": "58588e9d-09fd-44dc-8396-43c2f9a1bacf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_  #всего 6 классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "wdb5II-3VhnB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdb5II-3VhnB",
    "outputId": "fecabbbb-efcf-41cb-caa9-84dd43d67686"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.apply(len).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "P-szJ3pEVhpj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-szJ3pEVhpj",
    "outputId": "417b9cb3-2366-4d1b-8525-3b75880ad342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245016             ранее\n",
       "264665             Центр\n",
       "78107            премьер\n",
       "246937             равно\n",
       "68356      представитель\n",
       "               ...      \n",
       "121106          позволит\n",
       "41877     Исполнительный\n",
       "126486            Замана\n",
       "10461             газета\n",
       "166858                 в\n",
       "Name: word, Length: 66334, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "DXiWOtu5VhsF",
   "metadata": {
    "id": "DXiWOtu5VhsF"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hsy6GP34U7NZ",
   "metadata": {
    "id": "hsy6GP34U7NZ"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1qaRekkVWBkV",
   "metadata": {
    "id": "1qaRekkVWBkV"
   },
   "outputs": [],
   "source": [
    "def custom_net(input_data):\n",
    "\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "                            standardize=custom_net,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dVx4nd8mWhS_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVx4nd8mWhS_",
    "outputId": "c8bd0bb7-77e2-4e5a-f989-d6486565853b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29792"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9BNQjbNhWk3K",
   "metadata": {
    "id": "9BNQjbNhWk3K"
   },
   "outputs": [],
   "source": [
    "#эмбединги\n",
    "embedding_dim = 32\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax') # [OUT, PER, ORG, LOC, GEOPOLIT, MEDIA]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "\n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "\n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "Obru4tBKWk5m",
   "metadata": {
    "id": "Obru4tBKWk5m"
   },
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8tfZ1iDWzvB",
   "metadata": {
    "id": "A8tfZ1iDWzvB"
   },
   "source": [
    "Компилируем и обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aHxCePqCWk8P",
   "metadata": {
    "id": "aHxCePqCWk8P"
   },
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "RAhFi0OMW5vs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAhFi0OMW5vs",
    "outputId": "8f5fa4b5-3f5c-40ea-e7ec-bacb79b7e8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12438/12438 [==============================] - 213s 16ms/step - loss: 0.3079 - accuracy: 0.9098 - val_loss: 0.2221 - val_accuracy: 0.9346\n",
      "Epoch 2/3\n",
      "12438/12438 [==============================] - 198s 16ms/step - loss: 0.1317 - accuracy: 0.9616 - val_loss: 0.3430 - val_accuracy: 0.8921\n",
      "Epoch 3/3\n",
      "12438/12438 [==============================] - 177s 14ms/step - loss: 0.1120 - accuracy: 0.9655 - val_loss: 0.2920 - val_accuracy: 0.8928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c1e612b1de0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "uEmdxKhcW5yI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEmdxKhcW5yI",
    "outputId": "6640eb7d-f974-4414-d67c-492712a053e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)\n",
    "#предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "OY1R2YstW50y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OY1R2YstW50y",
    "outputId": "8c9c11f7-4313-4e7f-91a5-761fa7190567"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8996003324554235"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "AGzu6802W53P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGzu6802W53P",
    "outputId": "9e8f275d-1821-4a6c-d0a4-e21106309389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\r\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      1109\n",
      "           1       0.87      0.76      0.81      1127\n",
      "           2       0.92      0.77      0.84       627\n",
      "           3       0.87      0.55      0.67      3455\n",
      "           4       0.97      0.92      0.94     54660\n",
      "           5       0.50      0.88      0.63      5356\n",
      "\n",
      "    accuracy                           0.89     66334\n",
      "   macro avg       0.84      0.80      0.80     66334\n",
      "weighted avg       0.92      0.89      0.90     66334\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0    1    2     3      4     5\n",
      "row_0                                  \n",
      "0      985    7    1    61      9    46\n",
      "1       29  858    0    22     43   175\n",
      "2        5    7  485     8     67    55\n",
      "3       64   38   26  1910   1010   407\n",
      "4       13   75   14   200  50290  4068\n",
      "5        0    2    0     5    651  4698\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "_jegNfv5cCAL",
   "metadata": {
    "id": "_jegNfv5cCAL"
   },
   "outputs": [],
   "source": [
    "def custom_net(input_data):\n",
    "    # обучим сеть на биграммах и триграммах\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "                            standardize=custom_net,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9TXsjfqRcCDa",
   "metadata": {
    "id": "9TXsjfqRcCDa"
   },
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sV-JiRYVcjCU",
   "metadata": {
    "id": "sV-JiRYVcjCU"
   },
   "source": [
    "Компилируем и обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cF_77LQZcCGT",
   "metadata": {
    "id": "cF_77LQZcCGT"
   },
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "lmJMDXlKcYWA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmJMDXlKcYWA",
    "outputId": "c45c1277-f1d6-43a0-abd1-0b75852a8d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12438/12438 [==============================] - 201s 16ms/step - loss: 0.3058 - accuracy: 0.9103 - val_loss: 0.2232 - val_accuracy: 0.9359\n",
      "Epoch 2/3\n",
      "12438/12438 [==============================] - 204s 16ms/step - loss: 0.1308 - accuracy: 0.9617 - val_loss: 0.2377 - val_accuracy: 0.9387\n",
      "Epoch 3/3\n",
      "12438/12438 [==============================] - 208s 17ms/step - loss: 0.1126 - accuracy: 0.9654 - val_loss: 0.2670 - val_accuracy: 0.9396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c1e639bdae0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0WgoE64kcYYZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WgoE64kcYYZ",
    "outputId": "c843e1b0-ecbf-44d0-d920-dcf539060fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 8s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)\n",
    "#предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2YXG0IOdcYdH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YXG0IOdcYdH",
    "outputId": "8aa5be68-a703-469d-afcc-b6c70ee2449a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\r\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      1109\n",
      "           1       0.86      0.76      0.81      1127\n",
      "           2       0.93      0.78      0.84       627\n",
      "           3       0.88      0.54      0.67      3455\n",
      "           4       0.94      0.99      0.97     54660\n",
      "           5       0.98      0.71      0.82      5356\n",
      "\n",
      "    accuracy                           0.94     66334\n",
      "   macro avg       0.91      0.78      0.83     66334\n",
      "weighted avg       0.94      0.94      0.93     66334\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0    1    2     3      4     5\n",
      "row_0                                  \n",
      "0      981    7    0    58     62     1\n",
      "1       28  857    0    27    214     1\n",
      "2        4    7  486     6    124     0\n",
      "3       65   39   25  1877   1433    16\n",
      "4        9   79   14   168  54318    72\n",
      "5        0    2    0     5   1543  3806\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXxSHzltdskF",
   "metadata": {
    "id": "iXxSHzltdskF"
   },
   "source": [
    "\n",
    "На биграммах и триграммах метрики показали лучший результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ao4QImYfdCki",
   "metadata": {
    "id": "Ao4QImYfdCki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qu7aAEzdCnG",
   "metadata": {
    "id": "2qu7aAEzdCnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pbiV4-8AdCpi",
   "metadata": {
    "id": "pbiV4-8AdCpi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
