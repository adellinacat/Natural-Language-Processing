{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a32e866",
   "metadata": {
    "id": "4a32e866"
   },
   "source": [
    "###  9. Языковое моделирование\n",
    "\n",
    "#### Задание<br>\n",
    "Разобраться с моделькой генерации текста, собрать самим или взять датасет с вебинара и обучить генератор\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c37d37",
   "metadata": {
    "id": "54c37d37"
   },
   "source": [
    "Подготовим окружение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcbe116",
   "metadata": {
    "id": "7bcbe116"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c8f68",
   "metadata": {
    "id": "8d7c8f68"
   },
   "source": [
    "Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zsNpqg2b1RrT",
   "metadata": {
    "id": "zsNpqg2b1RrT"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2KieEA9I1RvM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "2KieEA9I1RvM",
    "outputId": "6741c49b-0621-40cf-ba58-f92d5da4cf3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4fe34256-dc72-45d9-b20e-d314f2d1a85a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-4fe34256-dc72-45d9-b20e-d314f2d1a85a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evgenyi_onegin.txt to evgenyi_onegin.txt\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612d43fd",
   "metadata": {
    "id": "612d43fd"
   },
   "outputs": [],
   "source": [
    "path = 'evgenyi_onegin.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c10721",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8c10721",
    "outputId": "21129dad-3d18-4ec3-89a0-6ee58ce6371d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 286984 characters\n"
     ]
    }
   ],
   "source": [
    "#удалим спецсимволы\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.replace('\\ufeff', '')\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f6f61c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f6f61c2",
    "outputId": "60572dc0-822b-481e-a428-e5dfb0ff3816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин\n",
      "\n",
      "                                Евгений Онегин\n",
      "                                Роман в стихах\n",
      "\n",
      "                        Не мысля гордый свет забавить,\n",
      "                        Вниманье дружбы возлюбя,\n",
      "                        Хотел бы я тебе представить\n",
      "                        Залог достойнее тебя,\n",
      "                        Достойнее души прекрасной,\n",
      "                        Святой исполненной мечты,\n",
      "                        Поэзии живой и ясной,\n",
      "                        Высо\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88297382",
   "metadata": {
    "id": "88297382"
   },
   "outputs": [],
   "source": [
    "text = text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07eba9e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07eba9e6",
    "outputId": "03d6e85a-c79f-42c9-ede9-66ce350454ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Уникальные символы в файле\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cbd478b",
   "metadata": {
    "id": "2cbd478b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# сопоставим уникальныуе символы с индексами\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77848e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77848e4b",
    "outputId": "06649d95-34bd-4b78-90a9-9b2152becb22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 71, 110, 104, ..., 104, 121,   0]),\n",
       " 'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                                Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Святой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высо',\n",
       " 573968,\n",
       " 573968)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int, text[:500], len(text_as_int), len(text) #символьная и числовая длина совпадает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1379fc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1379fc96",
    "outputId": "9f698e4b-6112-4ea8-fdc6-a129f9d8b49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А\n",
      "л\n",
      "е\n",
      "к\n",
      "с\n"
     ]
    }
   ],
   "source": [
    "#максимальная длина предложения\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "#создадим тренировочный датасет примеров\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f36ce7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0f36ce7",
    "outputId": "71570b5e-7d4f-41d6-cefb-050b650c2027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
      "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
      "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
      "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
      "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
     ]
    }
   ],
   "source": [
    "#размер последовательностей возьмем по 5\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bcb8336",
   "metadata": {
    "id": "4bcb8336"
   },
   "outputs": [],
   "source": [
    "#создадим целевую переменную в числовом и символьном представлении\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5467569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5467569",
    "outputId": "bee06836-dc8f-43e0-d75b-cd1842c3a72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed14b1",
   "metadata": {
    "id": "14ed14b1"
   },
   "source": [
    "Создадим кастомную модель на основе LSTM слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65c07de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e65c07de",
    "outputId": "091bda52-6618-4fc8-80bf-c24a18ccf3a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размер партии входящих символов\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23cd623",
   "metadata": {
    "id": "e23cd623"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) # длина символьного словаря\n",
    "\n",
    "embedding_dim = 256     #размеры эмбедингов\n",
    "\n",
    "rnn_units = 1024        #количество единиц RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a4a8419",
   "metadata": {
    "id": "1a4a8419"
   },
   "outputs": [],
   "source": [
    "def custom_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "         tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25762927",
   "metadata": {
    "id": "25762927"
   },
   "outputs": [],
   "source": [
    "model = custom_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e087ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22e087ab",
    "outputId": "dbc10030-77b1-40c0-9127-8fc40a6bd75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 9.82167876e-08 -1.31884935e-05  7.87777753e-06 ...  7.91770799e-06\n",
      "    1.46531102e-05  3.79539938e-06]\n",
      "  [-3.74233878e-06 -3.29119393e-05  2.52597147e-05 ...  2.00612685e-05\n",
      "    3.29373797e-05 -3.32847685e-06]\n",
      "  [-6.95966310e-06 -5.10529208e-05  6.47224952e-05 ...  3.85709791e-05\n",
      "    4.85859928e-05 -3.31991650e-05]\n",
      "  ...\n",
      "  [-2.26237928e-03 -3.90460691e-03  5.52886678e-03 ...  2.68567004e-04\n",
      "    7.69921998e-03 -1.67382369e-03]\n",
      "  [-2.36142986e-03 -4.03757952e-03  5.07123116e-03 ...  3.94784962e-04\n",
      "    7.61754485e-03 -1.66451256e-03]\n",
      "  [-2.40765372e-03 -4.15153662e-03  4.54044528e-03 ...  5.24297706e-04\n",
      "    7.49317836e-03 -1.64235989e-03]]\n",
      "\n",
      " [[ 1.01610349e-05 -1.10081119e-05 -2.56513704e-05 ...  1.38181749e-05\n",
      "    2.69811608e-05  5.10676728e-06]\n",
      "  [ 3.53446849e-05 -3.89087545e-05 -5.55728111e-05 ...  2.86995237e-05\n",
      "    8.35690298e-05  1.31986471e-05]\n",
      "  [ 7.84848744e-05 -9.46172513e-05 -5.94844496e-05 ...  2.63692236e-05\n",
      "    1.53860456e-04  1.40801194e-05]\n",
      "  ...\n",
      "  [-4.79541253e-04 -1.33200642e-03  6.36902545e-03 ... -4.93530417e-04\n",
      "    5.65544702e-03 -1.43309275e-03]\n",
      "  [-6.17515761e-04 -1.51935220e-03  6.51618792e-03 ... -4.29801992e-04\n",
      "    5.93028264e-03 -1.26545783e-03]\n",
      "  [-7.59833725e-04 -1.72033254e-03  6.63089380e-03 ... -3.64101958e-04\n",
      "    6.20258041e-03 -1.11914100e-03]]\n",
      "\n",
      " [[ 2.68841450e-06 -2.62875756e-06  1.81976957e-05 ... -8.99312818e-06\n",
      "    8.16416014e-06 -8.60338241e-06]\n",
      "  [ 1.28902966e-05 -2.09726895e-05  7.32584886e-05 ... -3.88048575e-05\n",
      "    2.19827489e-05 -3.93955961e-05]\n",
      "  [ 4.75312918e-05 -4.95729400e-05  1.53396060e-04 ... -7.45158977e-05\n",
      "    6.28162306e-05 -7.89585101e-05]\n",
      "  ...\n",
      "  [-1.19302724e-03 -3.34017281e-03  3.18897236e-03 ...  1.99547736e-03\n",
      "    5.36218099e-03 -1.82086125e-03]\n",
      "  [-1.17892469e-03 -3.26606771e-03  2.99225305e-03 ...  2.02036789e-03\n",
      "    5.07960794e-03 -2.07285583e-03]\n",
      "  [-1.19574717e-03 -3.20143835e-03  2.80135265e-03 ...  2.04688543e-03\n",
      "    4.84560151e-03 -2.34448817e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.77426882e-05  5.92961669e-07  2.71892395e-06 ...  7.91962884e-06\n",
      "    1.81660253e-05  4.48223045e-06]\n",
      "  [ 6.05954410e-05  2.05905235e-06 -2.64061782e-06 ...  3.34728065e-05\n",
      "    5.09218007e-05  4.26617225e-06]\n",
      "  [ 1.22681435e-04  5.17865556e-06  1.66131558e-05 ...  5.78950057e-05\n",
      "    1.02343067e-04 -1.72668515e-05]\n",
      "  ...\n",
      "  [-1.23744900e-03 -3.13939620e-03  7.63373822e-03 ... -8.78307619e-04\n",
      "    7.91446865e-03 -1.64552301e-03]\n",
      "  [-1.46331941e-03 -3.29133659e-03  7.41925603e-03 ... -7.67214340e-04\n",
      "    7.97665305e-03 -1.56263076e-03]\n",
      "  [-1.68832624e-03 -3.45283304e-03  7.13226432e-03 ... -6.16046949e-04\n",
      "    8.00525304e-03 -1.47421262e-03]]\n",
      "\n",
      " [[ 2.68841450e-06 -2.62875756e-06  1.81976957e-05 ... -8.99312818e-06\n",
      "    8.16416014e-06 -8.60338241e-06]\n",
      "  [ 1.27338681e-05 -8.03305375e-06  7.83782089e-05 ... -3.62257160e-05\n",
      "    3.94327799e-05 -3.53657088e-05]\n",
      "  [ 3.38144455e-05 -1.42676072e-05  2.01304414e-04 ... -8.74526304e-05\n",
      "    1.09457913e-04 -8.64899048e-05]\n",
      "  ...\n",
      "  [ 4.14839829e-04 -1.48977851e-03  6.22251537e-03 ... -5.48701501e-05\n",
      "    5.89711778e-03 -2.11793068e-03]\n",
      "  [ 2.97191204e-04 -1.62023283e-03  6.64225779e-03 ... -1.43462326e-04\n",
      "    6.10317430e-03 -2.08612438e-03]\n",
      "  [ 1.67799182e-04 -1.76782906e-03  6.99905120e-03 ... -2.05073506e-04\n",
      "    6.30810345e-03 -2.04247655e-03]]\n",
      "\n",
      " [[ 2.68841450e-06 -2.62875756e-06  1.81976957e-05 ... -8.99312818e-06\n",
      "    8.16416014e-06 -8.60338241e-06]\n",
      "  [ 1.27338681e-05 -8.03305375e-06  7.83782089e-05 ... -3.62257160e-05\n",
      "    3.94327799e-05 -3.53657088e-05]\n",
      "  [ 3.38144455e-05 -1.42676072e-05  2.01304414e-04 ... -8.74526304e-05\n",
      "    1.09457913e-04 -8.64899048e-05]\n",
      "  ...\n",
      "  [ 9.86717641e-05 -1.12060155e-03  7.47763552e-03 ... -9.23542480e-04\n",
      "    5.20135229e-03 -1.43222616e-03]\n",
      "  [ 1.54413283e-06 -1.25601003e-03  7.67584331e-03 ... -8.45969887e-04\n",
      "    5.44846617e-03 -1.37192768e-03]\n",
      "  [-1.05286948e-04 -1.41474186e-03  7.82291591e-03 ... -7.57804373e-04\n",
      "    5.69982920e-03 -1.31404772e-03]]], shape=(64, 100, 131), dtype=float32) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb655e96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb655e96",
    "outputId": "86d14f6a-fc34-462a-e538-019045115163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           33536     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 131)           134275    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30592899 (116.70 MB)\n",
      "Trainable params: 30592899 (116.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f150701",
   "metadata": {
    "id": "1f150701"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6066dd52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6066dd52",
    "outputId": "310ffcc2-daf3-4843-b96f-85e6a5623973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ввод: \n",
      " 'ет и шумит...\\n                        \"Погибну, - Таня говорит, -\\n                        Но гибель '\n",
      "\n",
      "Прогнозируемое продолжение текста: \n",
      " 'У2Y?lОвОMуъосuФ.)3D цIпАяG9ЧXдыGрOэ\\nsSEЭoН1ыlуR01чх(ЦFОM1ЕМtхОддЭ1AmQщИмЛБQЭъTXПLДплЯяIСонzэтгМ}п-4l'\n"
     ]
    }
   ],
   "source": [
    "print(\"Ввод: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Прогнозируемое продолжение текста: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1821573",
   "metadata": {
    "id": "b1821573"
   },
   "source": [
    "Как видно, с прогнозом все очень плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4b0b2d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4b0b2d5",
    "outputId": "5d74955f-ae65-4fac-ad37-e6ba8bcbcda4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 131)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.8758535\n"
     ]
    }
   ],
   "source": [
    "#добавим функцию потерь\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd1e5138",
   "metadata": {
    "id": "dd1e5138"
   },
   "outputs": [],
   "source": [
    "#скомпилируем модель\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b392102",
   "metadata": {
    "id": "1b392102"
   },
   "outputs": [],
   "source": [
    "#создадим папку, в которой будут сохранены контрольные точки\n",
    "!rm -rf /training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nyEsUKYt2dws",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyEsUKYt2dws",
    "outputId": "22fb772f-9bfc-4281-d390-e242c09b91b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/training_checkpoints': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "454c1bc1",
   "metadata": {
    "id": "454c1bc1"
   },
   "outputs": [],
   "source": [
    "name1 = 'training_checkpoints'\n",
    "\n",
    "os.mkdir('/{}'.format(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fc48732",
   "metadata": {
    "id": "0fc48732"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=7*10,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f955e49",
   "metadata": {
    "id": "9f955e49"
   },
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca74489d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca74489d",
    "outputId": "b1231b05-cbb9-41ce-9b1f-6efd1e420b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 39s 362ms/step - loss: 2.3546\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 33s 369ms/step - loss: 1.8593\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 35s 381ms/step - loss: 1.6293\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 35s 386ms/step - loss: 1.4845\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 1.4031\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 35s 382ms/step - loss: 1.3679\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 35s 379ms/step - loss: 1.3401\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 35s 392ms/step - loss: 1.3268\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 1.3044\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 34s 381ms/step - loss: 1.2734\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 1.2538\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 36s 392ms/step - loss: 1.2487\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 34s 381ms/step - loss: 1.2368\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 1.1958\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 35s 385ms/step - loss: 1.1696\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 35s 394ms/step - loss: 1.1543\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 1.1199\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 35s 387ms/step - loss: 1.1224\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 35s 380ms/step - loss: 1.0800\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 35s 393ms/step - loss: 1.0716\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 34s 380ms/step - loss: 1.0432\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 34s 382ms/step - loss: 1.0113\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 35s 384ms/step - loss: 0.9587\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 35s 391ms/step - loss: 0.9260\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 34s 379ms/step - loss: 0.8925\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "EPOCHS = 25\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd0376d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "cd0376d2",
    "outputId": "8f9c8e92-aa95-4795-cf64-35879654c82e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/training_checkpoints/ckpt_25'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dacfd329",
   "metadata": {
    "id": "dacfd329"
   },
   "outputs": [],
   "source": [
    "model = custom_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f50337b",
   "metadata": {
    "id": "5f50337b"
   },
   "outputs": [],
   "source": [
    "#функция генареции текста\n",
    "\n",
    "def generate_text(model, start_string, temperature=1, num_generate=500):# Шаг оценки (генерация текста с использованием обученной модели)\n",
    "\n",
    "    num_generate = num_generate # Количество символов для генерации\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string] # Преобразование нашей стартовой строки в числа (векторизация)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = [] # Пустая строка для хранения результатов\n",
    "\n",
    "    temperature = temperature\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions = predictions / temperature # использование категориального распределения для прогнозирования символа, возвращаемого моделью\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f5da4",
   "metadata": {
    "id": "d43f5da4"
   },
   "source": [
    "Посмотрим на резелтат обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60461857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60461857",
    "outputId": "3c181e66-7a69-47c2-9d7e-b8adbbf15efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных правил и замесак.\n",
      "\n",
      "                              Трещев, и бедною Ленский,\n",
      "                     XXIV\n",
      "\n",
      "                        - Сорхчаеные: Пыть по нуе\n",
      "      vбракго. Ада\n",
      "     Свено без трузы удругся,\n",
      "        Сомрейниц урених сельде.\n",
      "\n",
      "                                   XII\n",
      "\n",
      "         Россижа, друще   Чуть засета,\n",
      "      Томевиет, фозятелый,\n",
      "                     Поскаженным,\n",
      "                    голды Людувеголья\n",
      "             Подру     Тум нас?\n",
      ".. спатнепь, стмой природа,\n",
      "             VII\n",
      "\n",
      " ни, где часпищо\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Мой дядя самых честных правил \", temperature=1.3)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92cd42",
   "metadata": {
    "id": "fa92cd42"
   },
   "source": [
    "Генерация происходит в случайном порядке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27055886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27055886",
    "outputId": "386a5cf4-fe7d-45a9-d98e-c0093e5c9bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных правил небщо Он? Лес, и взор вознада. Мте в душа:\n",
      "                         Татьяна жизни за мою\n",
      "                        Как не врыжателия вин,\n",
      "                        Все вов верительно врешет\n",
      "                      - В летгихму сдол к молвало\n",
      "                        Селится он уго потолым;\n",
      "                        Круда            И Тан: хоть Вы зномоне слега\n",
      "                        В сожно    Забось, трепетник и веластель\n",
      "                        Кпершило все злово б горог\n",
      "           Кешко бе-V\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "#снизим температуру\n",
    "text_ = generate_text(model, start_string=u\"Мой дядя самых честных правил \", temperature=1.1)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01456ee4",
   "metadata": {
    "id": "01456ee4"
   },
   "source": [
    "В этом тексте угадываются несколько реальных отрывков, соединенных неудачной генерацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86175689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86175689",
    "outputId": "a96e24b0-9aa9-4a71-8cae-4dc7e4443d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных правил не могной\n",
      "                            Разжеслось они сеобраты;\n",
      "                        Он с ворожет? Томя звазьной\n",
      "                        И вдруг молодной обро                      Beosieok                        Прошал перекли не постелся? -\n",
      "                        И вашествих он отлабилась,\n",
      "                        Я упозерий пылью.\n",
      "                                           XIV\n",
      "\n",
      "                        Трову то жальный и простой\n",
      "                        Тапыя другает хоть дни болете,\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "#снизим температуру до 0.9. Модель покажет одинаковую генерацию большого длинного отрывка\n",
    "text_ = generate_text(model, start_string=u\"Мой дядя самых честных правил \", temperature=0.9)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec138dbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec138dbf",
    "outputId": "ee07eaed-302e-4bde-a9ac-4e9dffc27db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных правил своей\n",
      "                        И все не волос в садом строгих странной поленой\n",
      "                        И все не волос на столом\n",
      "                        И вот не забыть не постели\n",
      "                        И все под ним под ней подруга\n",
      "                        И вот не волос и постеле\n",
      "                        И все в толко слезы подруга\n",
      "                        Он под ней не после все в ней привета\n",
      "                        И все не волненье подавить\n",
      "                        И вот не слышит и полно\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Мой дядя самых честных правил \", temperature=0.09)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dd81b06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dd81b06",
    "outputId": "94d0a9b8-c9a1-4fca-9467-275584f0cfab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e481ff",
   "metadata": {
    "id": "76e481ff"
   },
   "source": [
    "#### Выводы:\n",
    "\n",
    "Модель воспроизводит длинную последовательность. Вместо составления смешных предложений из отдельных слов воспроизводит один или несколько целых отрывков. Это свидтельствует о признаках переобучения. Для более качественного обучения модели требуется больше текста, чтобы она не запоминала привязки отдельных символов к кускам текста. Различные начальные участки текста генерируют разный начальный символ, что приводит к воспроизведению разных участков текста. Если генерируется по одному символу, то предсказание следующего символа сильно осложнено, если применить к мену целую строку, то генерация приобретает большую степень осмысленности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814ef26",
   "metadata": {
    "id": "0814ef26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
